1
00:00:00,334 --> 00:00:07,341
♪ ♪

2
00:00:09,676 --> 00:00:11,645
Hi, my name is Brett Keating,

3
00:00:11,678 --> 00:00:15,549
and it's my pleasure to be introducing you
to what is new in the Vision framework.

4
00:00:15,582 --> 00:00:17,551
You may be new to Vision.

5
00:00:17,584 --> 00:00:21,388
Perhaps this is the first session
you've seen about the Vision framework.

6
00:00:21,421 --> 00:00:23,991
If so, welcome.

7
00:00:24,024 --> 00:00:28,862
For your benefit, let's briefly recap some
highlights about the Vision framework.

8
00:00:28,896 --> 00:00:31,665
Some Vision framework facts for you.

9
00:00:31,698 --> 00:00:34,434
Vision was first introduced in 2017,

10
00:00:34,468 --> 00:00:37,871
and since then, many thousands
of great apps have been developed

11
00:00:37,905 --> 00:00:40,841
with the technology Vision provides.

12
00:00:40,874 --> 00:00:43,510
Vision is a collection
of computer vision algorithms

13
00:00:43,544 --> 00:00:45,646
that continues to grow over time

14
00:00:45,679 --> 00:00:47,781
and includes such things
as face detection,

15
00:00:47,814 --> 00:00:52,019
image classification,
and contour detection to name a few.

16
00:00:52,052 --> 00:00:56,490
Each of these algorithms is made available
through an easy-to-use, consistent API.

17
00:00:56,523 --> 00:00:58,926
If you know how to run one algorithm
in the Vision framework,

18
00:00:58,959 --> 00:01:01,061
you know how to run them all.

19
00:01:01,094 --> 00:01:04,698
And Vision takes full advantage
of Apple Silicon

20
00:01:04,731 --> 00:01:06,934
on all of the platforms it supports,

21
00:01:06,967 --> 00:01:10,771
to power the machine learning at the core
of many of Vision's algorithms.

22
00:01:10,804 --> 00:01:13,473
Vision is available on tvOS,

23
00:01:13,507 --> 00:01:15,709
iOS, and macOS;

24
00:01:15,742 --> 00:01:19,179
and will fully leverage
Apple Silicon on the Mac.

25
00:01:20,113 --> 00:01:23,884
Some recent additions to the Vision
framework include Person segmentation,

26
00:01:23,917 --> 00:01:25,252
shown here.

27
00:01:27,287 --> 00:01:31,358
Also hand pose estimation,
shown in this demo.

28
00:01:34,761 --> 00:01:37,631
And here is our
Action and Vision sample app,

29
00:01:37,664 --> 00:01:41,134
which uses body pose estimation
and trajectory analysis.

30
00:01:42,069 --> 00:01:46,306
Our agenda today begins
with an overview of some new revisions,

31
00:01:46,340 --> 00:01:49,176
which are updates to existing requests

32
00:01:49,209 --> 00:01:54,081
that may provide increased functionality,
improve performance, or improve accuracy.

33
00:01:57,150 --> 00:02:00,454
First, we have a new revision
for text recognition.

34
00:02:00,487 --> 00:02:06,527
This is the third revision,
given by VNRecognizeTextRequestRevision3.

35
00:02:06,560 --> 00:02:10,464
This is the text recognizer
that powers the amazing Live Text feature.

36
00:02:10,497 --> 00:02:13,333
The text recognizer supports
several languages,

37
00:02:13,367 --> 00:02:15,969
and you may discover
which languages are supported

38
00:02:16,003 --> 00:02:19,173
by calling supportedRecognitionLanguages.

39
00:02:19,206 --> 00:02:23,443
We have now added a few new languages,
and I'll show you a couple examples.

40
00:02:23,477 --> 00:02:27,247
We are now supporting
the Korean language in Vision.

41
00:02:27,281 --> 00:02:31,618
Here is an example of Vision at work
transcribing a Korean receipt.

42
00:02:31,652 --> 00:02:35,255
And here is a corresponding example
for Japanese,

43
00:02:35,289 --> 00:02:38,058
also showing the results
of Vision's text recognition

44
00:02:38,091 --> 00:02:40,961
on this now-supported language.

45
00:02:40,994 --> 00:02:46,767
For text recognition, we have
a new automatic language identification.

46
00:02:46,800 --> 00:02:50,170
You may still specify
the recognition languages to use

47
00:02:50,204 --> 00:02:53,740
using the recognitionLanguages property.

48
00:02:53,774 --> 00:02:56,577
But suppose you don't know
ahead of time which languages

49
00:02:56,610 --> 00:02:59,913
your app user might be trying
to recognize.

50
00:02:59,947 --> 00:03:03,417
Now, but only
for accurate recognition mode,

51
00:03:03,450 --> 00:03:07,955
you may ask the text recognizer
to automatically detect the language

52
00:03:07,988 --> 00:03:11,091
by setting
automaticallyDetectsLanguage to true.

53
00:03:12,759 --> 00:03:15,562
It's best to use this
just for such a situation

54
00:03:15,596 --> 00:03:18,398
where you don't know
which language to recognize,

55
00:03:18,432 --> 00:03:21,702
because the language detection
can occasionally get this wrong.

56
00:03:21,735 --> 00:03:25,038
If you have the prior knowledge
about which language to recognize,

57
00:03:25,072 --> 00:03:27,975
it's still best to specify
these languages to Vision

58
00:03:28,008 --> 00:03:31,612
and leave automaticallyDetectsLanguage
turned off.

59
00:03:34,681 --> 00:03:39,119
Next, we have a new third revision
for our barcode detection,

60
00:03:39,152 --> 00:03:43,123
called VNDetectBarcodesRequestRevision3.

61
00:03:43,156 --> 00:03:46,360
This revision leverages
modern machine learning under the hood,

62
00:03:46,393 --> 00:03:49,162
which is a departure from prior revisions.

63
00:03:49,196 --> 00:03:51,431
Barcodes come in a variety of symbologies,

64
00:03:51,465 --> 00:03:55,769
from barcodes often seen on products
in stores, to QR codes,

65
00:03:55,802 --> 00:03:59,239
to specialty codes
used in healthcare applications.

66
00:03:59,273 --> 00:04:01,775
In order to know
which symbologies Vision supports,

67
00:04:01,808 --> 00:04:03,977
you may call supportedSymbologies.

68
00:04:06,113 --> 00:04:08,582
Let's talk about performance.

69
00:04:08,615 --> 00:04:10,617
Partly because we are using ML,

70
00:04:10,651 --> 00:04:15,222
we are detecting multiple codes
in one shot rather than one at a time,

71
00:04:15,255 --> 00:04:20,160
so the request will be faster
for images containing multiple codes.

72
00:04:20,194 --> 00:04:25,465
Also, more codes are detected
in a given image containing many codes,

73
00:04:25,499 --> 00:04:27,167
due to increased accuracy.

74
00:04:27,201 --> 00:04:31,538
And furthermore, there are few,
if any, duplicate detections.

75
00:04:31,572 --> 00:04:34,441
The bounding boxes are
improved for some codes,

76
00:04:34,474 --> 00:04:39,780
particularly linear codes such as ean13,
for which a line was formerly returned.

77
00:04:39,813 --> 00:04:42,850
Now, the bounding box surrounds
the entire visible code.

78
00:04:45,018 --> 00:04:49,022
Finally, the ML model is more able
to ignore such things as curved surfaces,

79
00:04:49,056 --> 00:04:53,393
reflections, and other artifacts that have
hindered detection accuracy in the past.

80
00:04:56,797 --> 00:04:59,733
Both of these new revisions,
for text recognition

81
00:04:59,766 --> 00:05:03,504
and for barcode detection,
form the technological foundations

82
00:05:03,537 --> 00:05:08,075
for the VisionKit Data Scanner API,
which is a drop-in UI element

83
00:05:08,108 --> 00:05:11,879
that sets up the camera stream
to scan and return barcodes and text.

84
00:05:11,912 --> 00:05:14,948
It's really a fantastic addition
to our SDK,

85
00:05:14,982 --> 00:05:19,119
and I highly recommend you check out
the session about it to learn more.

86
00:05:19,152 --> 00:05:22,956
The final new revision I'll tell you
about today is a new revision

87
00:05:22,990 --> 00:05:25,626
for our optical flow request called

88
00:05:25,659 --> 00:05:29,363
VNGenerateOpticalFlowRequestRevision2.

89
00:05:29,396 --> 00:05:32,900
Like the barcode detector,
this new revision also uses

90
00:05:32,933 --> 00:05:35,102
modern machine learning under the hood.

91
00:05:37,471 --> 00:05:41,508
Although optical flow is one of the
longest studied computer vision problems,

92
00:05:41,542 --> 00:05:44,444
you might not be aware of what it does,
compared to detection of things

93
00:05:44,478 --> 00:05:47,981
which form part of all of our daily lives,
like text and barcodes.

94
00:05:48,849 --> 00:05:52,452
Optical flow analyzes
two consecutive images,

95
00:05:52,486 --> 00:05:54,621
typically frames from a video.

96
00:05:54,655 --> 00:05:56,590
Depending on your use case,
you might look at motion

97
00:05:56,623 --> 00:06:00,027
between two adjacent frames,
or skip a few frames in between,

98
00:06:00,060 --> 00:06:02,996
but in any case, the two images
should be in chronological order.

99
00:06:04,831 --> 00:06:09,303
The analysis provides an estimate of
the direction and magnitude of the motion,

100
00:06:09,336 --> 00:06:13,774
or by how much parts of the first image
need to "move," so to speak,

101
00:06:13,807 --> 00:06:17,544
to be positioned correctly
in the second image.

102
00:06:17,578 --> 00:06:20,480
A VNPixelBufferObservation is the result,

103
00:06:20,514 --> 00:06:23,684
which represents this motion
at all places in the image.

104
00:06:23,717 --> 00:06:25,485
It is a two-channel image.

105
00:06:25,519 --> 00:06:28,021
One channel contains the X magnitude,

106
00:06:28,055 --> 00:06:30,791
and the other contains the Y magnitude.

107
00:06:30,824 --> 00:06:34,094
Together, these form 2D vectors
at each pixel

108
00:06:34,127 --> 00:06:37,064
arranged in this 2D image
so that their locations map

109
00:06:37,097 --> 00:06:41,134
to corresponding locations in the images
that were provided as input.

110
00:06:41,168 --> 00:06:43,470
Let's have a look at this visually.

111
00:06:43,504 --> 00:06:47,541
Suppose you have an incoming video
and several frames are coming in,

112
00:06:47,574 --> 00:06:50,210
but let's look
at these two images in particular.

113
00:06:50,244 --> 00:06:52,479
Here we have a dog running on the beach.

114
00:06:52,513 --> 00:06:54,248
From the left image to the right image,

115
00:06:54,281 --> 00:06:57,017
it appears the dog has moved
a bit to the left.

116
00:06:57,050 --> 00:06:59,419
How would you estimate
and represent this motion?

117
00:07:01,255 --> 00:07:03,323
Well, you would run optical flow

118
00:07:03,357 --> 00:07:05,926
and arrive at something
akin to the image below.

119
00:07:05,959 --> 00:07:08,562
The darker areas are
where motion has been found,

120
00:07:08,595 --> 00:07:12,566
and notice that it does indeed
look just like the shape of the dog.

121
00:07:12,599 --> 00:07:16,170
That's because only the dog
is really moving in this scene.

122
00:07:16,203 --> 00:07:19,907
We are showing the motion vectors
in this image by using "false color,"

123
00:07:19,940 --> 00:07:23,777
which maps the x,y
from the vectors into a color palette.

124
00:07:23,810 --> 00:07:27,681
In this false color representation,
"red" hues happen to indicate

125
00:07:27,714 --> 00:07:30,150
movement primarily to the left.

126
00:07:30,184 --> 00:07:33,253
Now that you've seen
an example from one frame,

127
00:07:33,287 --> 00:07:35,989
let's see how it looks
for a whole video clip.

128
00:07:36,023 --> 00:07:39,660
Here we compute optical flow
for a short clip of this dog

129
00:07:39,693 --> 00:07:42,196
fetching a water bottle on a beach.

130
00:07:42,229 --> 00:07:45,065
On the left is the result from revision 1.

131
00:07:45,098 --> 00:07:49,002
On the right is the result
from our new ML-based revision 2.

132
00:07:49,036 --> 00:07:52,639
Hopefully some of the improvements
in revision 2 are clear to see.

133
00:07:52,673 --> 00:07:55,142
For one thing, perhaps most obviously,

134
00:07:55,175 --> 00:07:58,712
the water bottle's motion is captured
much more accurately.

135
00:07:58,745 --> 00:08:03,083
You might also notice improvements in
some of the estimated motion of the dog.

136
00:08:03,116 --> 00:08:05,953
I notice improvements
in the tail most clearly

137
00:08:05,986 --> 00:08:10,023
but also can see the motion of his ears
flapping in the new revision.

138
00:08:10,057 --> 00:08:13,293
The first revision also contains
a bit of background noise motions,

139
00:08:13,327 --> 00:08:17,664
while the second revision more coherently
represents the backgrounds as not moving.

140
00:08:17,698 --> 00:08:21,668
Hopefully that example gave you
a good idea what this technology does.

141
00:08:21,702 --> 00:08:25,272
Now let's dive in a bit
on how you might use it in your app.

142
00:08:25,305 --> 00:08:29,910
Clearly the primary use case is
to discover local motion in a video.

143
00:08:29,943 --> 00:08:32,980
This feeds directly
into security video use cases,

144
00:08:33,013 --> 00:08:35,682
where it's most important to identify
and localize motions

145
00:08:35,716 --> 00:08:37,784
that deviate from the background,

146
00:08:37,818 --> 00:08:40,053
and it should be mentioned
that optical flow does work best

147
00:08:40,087 --> 00:08:44,391
for stationary cameras,
such as most security cameras.

148
00:08:44,424 --> 00:08:46,393
You might want to use
Vision's object tracker

149
00:08:46,426 --> 00:08:48,795
to track objects
that are moving in a video,

150
00:08:48,829 --> 00:08:51,265
but need to know
where to initialize a tracker.

151
00:08:51,298 --> 00:08:54,501
Optical flow can help you there as well.

152
00:08:54,535 --> 00:08:58,338
If you have some computer vision
or image processing savvy of your own,

153
00:08:58,372 --> 00:09:00,407
you might leverage
our optical flow results

154
00:09:00,440 --> 00:09:03,076
to enable further video processing.

155
00:09:03,110 --> 00:09:06,747
Video interpolation,
or video action analysis,

156
00:09:06,780 --> 00:09:10,851
can greatly benefit from
the information optical flow provides.

157
00:09:10,884 --> 00:09:13,854
Let's now dig into some
important additional differences

158
00:09:13,887 --> 00:09:16,023
between revision 1 and revision 2.

159
00:09:16,857 --> 00:09:19,459
Revision 1 always returns
optical flow fields

160
00:09:19,493 --> 00:09:22,029
that have the same resolution
as the input.

161
00:09:22,062 --> 00:09:25,032
Revision 2 will also do this by default.

162
00:09:25,065 --> 00:09:28,135
However, there is a tiny wrinkle:
partially due to the fact

163
00:09:28,168 --> 00:09:32,039
that revision 2 is ML-based,
the output of the underlying model

164
00:09:32,072 --> 00:09:36,710
is relatively low resolution
compared to most input image resolutions.

165
00:09:36,743 --> 00:09:40,414
Therefore, to match
revision 1 default behavior,

166
00:09:40,447 --> 00:09:42,382
some upsampling must be done,

167
00:09:42,416 --> 00:09:45,853
and we are using
bilinear upsampling to do this.

168
00:09:45,886 --> 00:09:48,856
Here is a visual example
explaining what upsampling does.

169
00:09:48,889 --> 00:09:52,659
On the left, we have a zoomed-in portion
of the network output,

170
00:09:52,693 --> 00:09:55,863
which is low resolution
and therefore appears pixelated.

171
00:09:55,896 --> 00:10:00,000
The overall flow field might have
an aspect ratio of 7:5.

172
00:10:00,033 --> 00:10:03,504
On the right, we have a similar region
taken from the same field,

173
00:10:03,537 --> 00:10:06,507
upsampled to the original
image resolution.

174
00:10:06,540 --> 00:10:11,645
Perhaps that image also has
a different aspect ratio, let's say 16:9.

175
00:10:11,678 --> 00:10:14,948
You will notice that the edges
of the flow field are smoothed out

176
00:10:14,982 --> 00:10:18,018
by the bilinear upsampling.

177
00:10:18,051 --> 00:10:20,854
Due to the potential
for the aspect ratios to differ,

178
00:10:20,888 --> 00:10:23,524
keep in mind that as part
of the upsampling process,

179
00:10:23,557 --> 00:10:25,893
the flow image will be stretched

180
00:10:25,926 --> 00:10:28,061
in order to properly correspond
the flow field

181
00:10:28,095 --> 00:10:30,230
to what is happening in the image.

182
00:10:30,264 --> 00:10:32,266
When working
with the network output directly,

183
00:10:32,299 --> 00:10:36,136
you should account for resolution
and aspect ratio in a similar fashion

184
00:10:36,170 --> 00:10:39,006
when mapping flow results
to the original images.

185
00:10:41,575 --> 00:10:43,777
You have the option to skip the upsampling

186
00:10:43,810 --> 00:10:47,581
by turning on keepNetworkOutput
on the request.

187
00:10:47,614 --> 00:10:49,983
This will give you the raw model output.

188
00:10:50,017 --> 00:10:54,154
There are four computationAccuracy
settings you may apply to the request

189
00:10:54,188 --> 00:10:57,357
in order to choose
an available output resolution.

190
00:10:57,391 --> 00:11:00,694
You can see the resolutions
for each accuracy setting in this table,

191
00:11:00,727 --> 00:11:03,297
but be sure to always check
the width and height of the pixel buffer

192
00:11:03,330 --> 00:11:05,265
contained in the observation.

193
00:11:06,066 --> 00:11:07,634
When should you use network output,

194
00:11:07,668 --> 00:11:10,404
and when should you allow Vision
to upsample?

195
00:11:10,437 --> 00:11:14,408
The default behavior is best
if you already are using optical flow

196
00:11:14,441 --> 00:11:17,678
and want the behavior
to remain backward compatible.

197
00:11:17,711 --> 00:11:20,280
It's also a good option
if you want upsampled output,

198
00:11:20,314 --> 00:11:24,885
and bilinear is acceptable to you and
worth the additional memory and latency.

199
00:11:24,918 --> 00:11:28,355
Network output is best
if you don't need full resolution

200
00:11:28,388 --> 00:11:33,227
and can form correspondences on the fly
or just want to initialize a tracker.

201
00:11:33,260 --> 00:11:35,262
Network output may also be
the right choice

202
00:11:35,295 --> 00:11:37,364
if you do need a full resolution flow,

203
00:11:37,397 --> 00:11:40,567
but would prefer to use
your own upsampling methods.

204
00:11:40,601 --> 00:11:44,705
That covers the new algorithm revisions
for this session.

205
00:11:44,738 --> 00:11:47,140
Let's move on to discuss
some spring cleaning we are doing

206
00:11:47,174 --> 00:11:50,511
in the Vision framework
and how it might impact you.

207
00:11:50,544 --> 00:11:53,780
We first introduced face detection
and face landmarks

208
00:11:53,814 --> 00:11:56,316
when Vision was initially released
five years ago,

209
00:11:56,350 --> 00:11:59,419
as "revision 1" for each algorithm.

210
00:11:59,453 --> 00:12:03,123
Since that time
we've released two newer revisions,

211
00:12:03,156 --> 00:12:06,593
which use more efficient
and more accurate technologies.

212
00:12:06,627 --> 00:12:10,330
Therefore, we are removing
the first revisions of these algorithms

213
00:12:10,364 --> 00:12:15,035
from Vision framework, while keeping
the second and third revisions only.

214
00:12:15,068 --> 00:12:18,138
However, if you use revision 1,
never fear.

215
00:12:18,172 --> 00:12:21,708
We will continue to support code
that specifies revision 1

216
00:12:21,742 --> 00:12:26,947
or code that has been compiled against
SDKs which only contained revision 1.

217
00:12:26,980 --> 00:12:28,749
How is that possible, you may ask?

218
00:12:28,782 --> 00:12:32,252
Revision 1 executes an algorithm
under the hood

219
00:12:32,286 --> 00:12:36,256
that I have called
"the revision 1 detector" in this diagram.

220
00:12:36,290 --> 00:12:40,527
In the same way, revision 2 uses
the revision 2 detector.

221
00:12:40,561 --> 00:12:42,796
What we have done
for this release of Vision

222
00:12:42,829 --> 00:12:45,065
is to satisfy revision 1 requests

223
00:12:45,098 --> 00:12:48,101
with the output
of the revision 2 detector.

224
00:12:48,135 --> 00:12:52,506
Additionally, the revision 1 request
will be marked as deprecated.

225
00:12:52,539 --> 00:12:56,376
This allows us to remove
the old revision 1 detector completely,

226
00:12:56,410 --> 00:12:59,613
allowing the Vision framework
to remain streamlined.

227
00:12:59,646 --> 00:13:01,448
This has several benefits,

228
00:13:01,481 --> 00:13:04,318
not the least of which
is to save space on disk,

229
00:13:04,351 --> 00:13:09,256
which makes our OS releases and SDKs
less expensive to download and install.

230
00:13:09,289 --> 00:13:13,760
All you Vision experts out there might be
saying to yourselves, "But wait a minute,

231
00:13:13,794 --> 00:13:18,298
"revision 2 returns upside down faces
while revision 1 does not.

232
00:13:18,332 --> 00:13:21,435
Couldn't this behavior difference
have an impact on some apps?"

233
00:13:21,468 --> 00:13:24,605
It certainly would,
except we will be taking precautions

234
00:13:24,638 --> 00:13:27,541
to preserve revision 1 behavior.

235
00:13:27,574 --> 00:13:32,312
We will not be returning upside-down faces
from the revision 2 detector.

236
00:13:32,346 --> 00:13:35,616
Similarly,
the revision 2 landmark detector

237
00:13:35,649 --> 00:13:40,053
will return results that match
the revision 1 landmark constellation.

238
00:13:40,087 --> 00:13:42,389
The execution time is on par,

239
00:13:42,422 --> 00:13:45,726
and you ought to experience
a boost in accuracy.

240
00:13:45,759 --> 00:13:48,262
In any case,
this change will not require any apps

241
00:13:48,295 --> 00:13:52,332
to make any modifications to their code,
and things will continue to work.

242
00:13:54,134 --> 00:13:57,237
Still, we have a call to action for you.

243
00:13:57,271 --> 00:13:59,573
You shouldn't be satisfied
with using revision 1

244
00:13:59,606 --> 00:14:02,509
when we have
much better options available.

245
00:14:02,543 --> 00:14:04,645
We always recommend
using the latest revisions,

246
00:14:04,678 --> 00:14:07,548
and for these requests,
that would be revision 3.

247
00:14:08,749 --> 00:14:11,251
Of course the main reason
for this recommendation

248
00:14:11,285 --> 00:14:14,588
is to use the latest technology,
which provides the highest level

249
00:14:14,621 --> 00:14:18,725
of accuracy and performance available,
and who doesn't want that?

250
00:14:18,759 --> 00:14:22,129
Furthermore, we have established
and communicated several times,

251
00:14:22,162 --> 00:14:24,565
and we reiterate again here,

252
00:14:24,598 --> 00:14:28,368
the best practice of always
explicitly specifying your revisions,

253
00:14:28,402 --> 00:14:31,371
rather than relying
upon default behaviors.

254
00:14:31,405 --> 00:14:34,408
And that's what we've done
for our spring cleaning.

255
00:14:34,441 --> 00:14:36,944
Now let's talk about how we've made it
easier to debug apps

256
00:14:36,977 --> 00:14:38,812
that use the Vision framework.

257
00:14:38,846 --> 00:14:41,949
We've added
Quick Look Preview support to Vision.

258
00:14:41,982 --> 00:14:44,685
What does this mean
for Vision in particular?

259
00:14:44,718 --> 00:14:48,255
Well, now you can mouse over
VNObservations in the debugger,

260
00:14:48,288 --> 00:14:52,860
and with one click, you can visualize
the result on your input image.

261
00:14:52,893 --> 00:14:55,863
We've also made this available
in Xcode Playgrounds.

262
00:14:55,896 --> 00:14:59,099
I think the only way to really explain
how this can benefit your debugging

263
00:14:59,132 --> 00:15:00,334
is to show you.

264
00:15:00,367 --> 00:15:02,603
Let's move to an Xcode demo.

265
00:15:04,471 --> 00:15:08,208
Here we have a simple routine
that will detect face landmarks

266
00:15:08,242 --> 00:15:11,345
and return the face observations.

267
00:15:11,378 --> 00:15:15,849
First, we set up a face landmarks request.

268
00:15:15,883 --> 00:15:20,921
Then, if we have an image
ready to go in our class, we display it.

269
00:15:20,954 --> 00:15:24,458
Then, we declare an array
to hold our results.

270
00:15:26,126 --> 00:15:27,995
Inside the autoreleasepool,

271
00:15:28,028 --> 00:15:31,098
we instantiate a request handler
with that image,

272
00:15:31,131 --> 00:15:34,034
and then perform our request.

273
00:15:34,067 --> 00:15:38,338
Assuming all went well, we can retrieve
the results from the request.

274
00:15:39,206 --> 00:15:44,011
I will run it and get to a breakpoint
after we retrieve the results.

275
00:15:44,044 --> 00:15:45,812
So now I'm in the debugger.

276
00:15:45,846 --> 00:15:47,581
When I mouse over the results,

277
00:15:47,614 --> 00:15:50,284
the overlay shows
I've detected three faces.

278
00:15:50,317 --> 00:15:53,754
That's great.
I do have three faces in my input image.

279
00:15:53,787 --> 00:15:56,557
But how do I know
which observation is which face?

280
00:15:56,590 --> 00:15:59,927
That's where
the Quick Look Preview support comes in.

281
00:15:59,960 --> 00:16:04,898
As I go into this request,
I can click on each "eye" icon

282
00:16:04,932 --> 00:16:07,501
in order to visualize the result.

283
00:16:07,534 --> 00:16:12,206
The image appears with overlays drawn
for the landmarks constellation

284
00:16:12,239 --> 00:16:14,408
and for the face bounding box.

285
00:16:15,676 --> 00:16:18,612
Now you know where the first
observation is in the image.

286
00:16:19,780 --> 00:16:23,250
I can click on the next one to draw
overlays for the second observation

287
00:16:23,283 --> 00:16:25,619
and for the third observation.

288
00:16:27,521 --> 00:16:30,924
Continuing to the next breakpoint,
we run some code

289
00:16:30,958 --> 00:16:34,561
that prints the face observations
to the debug console.

290
00:16:34,595 --> 00:16:37,497
As you can imagine,
here in the debug console

291
00:16:37,531 --> 00:16:40,534
where the face information is printed,
it's pretty hard

292
00:16:40,567 --> 00:16:43,570
to immediately visualize in your mind
which face is which

293
00:16:43,604 --> 00:16:46,907
or whether the results look correct
just from these printed coordinates.

294
00:16:48,976 --> 00:16:51,245
But there is one more thing
to point out here.

295
00:16:51,278 --> 00:16:54,515
Notice that I've somewhat artificially
forced the request handler

296
00:16:54,548 --> 00:16:57,918
out of scope by introducing
an autoreleasepool.

297
00:16:57,951 --> 00:17:00,420
Now that the request handler
is out of scope,

298
00:17:00,454 --> 00:17:03,924
let's use the Quick Look Preview
support again on the results.

299
00:17:03,957 --> 00:17:06,693
Well, what do you know,
the overlays are still drawn,

300
00:17:06,727 --> 00:17:08,395
but the image is not available.

301
00:17:09,496 --> 00:17:12,966
This is something to keep in mind:
the image request handler that was used

302
00:17:13,000 --> 00:17:16,737
to generate the observations
must still be in scope somewhere

303
00:17:16,770 --> 00:17:21,108
in order for Quick Look Preview support
to use the original image for display.

304
00:17:21,141 --> 00:17:25,212
That is because the image request handler
is where your input image resides.

305
00:17:25,245 --> 00:17:28,348
Things will continue to work,
but the image will not be available.

306
00:17:28,382 --> 00:17:31,952
This Quick Look preview support
can be especially useful

307
00:17:31,985 --> 00:17:34,054
in an Xcode Playgrounds session,

308
00:17:34,087 --> 00:17:37,691
while doing quick experiments
to see how things work.

309
00:17:37,724 --> 00:17:40,160
Let's have a look at that now.

310
00:17:40,194 --> 00:17:44,865
Here we have a simple Playground set up
to analyze images for barcodes.

311
00:17:44,898 --> 00:17:47,935
Rather than go through this code,
let's just make a couple modifications

312
00:17:47,968 --> 00:17:50,537
and check out how it impacts the results.

313
00:17:50,571 --> 00:17:52,773
We'll start off by using revision 2

314
00:17:52,806 --> 00:17:56,743
on an image with two barcodes
of different symbologies.

315
00:17:56,777 --> 00:18:00,113
All the results at once are displayed
if we ask for all the results,

316
00:18:00,147 --> 00:18:03,150
and just the first result
is also displayed at the end.

317
00:18:04,718 --> 00:18:07,287
Notice that revision 2 has
a couple issues.

318
00:18:07,321 --> 00:18:10,257
First, it missed the first barcode.

319
00:18:10,290 --> 00:18:13,427
Also, it detected
the second barcode twice.

320
00:18:13,460 --> 00:18:15,329
And it gives you a line
through the barcode

321
00:18:15,362 --> 00:18:17,564
rather than a complete bounding box.

322
00:18:19,566 --> 00:18:23,637
What happens if we change
to revision 3 now, instead of revision 2?

323
00:18:26,106 --> 00:18:28,609
First of all, we detect both barcodes.

324
00:18:28,642 --> 00:18:32,346
And, instead of a line,
we are given complete bounding boxes.

325
00:18:33,313 --> 00:18:35,949
What is great about this
Quick Look Preview support

326
00:18:35,983 --> 00:18:39,553
is that we've removed the need for you
to write a variety of utility functions

327
00:18:39,586 --> 00:18:41,522
to visualize the results.

328
00:18:41,555 --> 00:18:44,625
They can be overlaid directly
on your images in the debugger

329
00:18:44,658 --> 00:18:46,560
or in an Xcode Playground.

330
00:18:49,796 --> 00:18:54,101
So that is Quick Look
Preview support in Vision.

331
00:18:54,134 --> 00:18:58,071
Now you can more easily know
which observation is which.

332
00:18:58,105 --> 00:19:00,407
Just be sure to keep
the image request handler in scope

333
00:19:00,440 --> 00:19:02,976
in order to use it with your input image,

334
00:19:03,010 --> 00:19:06,113
and hopefully the Xcode Playground support
will make live tuning

335
00:19:06,146 --> 00:19:08,482
of your Vision framework code much easier.

336
00:19:08,515 --> 00:19:11,418
We've covered some important updates
to Vision today.

337
00:19:11,451 --> 00:19:14,588
To quickly review, we've added
some great new revisions

338
00:19:14,621 --> 00:19:19,493
to text recognition,
barcode detection, and optical flow.

339
00:19:21,295 --> 00:19:25,999
As we continue to add updated revisions,
we will also be removing older ones,

340
00:19:26,033 --> 00:19:27,768
so keep your revisions up-to-date

341
00:19:27,801 --> 00:19:30,771
and use the latest
and greatest technology.

342
00:19:30,804 --> 00:19:34,241
We've also made debugging
Vision applications much easier this year

343
00:19:34,274 --> 00:19:36,643
with Quick Look Preview support.

344
00:19:36,677 --> 00:19:41,048
I hope you enjoyed this session,
and have a wonderful WWDC.  ♪ ♪

