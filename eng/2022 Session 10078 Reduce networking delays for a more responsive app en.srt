1
00:00:00,000 --> 00:00:03,003
♪ Mellow instrumental
hip-hip music ♪

2
00:00:03,003 --> 00:00:10,143
♪

3
00:00:10,143 --> 00:00:14,348
Hi, I am Vidhi Goel,
and in this video,

4
00:00:14,348 --> 00:00:19,052
I will talk about how to reduce
networking delays in your apps

5
00:00:19,052 --> 00:00:21,722
and make them more responsive.

6
00:00:21,722 --> 00:00:25,959
First, I will explain why
reducing latency is crucial

7
00:00:25,959 --> 00:00:29,563
in making your apps responsive.

8
00:00:29,563 --> 00:00:31,832
Next, I will go over a list

9
00:00:31,832 --> 00:00:34,902
of things that you can do
in your app

10
00:00:34,902 --> 00:00:40,240
and on your server to get rid of
unnecessary delays.

11
00:00:40,240 --> 00:00:44,177
Finally, I will show what
you can do to reduce delays

12
00:00:44,177 --> 00:00:47,948
in the network itself.

13
00:00:47,948 --> 00:00:51,451
Network latency
is the time it takes for data

14
00:00:51,451 --> 00:00:55,055
to get from
one endpoint to another.

15
00:00:55,055 --> 00:00:58,725
It determines how quickly
content can be delivered

16
00:00:58,725 --> 00:01:01,028
to your app.

17
00:01:01,028 --> 00:01:04,531
All apps that use networking
can be affected

18
00:01:04,531 --> 00:01:06,767
by slow network transactions

19
00:01:06,767 --> 00:01:11,138
that result in
a poor app experience.

20
00:01:11,138 --> 00:01:15,676
For example,
video calls can sometimes freeze

21
00:01:15,676 --> 00:01:20,380
or become laggy,
which can interrupt meetings.

22
00:01:20,380 --> 00:01:23,550
To address this,
people often call up

23
00:01:23,550 --> 00:01:27,421
their service provider
to upgrade their bandwidth,

24
00:01:27,421 --> 00:01:31,792
and yet,
the problem still exists.

25
00:01:31,792 --> 00:01:34,594
To get to the root cause
of this problem,

26
00:01:34,594 --> 00:01:38,265
you need to understand
how your app's packets

27
00:01:38,265 --> 00:01:40,767
travel in a network.

28
00:01:40,767 --> 00:01:44,805
When your app or framework
requests data from a server,

29
00:01:44,805 --> 00:01:48,675
packets are sent
out by the networking stack.

30
00:01:48,675 --> 00:01:52,079
It is often assumed
that the packets go directly

31
00:01:52,079 --> 00:01:56,216
to the server with no delays
in the network.

32
00:01:56,216 --> 00:01:59,920
But, in reality,
the slowest link of the network

33
00:01:59,920 --> 00:02:05,592
usually has a large queue
of packets to process.

34
00:02:05,592 --> 00:02:07,561
So, the packet from your app

35
00:02:07,561 --> 00:02:10,864
actually waits behind
this large queue

36
00:02:10,864 --> 00:02:16,036
until the packets ahead of it
are processed.

37
00:02:16,036 --> 00:02:19,840
This queuing at the slowest link
increases the duration

38
00:02:19,840 --> 00:02:24,745
of each round trip between
your app and your server.

39
00:02:24,745 --> 00:02:29,916
This problem is aggravated when
it takes multiple round trips

40
00:02:29,916 --> 00:02:34,855
to get the first response
for your app's request.

41
00:02:34,855 --> 00:02:39,159
For example, the time to get
the first response packet

42
00:02:39,159 --> 00:02:43,830
when using TLS 1.2 over TCP

43
00:02:43,830 --> 00:02:46,400
is the duration
of each round trip

44
00:02:46,400 --> 00:02:50,871
multiplied by four trips.

45
00:02:50,871 --> 00:02:54,541
Given that each round-trip time
is already inflated

46
00:02:54,541 --> 00:02:56,810
by queuing in the network,

47
00:02:56,810 --> 00:03:02,215
the resulting total time
is simply too long.

48
00:03:02,215 --> 00:03:05,152
There are two factors
that multiply together

49
00:03:05,152 --> 00:03:09,156
to determine
your app's responsiveness:

50
00:03:09,156 --> 00:03:15,128
the duration of each round trip
and the number of round trips.

51
00:03:15,128 --> 00:03:18,865
Reducing these will lower
your app's latency,

52
00:03:18,865 --> 00:03:24,071
and increase your app's
responsiveness.

53
00:03:24,071 --> 00:03:26,606
There was a study
examining the impact

54
00:03:26,606 --> 00:03:30,677
of increasing bandwidth
versus decreasing latency

55
00:03:30,677 --> 00:03:33,680
on page load time.

56
00:03:33,680 --> 00:03:37,484
In the first test,
latency is kept fixed

57
00:03:37,484 --> 00:03:44,224
and bandwidth is increased
incrementally from 1 to 10Mbps.

58
00:03:44,224 --> 00:03:49,129
At first, increasing the
bandwidth from 1 to 2Mbps

59
00:03:49,129 --> 00:03:53,767
reduces page load time
by almost 40 percent,

60
00:03:53,767 --> 00:03:56,036
which is great.

61
00:03:56,036 --> 00:04:01,441
But after 4Mbps,
each incremental increase

62
00:04:01,441 --> 00:04:07,180
results in almost no improvement
in the page load time.

63
00:04:07,180 --> 00:04:11,618
This is why apps can be slow
even after upgrading

64
00:04:11,618 --> 00:04:14,187
to Gigabit Internet.

65
00:04:14,187 --> 00:04:19,226
On the other hand, the results
for the latency test show

66
00:04:19,226 --> 00:04:24,097
that for every 20 millisecond
decrease in latency,

67
00:04:24,097 --> 00:04:28,869
there is a linear improvement
in page load time.

68
00:04:28,869 --> 00:04:33,473
And these results apply
to all network activity

69
00:04:33,473 --> 00:04:35,642
in your apps.

70
00:04:35,642 --> 00:04:40,413
Now, I will go over a few
simple actions you can take

71
00:04:40,413 --> 00:04:45,285
to reduce latency and make
your app more responsive.

72
00:04:45,285 --> 00:04:48,221
You can reduce
your app's latency significantly

73
00:04:48,221 --> 00:04:50,624
by adopting modern protocols

74
00:04:50,624 --> 00:04:57,097
such as IPv6, TLS 1.3
and HTTP/3.

75
00:04:57,097 --> 00:05:01,067
And all you need to do
is use URLSession

76
00:05:01,067 --> 00:05:04,371
and Network.framework APIs
in your app

77
00:05:04,371 --> 00:05:07,741
and these protocols
will be used automatically

78
00:05:07,741 --> 00:05:12,879
once they are enabled
on your server.

79
00:05:12,879 --> 00:05:17,350
Since its rollout,
we have seen a constant increase

80
00:05:17,350 --> 00:05:22,355
in HTTP/3 usage,
and within just a year,

81
00:05:22,355 --> 00:05:28,261
20 percent of web traffic
already uses HTTP/3,

82
00:05:28,261 --> 00:05:32,499
and it continues to grow.

83
00:05:32,499 --> 00:05:37,604
Comparing Safari traffic
for different HTTP versions,

84
00:05:37,604 --> 00:05:42,876
HTTP/3 is the fastest
of them all.

85
00:05:42,876 --> 00:05:47,847
HTTP/3 requests take
a little over half the time

86
00:05:47,847 --> 00:05:50,550
as compared to HTTP/1,

87
00:05:50,550 --> 00:05:53,720
when looking at median
request completion time

88
00:05:53,720 --> 00:05:57,557
as a multiple
of round-trip time.

89
00:05:57,557 --> 00:06:03,663
This means your app's requests
will complete much faster.

90
00:06:03,663 --> 00:06:07,100
When a device moves
from Wi-Fi to cellular,

91
00:06:07,100 --> 00:06:11,071
it takes time to reestablish
new connections

92
00:06:11,071 --> 00:06:15,275
and that can make
your application stall.

93
00:06:15,275 --> 00:06:20,614
Using connection migration
eliminates those stalls.

94
00:06:20,614 --> 00:06:24,684
To opt in, set the
multipathServiceType property

95
00:06:24,684 --> 00:06:28,888
to .handover on your URLSession
configuration,

96
00:06:28,888 --> 00:06:32,325
or on your NWParameters.

97
00:06:32,325 --> 00:06:39,232
Enable this option and make sure
it works with your app.

98
00:06:39,232 --> 00:06:44,337
If you design your own protocol
that uses UDP directly,

99
00:06:44,337 --> 00:06:49,809
iOS 16 and macOS Ventura
introduce a better way

100
00:06:49,809 --> 00:06:52,445
to send datagrams.

101
00:06:52,445 --> 00:06:57,584
QUIC datagrams provide
many benefits over plain UDP,

102
00:06:57,584 --> 00:07:00,787
the most important being
that QUIC datagrams

103
00:07:00,787 --> 00:07:03,023
react to congestion
in the network

104
00:07:03,023 --> 00:07:07,894
which keeps the round-trip time
low and reduces packet loss.

105
00:07:07,894 --> 00:07:10,030
To opt in on the client,

106
00:07:10,030 --> 00:07:13,733
set isDatagram to true
on your QUIC options

107
00:07:13,733 --> 00:07:19,606
and set the maximum datagram
frame size you want to use.

108
00:07:19,606 --> 00:07:22,409
After creating
the datagram flow,

109
00:07:22,409 --> 00:07:27,714
you can send and receive on it
just like any other QUIC stream.

110
00:07:27,714 --> 00:07:30,216
Now you know
what to do in your app

111
00:07:30,216 --> 00:07:32,519
to reduce latency.

112
00:07:32,519 --> 00:07:36,156
Next, I will explain
how servers impact

113
00:07:36,156 --> 00:07:39,392
your app's responsiveness.

114
00:07:39,392 --> 00:07:42,762
Despite often running
on top-of-the-line hardware,

115
00:07:42,762 --> 00:07:46,666
it is possible that your server
actually becomes the reason

116
00:07:46,666 --> 00:07:49,302
for slowness in your app.

117
00:07:49,302 --> 00:07:54,307
We introduced the network
quality tool in macOS Monterey,

118
00:07:54,307 --> 00:07:56,443
and you can use this tool

119
00:07:56,443 --> 00:08:00,480
to measure buffer bloat in
your service provider's network

120
00:08:00,480 --> 00:08:03,183
as well as on your server.

121
00:08:03,183 --> 00:08:06,886
You need to configure your
server to act as a destination

122
00:08:06,886 --> 00:08:09,622
for the network quality tool.

123
00:08:09,622 --> 00:08:14,394
Once you have done that,
run the networkQuality tool,

124
00:08:14,394 --> 00:08:17,597
first against
Apple's default server

125
00:08:17,597 --> 00:08:22,569
and then against
your own configured server.

126
00:08:22,569 --> 00:08:26,406
If the tool scores well
using the default server,

127
00:08:26,406 --> 00:08:30,543
but not so well when talking
to your own server,

128
00:08:30,543 --> 00:08:33,580
there may be room to improve
the responsiveness

129
00:08:33,580 --> 00:08:35,949
of your server.

130
00:08:35,949 --> 00:08:40,887
Now, I will show you an example
where we used this technique

131
00:08:40,887 --> 00:08:45,792
to improve something that all
of you are doing right now --

132
00:08:45,792 --> 00:08:48,695
streaming video.

133
00:08:50,397 --> 00:08:52,399
You may have had the experience

134
00:08:52,399 --> 00:08:55,869
where you skip ahead
to a different place in a video

135
00:08:55,869 --> 00:09:01,808
and you end up waiting
a long time while it rebuffers.

136
00:09:01,808 --> 00:09:05,512
So, we investigated
the reason for this slowness

137
00:09:05,512 --> 00:09:08,281
in random access.

138
00:09:08,281 --> 00:09:10,583
We used the network quality tool

139
00:09:10,583 --> 00:09:13,820
to test the behavior
of a streaming server

140
00:09:13,820 --> 00:09:18,758
and we found that the
responsiveness score was poor.

141
00:09:18,758 --> 00:09:23,897
On the right side,
I streamed a WWDC video.

142
00:09:23,897 --> 00:09:26,900
Then, I skipped ahead
in the video.

143
00:09:26,900 --> 00:09:28,835
The screen
didn't display anything

144
00:09:28,835 --> 00:09:32,205
while the video rebuffered.

145
00:09:32,205 --> 00:09:36,309
After a few seconds,
the video showed up.

146
00:09:36,309 --> 00:09:38,678
With the help of detailed output

147
00:09:38,678 --> 00:09:41,948
from the network quality tool
on macOS,

148
00:09:41,948 --> 00:09:46,920
we found that there was
huge queuing at the server.

149
00:09:46,920 --> 00:09:52,225
So we took a look at
the server configuration.

150
00:09:52,225 --> 00:09:58,865
Specifically we looked at TCP,
TLS, and HTTP buffer sizes,

151
00:09:58,865 --> 00:10:09,142
which were configured to 4MB,
256KB, and 4MB, respectively.

152
00:10:09,142 --> 00:10:14,013
The buffers were huge
because RAM is plentiful.

153
00:10:14,013 --> 00:10:17,450
But just because
some buffering is good,

154
00:10:17,450 --> 00:10:22,155
doesn't always mean
that more buffering is better.

155
00:10:22,155 --> 00:10:27,427
Our responsiveness measurements
highlighted this exact issue --

156
00:10:27,427 --> 00:10:32,065
a newly generated packet
was queued behind stale data

157
00:10:32,065 --> 00:10:34,167
in these large buffers,

158
00:10:34,167 --> 00:10:37,237
and this created a lot
of additional delay

159
00:10:37,237 --> 00:10:41,241
in delivering
the most recent packet.

160
00:10:41,241 --> 00:10:47,981
So, we reduced the buffer size
to 256KB for HTTP,

161
00:10:47,981 --> 00:10:54,854
16KB for TLS,
and 128KB for TCP.

162
00:10:57,690 --> 00:11:01,327
This is the config file
for Apache Traffic Server

163
00:11:01,327 --> 00:11:05,398
which shows the options
that were configured.

164
00:11:05,398 --> 00:11:10,870
TCP not-sent low-water mark
was set to 128KB

165
00:11:10,870 --> 00:11:16,976
along with other options that
were enabled to lower buffering.

166
00:11:16,976 --> 00:11:21,748
For TLS, we enabled
dynamic record sizes

167
00:11:21,748 --> 00:11:26,753
and for HTTP/2,
we reduced the low-water mark

168
00:11:26,753 --> 00:11:28,788
and buffer block size.

169
00:11:28,788 --> 00:11:31,724
We recommend
using these configurations

170
00:11:31,724 --> 00:11:34,627
for your Apache Traffic Server,

171
00:11:34,627 --> 00:11:38,031
and if you are using
a different web server,

172
00:11:38,031 --> 00:11:41,301
look for its equivalent options.

173
00:11:41,301 --> 00:11:43,736
After making these changes,

174
00:11:43,736 --> 00:11:47,540
we ran the network
quality tool again.

175
00:11:47,540 --> 00:11:52,645
And this time we got
a high RPM score!

176
00:11:52,645 --> 00:11:56,249
On the right,
I streamed the same video,

177
00:11:56,249 --> 00:11:58,918
but this time
when I skipped ahead,

178
00:11:58,918 --> 00:12:03,590
the video resumed instantly.

179
00:12:03,590 --> 00:12:07,594
By getting rid of unnecessary
queuing at the server,

180
00:12:07,594 --> 00:12:11,698
we made random access
much more responsive.

181
00:12:11,698 --> 00:12:15,868
Regardless of how your app
uses networking,

182
00:12:15,868 --> 00:12:20,840
these changes on your server can
make your app more responsive

183
00:12:20,840 --> 00:12:24,877
and deliver
a better user experience.

184
00:12:24,877 --> 00:12:29,916
That's how to improve your app
and update your server.

185
00:12:29,916 --> 00:12:34,854
There is a third factor that
affects responsiveness greatly;

186
00:12:34,854 --> 00:12:37,223
the network itself.

187
00:12:37,223 --> 00:12:41,527
Apple introduced the
network quality tool in iOS 15

188
00:12:41,527 --> 00:12:44,364
and macOS Monterey.

189
00:12:44,364 --> 00:12:48,401
Since then, others have used
the same methodology

190
00:12:48,401 --> 00:12:52,972
to develop
network quality tests.

191
00:12:52,972 --> 00:12:56,976
Waveform has launched
a Bufferbloat test.

192
00:12:56,976 --> 00:12:59,245
There's an open source
implementation

193
00:12:59,245 --> 00:13:03,983
of the responsiveness test,
written in Go.

194
00:13:03,983 --> 00:13:07,854
And Ookla has added
a responsiveness measurement

195
00:13:07,854 --> 00:13:11,524
to their Speedtest app.

196
00:13:11,524 --> 00:13:15,395
Ookla's app shows
round trip time in milliseconds,

197
00:13:15,395 --> 00:13:18,965
and if you divide 60,000
by that number,

198
00:13:18,965 --> 00:13:24,103
you get the number of
round trips per minute, or RPM.

199
00:13:24,103 --> 00:13:26,673
You can use these tools
to measure

200
00:13:26,673 --> 00:13:30,943
how well your own network
is performing.

201
00:13:30,943 --> 00:13:34,113
The best way to understand
delays in a network

202
00:13:34,113 --> 00:13:37,750
is with a delay-sensitive
application.

203
00:13:37,750 --> 00:13:41,054
So, I will show you
my screen sharing experience

204
00:13:41,054 --> 00:13:43,823
to a remote machine.

205
00:13:43,823 --> 00:13:45,558
I set up network conditions

206
00:13:45,558 --> 00:13:48,828
to mimic a representative
access network,

207
00:13:48,828 --> 00:13:54,233
with traffic from other devices
sharing that network.

208
00:13:54,233 --> 00:13:59,105
Here, I logged on to my remote
machine using Screen Sharing.

209
00:14:01,174 --> 00:14:04,377
I clicked on different
Finder menus

210
00:14:04,377 --> 00:14:09,148
but the display of each menu
was very sluggish.

211
00:14:09,148 --> 00:14:12,552
To check how much
this interaction was delayed,

212
00:14:12,552 --> 00:14:16,823
I launched an app that displays
time on my local machine,

213
00:14:16,823 --> 00:14:21,360
and I launched the same app
on my remote machine.

214
00:14:21,360 --> 00:14:25,832
Even though time on these
computers is synchronized,

215
00:14:25,832 --> 00:14:29,702
my remote screen
didn't update regularly

216
00:14:29,702 --> 00:14:34,907
and showed time delayed
by a few seconds.

217
00:14:34,907 --> 00:14:37,343
The reason
for this delayed update

218
00:14:37,343 --> 00:14:39,979
was the presence
of a large queue

219
00:14:39,979 --> 00:14:42,782
at the slowest link
of the network

220
00:14:42,782 --> 00:14:45,485
and packets from
the Screen Sharing app

221
00:14:45,485 --> 00:14:48,821
were stuck in this large queue.

222
00:14:50,823 --> 00:14:53,259
To solve this queuing issue,

223
00:14:53,259 --> 00:14:56,028
Apple is working with
the networking community

224
00:14:56,028 --> 00:14:59,899
on a new technology called L4S.

225
00:14:59,899 --> 00:15:06,773
It is available as a beta
in iOS 16 and macOS Ventura.

226
00:15:06,773 --> 00:15:10,109
L4S reduces
queuing delay significantly

227
00:15:10,109 --> 00:15:14,747
and also achieves
zero congestion loss.

228
00:15:14,747 --> 00:15:17,550
To keep a consistently
short queue,

229
00:15:17,550 --> 00:15:20,419
the network explicitly
signals congestion

230
00:15:20,419 --> 00:15:22,622
instead of dropping packets,

231
00:15:22,622 --> 00:15:25,491
and the sender adjusts
its sending rate

232
00:15:25,491 --> 00:15:29,529
based on the congestion feedback
from the network.

233
00:15:29,529 --> 00:15:34,133
This makes it possible to keep
very low queuing in the network

234
00:15:34,133 --> 00:15:37,069
without any packet loss,

235
00:15:37,069 --> 00:15:41,874
and that will make your app
highly responsive.

236
00:15:41,874 --> 00:15:48,281
Now, let's look at how L4S
improved Screen Sharing.

237
00:15:48,281 --> 00:15:52,585
Here, I used the same machines
and the same network

238
00:15:52,585 --> 00:15:57,557
except this time,
I enabled L4S.

239
00:15:57,557 --> 00:16:00,359
When I clicked on
different Finder menus,

240
00:16:00,359 --> 00:16:02,895
they opened immediately.

241
00:16:02,895 --> 00:16:06,899
I launched the Time app
on both the machines.

242
00:16:06,899 --> 00:16:09,769
And now, time on both
the remote screen

243
00:16:09,769 --> 00:16:16,943
and the local machine
is almost perfectly in sync.

244
00:16:16,943 --> 00:16:21,380
This technology is not just
for screen sharing.

245
00:16:21,380 --> 00:16:25,484
L4S improves
all of today's apps,

246
00:16:25,484 --> 00:16:28,120
and opens the door
for future apps

247
00:16:28,120 --> 00:16:31,958
that wouldn't even
be possible today.

248
00:16:31,958 --> 00:16:36,229
This chart plots the observed
average round trip time

249
00:16:36,229 --> 00:16:39,165
of packets from
the Screen Sharing app

250
00:16:39,165 --> 00:16:41,534
which was running
concurrently with traffic

251
00:16:41,534 --> 00:16:46,005
from other devices
sharing the same network.

252
00:16:46,005 --> 00:16:49,508
Comparing classic queuing
versus L4S

253
00:16:49,508 --> 00:16:52,712
shows that there
is a massive reduction

254
00:16:52,712 --> 00:16:56,415
in round trip time with L4S.

255
00:16:56,415 --> 00:17:00,386
This is the primary reason
for the dramatic improvement

256
00:17:00,386 --> 00:17:05,157
in my screen-sharing experience.

257
00:17:05,157 --> 00:17:11,130
Test your app that uses
HTTP/3 or QUIC with L4S.

258
00:17:11,130 --> 00:17:16,969
You can enable L4S in iOS 16
inside Developer settings

259
00:17:16,969 --> 00:17:22,808
or on macOS Ventura
via a defaults write.

260
00:17:22,808 --> 00:17:25,611
To test using a Linux server,

261
00:17:25,611 --> 00:17:30,082
your QUIC implementation
needs to support accurate ECN

262
00:17:30,082 --> 00:17:34,520
and a scalable congestion
control algorithm.

263
00:17:34,520 --> 00:17:36,622
To ensure that you are ready

264
00:17:36,622 --> 00:17:40,226
when L4S-capable networks
are deployed,

265
00:17:40,226 --> 00:17:44,263
test your app
for compatibility with L4S,

266
00:17:44,263 --> 00:17:50,503
and provide feedback with any
issues you might encounter.

267
00:17:50,503 --> 00:17:54,440
Now you know that
reducing latency is crucial

268
00:17:54,440 --> 00:17:58,044
to improve your app's
responsiveness.

269
00:17:58,044 --> 00:18:02,381
So, adopt HTTP/3 and QUIC,

270
00:18:02,381 --> 00:18:04,717
to reduce the number
of round trips

271
00:18:04,717 --> 00:18:09,822
and for faster delivery
of content to your app.

272
00:18:09,822 --> 00:18:13,459
Eliminate unnecessary queuing
on your server

273
00:18:13,459 --> 00:18:17,363
to provide a more
responsive interaction.

274
00:18:17,363 --> 00:18:21,701
Test your app's compatibility
with L4S by enabling it

275
00:18:21,701 --> 00:18:25,938
in Developer settings
and provide feedback.

276
00:18:25,938 --> 00:18:29,041
And finally,
talk to your server provider

277
00:18:29,041 --> 00:18:32,878
about enabling L4S support.

278
00:18:32,878 --> 00:18:34,747
Thanks for watching!

279
00:18:34,747 --> 00:18:39,118
♪

