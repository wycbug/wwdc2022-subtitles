1
00:00:00,501 --> 00:00:08,509
♪ ♪

2
00:00:09,643 --> 00:00:13,647
Ken Greenebaum: Hi everyone!
Welcome to WWDC 2022.

3
00:00:13,680 --> 00:00:15,082
My name is Ken Greenebaum,

4
00:00:15,115 --> 00:00:18,352
and I'm with the Color and Display
Technologies team at Apple.

5
00:00:18,385 --> 00:00:21,388
We are thrilled to have three EDR talks
this year.

6
00:00:21,421 --> 00:00:25,559
Hope you've had an opportunity
to watch "Explore EDR on iOS,"

7
00:00:25,592 --> 00:00:29,263
where we announced
EDR API support for iOS,

8
00:00:29,296 --> 00:00:33,901
as well as "Display EDR content
with Core Image, Metal, and SwiftUI."

9
00:00:33,934 --> 00:00:37,838
Some of you may have also watched
my EDR talk last year,

10
00:00:37,871 --> 00:00:40,240
where we demonstrated
how to use AVPlayer

11
00:00:40,274 --> 00:00:43,177
to play back HDR video, using EDR.

12
00:00:44,178 --> 00:00:46,246
In this talk we're gonna go deeper,

13
00:00:46,280 --> 00:00:49,550
and explore how to use
Core Media interfaces to provide,

14
00:00:49,583 --> 00:00:51,785
not only EDR playback,

15
00:00:51,818 --> 00:00:54,821
but also how to decode
and playback HDR video,

16
00:00:54,855 --> 00:00:57,591
into your own EDR layers or views.

17
00:00:59,326 --> 00:01:02,396
Then we'll continue beyond
simply playing back content,

18
00:01:02,429 --> 00:01:05,732
to show how to access the decoded
video frames in real time,

19
00:01:05,766 --> 00:01:08,101
via Core Video's display link,

20
00:01:08,135 --> 00:01:11,271
send those frames to CoreImage Filters,
or a Metal Shader,

21
00:01:11,305 --> 00:01:13,473
to add color management, visual effects,

22
00:01:13,507 --> 00:01:15,909
or apply other signal processing,

23
00:01:15,943 --> 00:01:20,247
and finally, plumb
the resulting frames to Metal to render.

24
00:01:20,280 --> 00:01:24,751
We're going to start by reviewing
the EDR compatible video media frameworks,

25
00:01:24,785 --> 00:01:28,522
to help you decide which best matches
your application's requirements.

26
00:01:30,057 --> 00:01:32,226
Next we will briefly discuss
the high level AVKit

27
00:01:32,259 --> 00:01:35,028
and AVFoundation frameworks,

28
00:01:35,062 --> 00:01:37,764
that can do all of the work
of playing HDR video,

29
00:01:37,798 --> 00:01:40,701
if your application
requires straight forward playback.

30
00:01:42,302 --> 00:01:44,872
And finally,
we'll discuss best practices

31
00:01:44,905 --> 00:01:48,175
for using decoded video frames,
with Core Video and Metal,

32
00:01:48,208 --> 00:01:51,979
in your EDR playback, editing,
or image processing engine.

33
00:01:54,781 --> 00:01:58,752
Let's begin by taking a quick survey
of Apple's video frameworks;

34
00:01:58,785 --> 00:02:01,355
Starting with
the highest level interfaces;

35
00:02:01,388 --> 00:02:03,323
which are the easiest to use;

36
00:02:03,357 --> 00:02:06,827
and continuing to lower level frameworks
that offer more opportunities,

37
00:02:06,860 --> 00:02:10,364
at the expense of adding
complexity to your code.

38
00:02:10,397 --> 00:02:13,467
It is best to use
the highest level framework possible

39
00:02:13,500 --> 00:02:17,271
to take advantage of the optimizations
provided automatically for you.

40
00:02:17,304 --> 00:02:20,440
This will get us ready to dive into
the body of the talk,

41
00:02:20,474 --> 00:02:23,177
where we will be exploring
a number of scenarios,

42
00:02:23,210 --> 00:02:24,945
from simple EDR playback

43
00:02:24,978 --> 00:02:27,981
to more sophisticated plumbing
of decoded video frames

44
00:02:28,015 --> 00:02:31,652
to CoreImage or Metal
for real time processing.

45
00:02:31,685 --> 00:02:34,154
At the highest level, there is AVKit.

46
00:02:34,188 --> 00:02:38,292
With AVKit you can create
user interfaces for media playback;

47
00:02:38,325 --> 00:02:41,562
complete with transport controls,
chapter navigation,

48
00:02:41,595 --> 00:02:43,030
Picture in Picture support,

49
00:02:43,063 --> 00:02:45,632
and display of subtitles
and closed captions.

50
00:02:45,666 --> 00:02:49,136
AVKit can playback HDR content as EDR,

51
00:02:49,169 --> 00:02:52,840
as we will demonstrate
using AVPlayerViewController.

52
00:02:52,873 --> 00:02:57,377
However, if your application requires
further processing of video frames,

53
00:02:57,411 --> 00:02:59,179
you will have to use a media framework

54
00:02:59,213 --> 00:03:01,815
that can give you more control
over your pipeline.

55
00:03:01,849 --> 00:03:04,618
Next there is AVFoundation.

56
00:03:04,651 --> 00:03:07,688
AVFoundation is
the full-featured framework

57
00:03:07,721 --> 00:03:12,860
for working with time based audio visual
media on Apple Platforms.

58
00:03:12,893 --> 00:03:16,096
Using AVFoundation,
you can easily play, create,

59
00:03:16,129 --> 00:03:19,032
and edit QuickTime movies
and MPEG 4 files,

60
00:03:19,066 --> 00:03:20,634
play HLS streams,

61
00:03:20,667 --> 00:03:24,271
and build powerful media functionality
into your apps.

62
00:03:24,304 --> 00:03:26,306
We'll be exploring use of AVPlayer

63
00:03:26,340 --> 00:03:30,577
and the related AVPlayerLayer interface
in this talk.

64
00:03:30,611 --> 00:03:35,182
Core Video is a framework that provides
a pipeline model for digital video.

65
00:03:35,215 --> 00:03:37,417
It simplifies the way
you work with videos

66
00:03:37,451 --> 00:03:40,220
by partitioning the process
into discrete steps.

67
00:03:40,254 --> 00:03:43,790
Core Video also makes it easier
for you to access and manipulate

68
00:03:43,824 --> 00:03:46,527
individual frames,
without having to worry about

69
00:03:46,560 --> 00:03:48,262
translating between data types

70
00:03:48,295 --> 00:03:51,465
or worrying about display synchronization.

71
00:03:51,498 --> 00:03:53,534
We'll be demonstrating
use of DisplayLink,

72
00:03:53,567 --> 00:03:56,036
and CVPixelBuffer's with Core Image.

73
00:03:56,069 --> 00:03:59,239
And CVMetalTextureCache, with Metal.

74
00:03:59,273 --> 00:04:01,341
Next there is Video Toolbox.

75
00:04:01,375 --> 00:04:04,011
This is a low-level framework
that provides direct access

76
00:04:04,044 --> 00:04:06,446
to hardware encoders and decoders.

77
00:04:06,480 --> 00:04:10,784
Video Toolbox provides services for
video compression and decompression,

78
00:04:10,817 --> 00:04:13,253
and for conversion between
raster image formats

79
00:04:13,287 --> 00:04:15,556
stored in Core Video pixel buffers.

80
00:04:15,589 --> 00:04:19,159
VTDecompressionSession
is a powerful low-level interface

81
00:04:19,193 --> 00:04:21,161
that is outside of the scope of this talk,

82
00:04:21,195 --> 00:04:24,631
but advanced developers
might want to investigate further.

83
00:04:24,665 --> 00:04:26,800
And finally, there is Core Media.

84
00:04:26,834 --> 00:04:30,537
This framework defines
the media pipeline used by AVFoundation,

85
00:04:30,571 --> 00:04:33,373
and the other high-level media frameworks.

86
00:04:33,407 --> 00:04:37,110
You can always use Core Media's
low-level data types and interfaces

87
00:04:37,144 --> 00:04:39,546
to efficiently process media samples

88
00:04:39,580 --> 00:04:41,648
and manage queues of media data.

89
00:04:41,682 --> 00:04:45,319
In the remainder of this talk
we will demonstrate how and when

90
00:04:45,352 --> 00:04:47,688
to use these frameworks in your app.

91
00:04:47,721 --> 00:04:51,225
First, how to use AVKit
and AVFoundation

92
00:04:51,258 --> 00:04:55,262
to easily playback HDR video
rendered as EDR.

93
00:04:55,295 --> 00:04:59,399
Then a series of more sophisticated
applications of AVPlayer:

94
00:04:59,433 --> 00:05:01,235
to render to your own layer,

95
00:05:01,268 --> 00:05:05,105
to access individually decoded frames
via CADisplayLink

96
00:05:05,138 --> 00:05:09,243
and send the resulting CVPixelBuffers
to Core Image for processing,

97
00:05:09,276 --> 00:05:13,247
and finally, accessing the decoded frames
as Metal textures

98
00:05:13,280 --> 00:05:15,182
via the CVMetalTextureCache

99
00:05:15,215 --> 00:05:17,818
for processing and rendering in Metal.

100
00:05:17,851 --> 00:05:22,322
Now that we have an overview of
the video media layer on Apple platforms,

101
00:05:22,356 --> 00:05:26,260
we'll focus on AVKit
and AVFoundation frameworks.

102
00:05:26,293 --> 00:05:28,795
Let's get things started
by first discussing playback

103
00:05:28,829 --> 00:05:30,597
of your HDR video content

104
00:05:30,631 --> 00:05:33,433
using AVFoundation's AVPlayer interface.

105
00:05:33,467 --> 00:05:35,969
An AVPlayer is a controller object,

106
00:05:36,003 --> 00:05:39,773
used to manage the playback
and timing of a media asset.

107
00:05:39,806 --> 00:05:44,244
The AVPlayer interface can be used for
high-performance playback of HDR video,

108
00:05:44,278 --> 00:05:47,314
automatically rendering
the result as EDR when possible.

109
00:05:48,282 --> 00:05:51,785
With AVPlayer, you can play local,
and remote file based media,

110
00:05:51,818 --> 00:05:53,820
such as QuickTime movies;

111
00:05:53,854 --> 00:05:58,025
as well as streaming media,
served using HLS.

112
00:05:58,058 --> 00:06:02,729
Essentially, AVPlayer is used to play
one media asset at a time.

113
00:06:02,763 --> 00:06:07,167
You can reuse the player instance
to serially play additional media assets,

114
00:06:07,201 --> 00:06:12,272
or even create multiple instances to play
more than one asset simultaneously,

115
00:06:12,306 --> 00:06:17,277
but AVPlayer manages the playback
of only a single media asset at a time.

116
00:06:17,311 --> 00:06:21,348
AVFoundation framework also provides
a subclass of AVPlayer

117
00:06:21,381 --> 00:06:24,218
called AVQueuePlayer
that you can use to create

118
00:06:24,251 --> 00:06:29,556
and manage the queuing and playing
of sequential HDR media assets.

119
00:06:29,590 --> 00:06:33,193
If your application requires
simple playback of HDR video media

120
00:06:33,227 --> 00:06:37,164
rendered to EDR, then AVPlayer
with AVPlayerViewController,

121
00:06:37,197 --> 00:06:39,233
may be the best approach.

122
00:06:39,266 --> 00:06:41,668
Use AVPlayer with AVPlayerLayer

123
00:06:41,702 --> 00:06:45,405
to playback your own views
on iOS or macOS.

124
00:06:46,640 --> 00:06:49,810
These are the most straightforward ways
of using AVPlayer.

125
00:06:49,843 --> 00:06:51,845
Let's look at examples of both.

126
00:06:51,879 --> 00:06:55,916
First we will look how you can
use AVFoundation's AVPlayer interface,

127
00:06:55,949 --> 00:06:59,853
in conjunction with AVKit's
AVPlayer View Controller.

128
00:06:59,887 --> 00:07:03,957
Here, we start by instantiating AVPlayer
from the media's URL.

129
00:07:06,493 --> 00:07:09,630
Next we create
an AVPlayerViewController,

130
00:07:09,663 --> 00:07:12,332
then set the player property
of our viewer controller

131
00:07:12,366 --> 00:07:15,235
to the player we just created
from the media's URL.

132
00:07:18,005 --> 00:07:23,076
And present the view controller modally
to start playback of the video.

133
00:07:23,110 --> 00:07:25,579
AVKit manages all the details for you

134
00:07:25,612 --> 00:07:29,116
and will automatically play back
HDR Video as EDR

135
00:07:29,149 --> 00:07:31,852
on displays supporting EDR.

136
00:07:31,885 --> 00:07:35,656
As I mentioned, some applications will
need to play back HDR video media

137
00:07:35,689 --> 00:07:37,491
into their own view.

138
00:07:37,524 --> 00:07:42,563
Let's look at how to accomplish this
using AVPlayer with AVPlayerLayer.

139
00:07:42,596 --> 00:07:46,266
To play HDR video media as EDR
in your own view,

140
00:07:46,300 --> 00:07:51,371
we again start by creating an AVPlayer
with the media's URL.

141
00:07:51,405 --> 00:07:54,474
However this time
we instantiate an AVPlayerLayer

142
00:07:54,508 --> 00:07:57,444
with the player we just created.

143
00:07:57,477 --> 00:08:00,013
Next we need to set the bounds
on the player layer,

144
00:08:00,047 --> 00:08:02,649
which we get from the view.

145
00:08:02,683 --> 00:08:05,385
Now that the player layer
has the bounds from the view,

146
00:08:05,419 --> 00:08:10,023
we can add the player layer
as a sublayer to the view.

147
00:08:10,057 --> 00:08:12,926
Finally,
to play back the HDR video media,

148
00:08:12,960 --> 00:08:15,596
we call AVPlayer's play method.

149
00:08:15,629 --> 00:08:19,333
That's all that is needed to play back
HDR video media as EDR

150
00:08:19,366 --> 00:08:23,971
in your own layer
using AVPlayer and AVPlayerLayer.

151
00:08:24,004 --> 00:08:26,139
We just explored
the two most straightforward

152
00:08:26,173 --> 00:08:29,443
HDR video playback workflows
using AVPlayer.

153
00:08:29,476 --> 00:08:34,014
However, many applications require more
than simple media playback.

154
00:08:35,282 --> 00:08:38,852
For example, an application
might require image processing,

155
00:08:38,886 --> 00:08:43,223
such as color grading or chroma keying
to be applied to the video.

156
00:08:43,257 --> 00:08:48,095
Let's explore a workflow that gets
decoded video frames from AVPlayer,

157
00:08:48,128 --> 00:08:51,865
applies Core Image filters
or Metal shaders in real time,

158
00:08:51,899 --> 00:08:55,369
and renders the results as EDR.

159
00:08:55,402 --> 00:08:59,072
We will be demonstrating how to use
AVPlayer and the AVPlayerItem

160
00:08:59,106 --> 00:09:03,010
to decode EDR frames
from your HDR video media,

161
00:09:03,043 --> 00:09:06,947
access the decoded frames
from the Core Video display link,

162
00:09:06,980 --> 00:09:11,919
send the resulting pixel buffers
to Core Image or Metal for processing,

163
00:09:11,952 --> 00:09:14,354
then render the results
in a CAMetalLayer

164
00:09:14,388 --> 00:09:17,958
as EDR on displays with EDR support.

165
00:09:17,991 --> 00:09:20,360
With this in mind,
let's first demonstrate

166
00:09:20,394 --> 00:09:23,330
setting a few key properties
on the CAMetalLayer,

167
00:09:23,363 --> 00:09:28,202
which are required to ensure HDR media
will render correctly as EDR.

168
00:09:28,235 --> 00:09:30,337
First we need to get the CAMetalLayer

169
00:09:30,370 --> 00:09:34,274
that we will be rendering
the HDR video content to.

170
00:09:34,308 --> 00:09:37,177
On that layer
we opt into EDR by setting

171
00:09:37,211 --> 00:09:40,614
the wantsExtendedDynamicRangeContent
flag to true.

172
00:09:42,883 --> 00:09:48,222
Please be sure to use a pixel format that
supports Extended Dynamic Range content.

173
00:09:48,255 --> 00:09:52,459
For the AVPlayer example that follows,
we will set the CAMetalLayer to use

174
00:09:52,492 --> 00:09:54,528
a half float pixel format,

175
00:09:54,561 --> 00:09:58,265
however a ten bit format
used in conjunction with a PQ

176
00:09:58,298 --> 00:10:01,535
or HLG transfer function would also work.

177
00:10:01,568 --> 00:10:03,904
To avoid limiting the result to SDR,

178
00:10:03,937 --> 00:10:06,707
we also need to set the layer
to an EDR compatible

179
00:10:06,740 --> 00:10:08,909
extended range color space.

180
00:10:10,577 --> 00:10:13,580
In our examples we will be setting
the half float metal texture

181
00:10:13,614 --> 00:10:18,151
to the extended linear display P3
color space.

182
00:10:18,185 --> 00:10:20,487
We just scratched the surface
regarding EDR,

183
00:10:20,521 --> 00:10:23,090
color spaces, and pixel buffer formats.

184
00:10:23,123 --> 00:10:25,626
You might want to check out
my session from last year,

185
00:10:25,659 --> 00:10:27,528
"HDR rendering with EDR,"

186
00:10:27,561 --> 00:10:31,999
as well as this year's "EDR on iOS,"
for more details.

187
00:10:33,300 --> 00:10:36,303
Now that we have set the basic properties
on the CAMetalLayer,

188
00:10:36,336 --> 00:10:39,873
let's continue the demonstration
by adding real time image processing

189
00:10:39,907 --> 00:10:42,376
using a Core Image, or Metal shader.

190
00:10:42,409 --> 00:10:45,412
We'll be using a display link
in conjunction with AVPlayer

191
00:10:45,445 --> 00:10:48,415
to access the decoded video frames
in real time.

192
00:10:49,449 --> 00:10:53,820
For this workflow, you start by creating
an AVPlayer from an AVPlayerItem.

193
00:10:53,854 --> 00:10:57,291
Next, you instantiate
an AVPlayerItemVideoOutput,

194
00:10:57,324 --> 00:11:02,496
configured with appropriate pixel buffer
format and color space for EDR.

195
00:11:02,529 --> 00:11:05,399
Then you create
and configure a Display link.

196
00:11:05,432 --> 00:11:08,669
And lastly, you run the Display link
to get the pixel buffers

197
00:11:08,702 --> 00:11:11,805
to Core Image or Metal for processing.

198
00:11:11,839 --> 00:11:16,310
We will demonstrate a CADisplayLink
as is used on iOS.

199
00:11:16,343 --> 00:11:21,381
Please use the equivalent CVDisplayLink
interface when developing for macOS.

200
00:11:21,415 --> 00:11:24,084
This time we choose
to create an AVPlayerItem

201
00:11:24,117 --> 00:11:26,486
from our media's URL,

202
00:11:26,520 --> 00:11:32,326
and instantiate an AVPlayer with
the AVPlayerItem that we just created.

203
00:11:32,359 --> 00:11:35,562
Now we create a pair of dictionaries
to specify the color space

204
00:11:35,596 --> 00:11:38,999
and pixel buffer format
of the decoded frames.

205
00:11:39,032 --> 00:11:41,635
The first dictionary,
videoColorProperties,

206
00:11:41,668 --> 00:11:45,172
is where the color space
and transfer function are specified.

207
00:11:45,205 --> 00:11:48,475
In this example we request
the Display P3 colorspace,

208
00:11:48,509 --> 00:11:51,945
which corresponds to the color space
of most Apple displays,

209
00:11:51,979 --> 00:11:55,115
and the linear transfer function
which allows AVFoundation

210
00:11:55,148 --> 00:11:58,852
to maintain the extended range values
required for EDR.

211
00:12:00,153 --> 00:12:02,756
The second dictionary,
outputVideoSettings,

212
00:12:02,789 --> 00:12:06,026
specifies the characteristics of
the pixel buffer format

213
00:12:06,059 --> 00:12:08,929
and also provides a reference
to the videoColorProperties dictionary

214
00:12:08,962 --> 00:12:11,899
we just created.

215
00:12:11,932 --> 00:12:17,504
In this example, we request wide color
and the half float pixel buffer format.

216
00:12:17,538 --> 00:12:20,908
It is very helpful
that AVPlayerItemVideoOutput,

217
00:12:20,941 --> 00:12:24,378
not only decodes video
into the pixel buffer format we specify

218
00:12:24,411 --> 00:12:26,480
in the output settings dictionary,

219
00:12:26,513 --> 00:12:30,517
but also automatically performs
any color conversion required

220
00:12:30,551 --> 00:12:34,021
via a pixel transfer session.

221
00:12:34,054 --> 00:12:36,890
Recall, a video might contain
multiple clips,

222
00:12:36,924 --> 00:12:39,393
potentially with different colorspaces.

223
00:12:39,426 --> 00:12:42,796
AVFoundation automatically manages
these for us,

224
00:12:42,829 --> 00:12:45,065
and as we'll soon be demonstrating,

225
00:12:45,098 --> 00:12:48,368
this behavior also allows
the resulting decoded video frames

226
00:12:48,402 --> 00:12:51,338
to be sent to low level frameworks
like Metal

227
00:12:51,371 --> 00:12:54,341
that don't themselves provide
automatic colorspace conversion

228
00:12:54,374 --> 00:12:56,643
to the display's colorspace.

229
00:12:57,377 --> 00:13:00,447
Now we create
the AVPlayerItemVideoOutput

230
00:13:00,480 --> 00:13:03,784
with the outputVideoSettings dictionary.

231
00:13:03,817 --> 00:13:06,353
As the third step,
we setup the Display link,

232
00:13:06,386 --> 00:13:10,824
which will be used to access
the decoded frames in real time.

233
00:13:10,858 --> 00:13:15,562
CADisplayLink takes a call back
that is run on each display update.

234
00:13:15,596 --> 00:13:19,366
In our example we call a local function
that we will explore in a moment

235
00:13:19,399 --> 00:13:24,805
to get the CVPixelBuffers that we will
be sending to Core Image for processing.

236
00:13:24,838 --> 00:13:27,508
Next we create a video player item
observer

237
00:13:27,541 --> 00:13:31,512
to allow us to handle changes
to specified player Item properties.

238
00:13:33,113 --> 00:13:35,516
Our example will execute this code

239
00:13:35,549 --> 00:13:38,652
every time for the player item's
status changes.

240
00:13:41,088 --> 00:13:44,191
When the player item's status
changes to readyToPlay,

241
00:13:44,224 --> 00:13:47,060
we add our AVPlayerItemVideoOutput

242
00:13:47,094 --> 00:13:52,165
to the new AVPlayerItem
that was just returned,

243
00:13:52,199 --> 00:13:57,070
register CADisplayLink
with the main run loop set to common mode,

244
00:13:57,104 --> 00:13:59,673
and start the real time decoding
of the HDR video

245
00:13:59,706 --> 00:14:02,976
by calling play on the video player.

246
00:14:04,478 --> 00:14:08,849
Finally, we will take a look at an example
CADisplayLink callback implementation,

247
00:14:08,882 --> 00:14:12,653
which we referred to earlier
as the `displayLinkCopyPixelBuffers`

248
00:14:12,686 --> 00:14:15,355
local function.

249
00:14:15,389 --> 00:14:17,591
Once the HDR video begins to play,

250
00:14:17,624 --> 00:14:22,629
the CADisplayLink callback function
is called on each display refresh.

251
00:14:22,663 --> 00:14:27,401
For instance it might be called 60 times
a second for a typical display.

252
00:14:27,434 --> 00:14:30,671
This is our code's opportunity
to update the frame displayed

253
00:14:30,704 --> 00:14:34,141
if there is a new CVPixelBuffer.

254
00:14:34,174 --> 00:14:37,611
On each display callback,
we attempt to copy a CVPixelBuffer

255
00:14:37,644 --> 00:14:40,447
containing the decoded video frame
to be displayed

256
00:14:40,480 --> 00:14:43,717
at the current wall clock time.

257
00:14:43,750 --> 00:14:47,054
However,
the `copyPixelBuffer` call might fail,

258
00:14:47,087 --> 00:14:50,123
as there won't always be a new
CVPixelBuffer available

259
00:14:50,157 --> 00:14:52,359
at every display refresh,

260
00:14:52,392 --> 00:14:56,797
especially when the screen refresh rate
exceeds that of the video being played.

261
00:14:56,830 --> 00:14:59,032
If there is not a new CVPixelBuffer,

262
00:14:59,066 --> 00:15:01,735
then the call fails
and we skip the render.

263
00:15:01,768 --> 00:15:06,373
This causes the preceding frame to remain
on-screen for another display refresh.

264
00:15:06,406 --> 00:15:10,143
But if the copy succeeds,
then we have a fresh frame of video

265
00:15:10,177 --> 00:15:12,112
in a CVPixelBuffer.

266
00:15:12,145 --> 00:15:16,350
There are a number of ways that we might
process and render this new frame.

267
00:15:16,383 --> 00:15:19,219
One opportunity is to send
the CVPixelBuffer to Core Image

268
00:15:19,253 --> 00:15:21,388
for processing.

269
00:15:21,421 --> 00:15:24,424
Core Image can string together
one or more CIFilters

270
00:15:24,458 --> 00:15:28,195
to provide GPU accelerated
image processing to the video frame.

271
00:15:29,596 --> 00:15:33,066
Please note that not all CIFilters
are compatible with EDR

272
00:15:33,100 --> 00:15:35,369
and might have trouble with HDR content,

273
00:15:35,402 --> 00:15:38,772
including clamping to SDR or worse.

274
00:15:38,805 --> 00:15:42,476
Core Image provides
many EDR compatible Filters.

275
00:15:42,509 --> 00:15:46,180
Use filter names with
CICategoryHighDynamicRange,

276
00:15:46,213 --> 00:15:49,917
to enumerate EDR compatible
Core Image filters.

277
00:15:49,950 --> 00:15:53,787
In our example, we will be adding
a simple sepia tone effect.

278
00:15:53,820 --> 00:15:58,025
Now let's return to our example
and integrate Core Image.

279
00:15:58,058 --> 00:16:01,461
On each display link callback
that yields a fresh CVPixelBuffer,

280
00:16:01,495 --> 00:16:04,398
create a CIImage from that pixel buffer.

281
00:16:06,099 --> 00:16:09,369
Instance the CIFilter
to implement the desired effect.

282
00:16:09,403 --> 00:16:13,574
I am using the sepia tone filter
because of its parameter-less simplicity,

283
00:16:13,607 --> 00:16:16,810
however there are many CIFilters
built into the system,

284
00:16:16,844 --> 00:16:20,314
and it is straightforward
to write your own, too.

285
00:16:20,347 --> 00:16:24,685
Set the CIFilter's inputImage
to the CIImage we just created.

286
00:16:26,153 --> 00:16:29,022
And the processed video result
will be available

287
00:16:29,056 --> 00:16:32,326
in the filter's outputImage.

288
00:16:32,359 --> 00:16:37,097
Chain as many CIFilters together as are
required to achieve your desired effect.

289
00:16:37,130 --> 00:16:39,399
Then use a CIRenderDestination

290
00:16:39,433 --> 00:16:42,970
to render the resulting image
to your application's view code.

291
00:16:44,805 --> 00:16:50,043
Please refer to the WWDC 2020 talk
"Optimize the Core Image pipeline for your video app"

292
00:16:50,077 --> 00:16:51,845
to learn more about this workflow.

293
00:16:51,879 --> 00:16:55,883
Another opportunity, is to process
and render the fresh CVPixelBuffer

294
00:16:55,916 --> 00:16:59,119
using Metal and custom Metal shaders.

295
00:16:59,152 --> 00:17:02,523
We will briefly describe the process
of converting the CVPixelBuffer

296
00:17:02,556 --> 00:17:04,057
to a Metal texture.

297
00:17:04,091 --> 00:17:07,828
However, implementing this conversion
maintaining best performance

298
00:17:07,861 --> 00:17:10,764
is a deep topic
best left for another talk.

299
00:17:10,797 --> 00:17:13,400
We instead recommend
deriving the Metal texture

300
00:17:13,433 --> 00:17:15,569
from the CoreVideo Metal texture cache,

301
00:17:15,602 --> 00:17:19,640
and will walk through that process
as the final example in this talk.

302
00:17:19,673 --> 00:17:24,711
Generally speaking, the process is to get
the IOSurface from the CVPixelBuffer,

303
00:17:24,745 --> 00:17:27,014
create a MetalTextureDescriptor,

304
00:17:27,047 --> 00:17:29,483
and then create a MetalTexture
from the MetalDevice,

305
00:17:29,516 --> 00:17:32,319
using `newTextureWithDescriptor`.

306
00:17:33,887 --> 00:17:37,324
However, there is a danger
that the textures may be re-used,

307
00:17:37,357 --> 00:17:41,428
and over-drawn,
if careful locking is not applied.

308
00:17:41,461 --> 00:17:45,799
Further, not all PixelBuffer formats
are natively supported by MetalTexture,

309
00:17:45,832 --> 00:17:49,169
which is why we use
half float in this example.

310
00:17:49,203 --> 00:17:51,972
Because of these complications,
we instead recommend

311
00:17:52,005 --> 00:17:56,677
directly accessing Metal textures from
Core Video, as we will now demonstrate.

312
00:17:56,710 --> 00:18:00,147
Let's further explore Core Video
and Metal.

313
00:18:00,180 --> 00:18:03,851
As mentioned, CVMetalTextureCache
is both a straightforward

314
00:18:03,884 --> 00:18:07,421
and efficient way
to use CVPixelBuffers with Metal.

315
00:18:07,454 --> 00:18:10,757
CVMetalTextureCache is handy
because you get a Metal texture

316
00:18:10,791 --> 00:18:14,862
directly from the cache
without need for further conversion.

317
00:18:14,895 --> 00:18:18,031
CVMetalTextureCache
automatically bridges between

318
00:18:18,065 --> 00:18:21,101
CVPixelBuffer's, and MetalTexture's,

319
00:18:21,134 --> 00:18:26,073
thereby both simplifying your code
and keeping you on the fast path.

320
00:18:26,106 --> 00:18:28,876
In conjunction with CVPixelBufferPools,

321
00:18:28,909 --> 00:18:32,646
CVMetalTextureCache
also provides performance benefits,

322
00:18:32,679 --> 00:18:36,116
by keeping MTLTexture
to IOSurface mapping alive.

323
00:18:37,784 --> 00:18:41,054
Finally, using CVMetalTextureCache
removes the need

324
00:18:41,088 --> 00:18:43,557
to manually track IOSurfaces.

325
00:18:43,590 --> 00:18:46,293
Now the final example in our talk:

326
00:18:46,326 --> 00:18:49,196
how to extract Metal textures
directly from Core Video

327
00:18:49,229 --> 00:18:51,698
using CVMetalTextureCache.

328
00:18:52,499 --> 00:18:55,469
Here, we start by getting
the system default Metal device.

329
00:18:55,502 --> 00:18:58,071
We use that to create
a Metal Texture Cache,

330
00:18:58,105 --> 00:19:01,341
and then instantiate
a Core Video Metal Texture Cache

331
00:19:01,375 --> 00:19:04,111
associated with the Metal Texture Cache.

332
00:19:04,144 --> 00:19:08,549
That can then be used to access
decoded video frames as Metal Textures,

333
00:19:08,582 --> 00:19:13,353
which conveniently,
can be directly used in our Metal engine.

334
00:19:13,387 --> 00:19:18,292
In this example, we create and use
the Metal system default device.

335
00:19:18,325 --> 00:19:21,261
Next we create the CVMetalTextureCache

336
00:19:21,295 --> 00:19:23,530
with CVMetalTextureCacheCreate,

337
00:19:23,564 --> 00:19:26,967
specifying the Metal device
we just created.

338
00:19:27,000 --> 00:19:29,970
We get the height and width
of the CVPixelBuffer

339
00:19:30,003 --> 00:19:33,574
needed to create the Core Video
Metal texture.

340
00:19:33,607 --> 00:19:37,511
Then we call
`CVMetalTextureCacheCreateTextureFromImage`,

341
00:19:37,544 --> 00:19:40,247
to instantiate a CVMetalTexture object

342
00:19:40,280 --> 00:19:43,750
and associate that
with the CVPixelBuffer.

343
00:19:43,784 --> 00:19:46,653
Finally we call
`CVMetalTextureGetTexture`,

344
00:19:46,687 --> 00:19:50,057
to get the desired Metal texture.

345
00:19:50,090 --> 00:19:54,394
Swift applications should use
a strong reference for CVMetalTexture,

346
00:19:54,428 --> 00:19:57,731
however, when using Objective-C,
you must ensure that Metal is done

347
00:19:57,764 --> 00:20:01,835
with your texture before you release
the CVMetalTextureRef.

348
00:20:01,869 --> 00:20:05,806
This may be accomplished using
metal command buffer completion handlers.

349
00:20:07,674 --> 00:20:09,042
And that's all, folks!

350
00:20:09,076 --> 00:20:11,678
To review,
we explored a number of workflows

351
00:20:11,712 --> 00:20:14,715
that will render your HDR video media
to EDR,

352
00:20:14,748 --> 00:20:17,684
for playback, editing,
or image processing.

353
00:20:18,685 --> 00:20:23,190
You learned how to go from AVPlayer to
AVKit's AVPlayerViewController,

354
00:20:23,223 --> 00:20:26,360
for playback of HDR media.

355
00:20:26,393 --> 00:20:30,264
You also learned how use AVPlayer,
along with AVPlayerLayer,

356
00:20:30,297 --> 00:20:34,234
to display HDR media on your own view.

357
00:20:34,268 --> 00:20:38,505
And finally, we explored how to add
real time effects during playback.

358
00:20:38,539 --> 00:20:42,009
Connecting AVFoundation's AVPlayer
to CoreVideo

359
00:20:42,042 --> 00:20:44,178
and then to Metal for rendering.

360
00:20:44,211 --> 00:20:46,947
And applying real time effects
using CoreImage filters,

361
00:20:46,980 --> 00:20:49,082
as well as Metal shaders.

362
00:20:51,852 --> 00:20:55,789
If you want to dig deeper,
I recommend a few WWDC sessions

363
00:20:55,822 --> 00:20:58,258
related to creating video workflows,

364
00:20:58,292 --> 00:21:02,429
as well as integrating HDR media with EDR.

365
00:21:02,462 --> 00:21:04,565
I especially want to call out the session

366
00:21:04,598 --> 00:21:08,368
"Edit and play back HDR video
with AVFoundation".

367
00:21:08,402 --> 00:21:11,505
This session explores use
of AVVideoComposition

368
00:21:11,538 --> 00:21:17,211
with `applyingCIFiltersWithHandler`
for applying effects to your HDR media.

369
00:21:17,244 --> 00:21:20,414
In this session you'll also learn
how to use custom compositor,

370
00:21:20,447 --> 00:21:23,183
which can then be used
with a CVPixelBuffer,

371
00:21:23,217 --> 00:21:26,820
when each video frame becomes
available for processing.

372
00:21:26,854 --> 00:21:29,857
As I mentioned at the beginning,
this year we're also presenting

373
00:21:29,890 --> 00:21:33,827
two other sessions on EDR:
"EDR on iOS,"

374
00:21:33,861 --> 00:21:38,665
where we announced EDR API support
has expanded to include iOS,

375
00:21:38,699 --> 00:21:44,204
and "HDR content display with
EDR using CoreImage, Metal and SwiftUI,"

376
00:21:44,238 --> 00:21:48,976
where we further explore integrating
EDR with other media frameworks.

377
00:21:49,009 --> 00:21:52,946
Hope you incorporate HDR video
into your EDR enabled applications

378
00:21:52,980 --> 00:21:56,116
on both macOS and now iOS.

379
00:21:56,149 --> 00:21:57,751
Thanks for watching.

