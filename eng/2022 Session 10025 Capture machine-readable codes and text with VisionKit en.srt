1
00:00:00,334 --> 00:00:06,340
[upbeat music]

2
00:00:09,309 --> 00:00:13,313
Ron Santos: Hey, hope you're well.
I'm Ron Santos, an input engineer.

3
00:00:13,347 --> 00:00:16,550
Today I’m here to talk to you
about capturing machine-readable codes

4
00:00:16,583 --> 00:00:18,285
and text from a video feed,

5
00:00:18,318 --> 00:00:21,722
or, as we like to call it, data scanning.

6
00:00:21,755 --> 00:00:24,124
What exactly do we mean by data scanning?

7
00:00:24,157 --> 00:00:28,662
It’s simply a way of using a sensor,
like a camera, to read data.

8
00:00:29,763 --> 00:00:32,199
Typically that data comes
in the form of text.

9
00:00:32,232 --> 00:00:35,035
For example,
a receipt with interesting information

10
00:00:35,068 --> 00:00:38,338
like telephone numbers, dates, and prices.

11
00:00:39,706 --> 00:00:42,709
Or maybe data comes
as a machine-readable code,

12
00:00:42,743 --> 00:00:45,612
like the ubiquitous QR code.

13
00:00:45,646 --> 00:00:48,282
You’ve probably
used a data scanner before,

14
00:00:48,315 --> 00:00:51,218
maybe in the Camera app
or by using the Live Text features

15
00:00:51,251 --> 00:00:53,720
introduced in iOS 15.

16
00:00:53,754 --> 00:00:56,323
And I bet you’ve used apps
in your day-to-day life

17
00:00:56,356 --> 00:00:59,760
with their own custom scanning experience.

18
00:00:59,793 --> 00:01:02,596
But what if you had
to build your own data scanner?

19
00:01:02,629 --> 00:01:04,198
How would you do it?

20
00:01:04,231 --> 00:01:07,034
The iOS SDK has more
than one solution for you,

21
00:01:07,067 --> 00:01:08,602
depending on your needs.

22
00:01:09,303 --> 00:01:12,472
One option is that you could
use the AVFoundation framework

23
00:01:12,506 --> 00:01:14,241
to set up the camera graph,

24
00:01:14,274 --> 00:01:17,945
connecting inputs and outputs
to a session and configuring it

25
00:01:17,978 --> 00:01:22,549
to yield AVMetadataObjects
like machine-readable codes.

26
00:01:22,583 --> 00:01:25,752
If you also wanted to capture text,
another option would be

27
00:01:25,786 --> 00:01:29,990
to combine both the AVFoundation
and the Vision frameworks together.

28
00:01:30,023 --> 00:01:32,693
In this diagram,
instead of metadata output,

29
00:01:32,726 --> 00:01:35,662
you create video data output.

30
00:01:35,696 --> 00:01:40,200
The video data output results
in the delivery of sample buffers

31
00:01:40,234 --> 00:01:43,170
that can be fed
to the Vision framework for use with text

32
00:01:43,203 --> 00:01:47,774
and barcode recognition requests,
resulting in Vision observation objects.

33
00:01:47,808 --> 00:01:49,843
For more on using Vision
for data scanning,

34
00:01:49,877 --> 00:01:54,581
check out the “Extract document data
using Vision” from WWDC21.

35
00:01:54,615 --> 00:01:58,519
Okay, so that’s using AVFoundation
and Vision for data scanning.

36
00:01:58,552 --> 00:02:03,724
In iOS 16, we have a new option
that encapsulates all of that for you.

37
00:02:03,757 --> 00:02:07,494
Introducing the DataScannerViewController
in the VisionKit framework.

38
00:02:07,528 --> 00:02:10,964
It combines the features
of AVFoundation and Vision

39
00:02:10,998 --> 00:02:13,934
specifically for the purpose
of data scanning.

40
00:02:13,967 --> 00:02:17,671
The DataScannerViewController users
are treated to features like

41
00:02:17,704 --> 00:02:22,176
a live camera preview,
helpful guidance labels,

42
00:02:22,209 --> 00:02:24,411
item highlighting,

43
00:02:24,444 --> 00:02:29,049
tap-to-focus
which is also used for selection,

44
00:02:29,082 --> 00:02:32,920
and lastly,
pinch-to-zoom to get a closer look.

45
00:02:34,221 --> 00:02:37,090
And let’s talk about features
for developers like you.

46
00:02:37,124 --> 00:02:40,561
The DataScannerViewController
is a UIViewController subclass

47
00:02:40,594 --> 00:02:42,896
that you can present however you choose.

48
00:02:42,930 --> 00:02:46,466
Coordinates for recognized items
are always in view coordinates,

49
00:02:46,500 --> 00:02:48,702
saving you from converting
from image space,

50
00:02:48,735 --> 00:02:51,572
to Vision coordinates,
to view coordinates.

51
00:02:51,605 --> 00:02:54,241
You’ll also be able to limit
the active portion of the view

52
00:02:54,274 --> 00:02:58,979
by specifying a region-of-interest,
which is also in view coordinates.

53
00:02:59,012 --> 00:03:01,815
For text recognition,
you can specify content types

54
00:03:01,849 --> 00:03:04,918
to limit the type of text you find.

55
00:03:04,952 --> 00:03:06,486
And for machine-readable codes,

56
00:03:06,520 --> 00:03:10,290
you can specify exactly
which symbologies to look for.

57
00:03:10,324 --> 00:03:12,359
I get it;
I use your apps, and I understand

58
00:03:12,392 --> 00:03:15,929
that data scanning is only a small portion
of their functionality.

59
00:03:15,963 --> 00:03:18,365
But it could require a lot of code.

60
00:03:18,398 --> 00:03:20,067
With DataScannerViewController,

61
00:03:20,100 --> 00:03:22,369
our goal is
to perform the common tasks for you,

62
00:03:22,402 --> 00:03:24,638
so you can focus your time elsewhere.

63
00:03:24,671 --> 00:03:28,008
Next, I’ll walk you
through adding it to your app.

64
00:03:28,041 --> 00:03:31,011
Let’s start
with the privacy usage description.

65
00:03:31,044 --> 00:03:34,448
When apps try to capture video,
iOS asks the user

66
00:03:34,481 --> 00:03:38,085
to grant their explicit permission
to access the camera.

67
00:03:38,118 --> 00:03:42,256
You’ll want to provide a descriptive
message justifying your need.

68
00:03:42,289 --> 00:03:45,759
To do that,
add a “privacy - camera usage description”

69
00:03:45,792 --> 00:03:48,095
to your app’s Info.plist file.

70
00:03:48,128 --> 00:03:52,966
Remember, be as descriptive as possible,
so users know what they’re agreeing to.

71
00:03:53,000 --> 00:03:55,102
Now onto the code.

72
00:03:55,135 --> 00:03:57,504
Wherever you would like
to present a data scanner,

73
00:03:57,538 --> 00:03:59,773
start by importing VisionKit.

74
00:04:01,008 --> 00:04:05,012
Next, because data scanning
isn’t supported on all devices,

75
00:04:05,045 --> 00:04:08,549
use the isSupported class property
to hide any buttons or menus

76
00:04:08,582 --> 00:04:10,217
exposing the functionality,

77
00:04:10,250 --> 00:04:13,620
so users aren’t presented
with something they can’t use.

78
00:04:14,888 --> 00:04:19,259
If you’re curious,
any 2018 and newer iPhone and iPad devices

79
00:04:19,293 --> 00:04:22,396
with the Apple Neural Engine
support data scanning.

80
00:04:22,429 --> 00:04:24,932
You’ll also want
to check for availability.

81
00:04:24,965 --> 00:04:27,801
Recall the privacy usage description?

82
00:04:27,835 --> 00:04:31,572
Scanning is available if the user
approves the app for camera access

83
00:04:31,605 --> 00:04:34,374
and if the device is free
of any restrictions,

84
00:04:34,408 --> 00:04:37,110
like the Camera access restriction
set here,

85
00:04:37,144 --> 00:04:41,081
in Screen Time’s Content
& Privacy Restrictions.

86
00:04:41,114 --> 00:04:43,584
Now you’re ready
to configure an instance.

87
00:04:43,617 --> 00:04:47,254
That’s done by first specifying the types
of data you’re interested in.

88
00:04:47,287 --> 00:04:51,458
For example,
you can scan for both QR codes and text.

89
00:04:52,559 --> 00:04:55,829
You can optionally pass a list
of languages for the text recognizer

90
00:04:55,863 --> 00:04:59,099
to use as a hint
for various processing aspects,

91
00:04:59,132 --> 00:05:01,201
like language correction.

92
00:05:01,235 --> 00:05:04,805
If you have an idea what languages
to expect, list them out.

93
00:05:04,838 --> 00:05:08,308
It’s especially useful when two languages
have similar looking scripts.

94
00:05:08,342 --> 00:05:10,244
If you do not provide any languages,

95
00:05:10,277 --> 00:05:13,947
the user’s preferred languages
are used by default.

96
00:05:13,981 --> 00:05:17,050
You can also request
a specific text content type.

97
00:05:17,084 --> 00:05:20,554
In this example,
I want my scanner to look for URLs.

98
00:05:20,587 --> 00:05:23,056
Now that you stated the types
of data to recognize,

99
00:05:23,090 --> 00:05:26,193
you can create your DataScanner instance.

100
00:05:26,226 --> 00:05:29,630
In the previous example,
I specified a barcode symbology,

101
00:05:29,663 --> 00:05:33,267
a recognition language,
and a text content type.

102
00:05:33,300 --> 00:05:37,471
Let me take a moment to explain
the other options for each of those.

103
00:05:37,504 --> 00:05:41,108
For barcode symbologies,
we support all the same symbologies

104
00:05:41,141 --> 00:05:43,877
as Vision’s barcode detector.

105
00:05:43,911 --> 00:05:46,980
In terms of languages,
as part of the LiveText feature,

106
00:05:47,014 --> 00:05:50,150
DataScannerViewController
supports the same exact languages.

107
00:05:50,184 --> 00:05:55,455
And in iOS 16, I’m happy to say we’re
adding support for Japanese and Korean.

108
00:05:55,489 --> 00:05:58,325
Of course,
this can change again in future.

109
00:05:58,358 --> 00:06:00,694
So use the
supportedTextRecognitionLanguages

110
00:06:00,727 --> 00:06:04,498
class property to retrieve
the most up to date list.

111
00:06:04,531 --> 00:06:07,868
Finally, when scanning for text
with specific semantic meaning,

112
00:06:07,901 --> 00:06:10,971
the DataScannerViewController
can find these seven types.

113
00:06:11,905 --> 00:06:14,908
We’re now ready to present
the Data Scanner to the user.

114
00:06:14,942 --> 00:06:18,846
Present it like any other view controller,
going fullscreen,

115
00:06:18,879 --> 00:06:22,883
using a sheet, or adding it
to another view hierarchy altogether.

116
00:06:22,916 --> 00:06:24,184
It’s all up to you.

117
00:06:24,218 --> 00:06:26,520
Afterwards,
when presentation completes,

118
00:06:26,553 --> 00:06:29,890
call startScanning()
to begin looking for data.

119
00:06:29,923 --> 00:06:32,626
So now I want to take a step back
and spend some time going

120
00:06:32,659 --> 00:06:35,729
over Data Scanner’s
initialization parameters.

121
00:06:35,762 --> 00:06:38,365
I used one here, recognizedDataTypes.

122
00:06:38,398 --> 00:06:42,169
But there are others that can
help you customize your experience.

123
00:06:43,136 --> 00:06:44,805
Let’s go through each one.

124
00:06:44,838 --> 00:06:49,243
recognizedDataTypes allows you
to specify what kind of data to recognize.

125
00:06:49,276 --> 00:06:52,946
Text, machine-readable codes,
and what types of each.

126
00:06:52,980 --> 00:06:56,183
qualityLevel can be balanced,
fast, or accurate.

127
00:06:56,216 --> 00:06:59,353
Fast will sacrifice resolution
in favor of speed in scenarios

128
00:06:59,386 --> 00:07:02,222
where you expect large
and easily-legible items,

129
00:07:02,256 --> 00:07:04,024
like text on signs.

130
00:07:04,057 --> 00:07:05,959
Accurate will give you the best accuracy,

131
00:07:05,993 --> 00:07:10,998
even with small items like
micro QR codes or tiny serial numbers.

132
00:07:11,031 --> 00:07:15,636
I recommend starting with balanced,
which should work great for most cases.

133
00:07:15,669 --> 00:07:18,572
recognizesMultipleItems
gives you the option to look

134
00:07:18,605 --> 00:07:20,774
for one or more items in the frame,

135
00:07:20,807 --> 00:07:23,877
like if you want
to scan multiple barcodes at a time.

136
00:07:23,911 --> 00:07:26,580
When it’s false,
the center-most item is recognized

137
00:07:26,613 --> 00:07:29,950
by default until the user taps elsewhere.

138
00:07:29,983 --> 00:07:33,153
Enable high frame rate tracking
when you draw highlights.

139
00:07:33,187 --> 00:07:36,190
It allows the highlights
to follow items as closely as possible

140
00:07:36,223 --> 00:07:39,927
when the camera moves
or the scene changes.

141
00:07:39,960 --> 00:07:43,130
Enable pinch-to-zoom or disable it.

142
00:07:43,163 --> 00:07:47,434
We also have methods you can use
to modify the zoom level yourself.

143
00:07:47,467 --> 00:07:49,870
When you enable guidance,
labels show at the top

144
00:07:49,903 --> 00:07:52,806
of the screen to help direct the user.

145
00:07:52,840 --> 00:07:56,877
And, finally, you can enable
system highlighting if you need it,

146
00:07:56,910 --> 00:07:59,546
or you can disable it to draw
your own custom highlighting.

147
00:08:00,447 --> 00:08:02,349
Now that you know how
to present the data scanner,

148
00:08:02,382 --> 00:08:04,985
let’s talk about how you’d
ingest the recognized items,

149
00:08:05,018 --> 00:08:07,888
and also how you’d
draw your own custom highlights.

150
00:08:08,922 --> 00:08:12,326
First, provide a delegate
to the data scanner.

151
00:08:12,359 --> 00:08:13,994
Now that you have a delegate,

152
00:08:14,027 --> 00:08:17,097
you can implement
the dataScanner didTapOn method,

153
00:08:17,130 --> 00:08:20,133
which is called
when the user taps on an item.

154
00:08:20,167 --> 00:08:24,538
With it, you’ll receive an instance
of this new type RecognizeItem.

155
00:08:24,571 --> 00:08:29,776
RecognizedItem is an enum that holds text
or a barcode as an associated value.

156
00:08:29,810 --> 00:08:33,647
For text, the transcription
property holds the recognized string.

157
00:08:33,680 --> 00:08:36,383
For barcodes,
if its payload contains a string,

158
00:08:36,416 --> 00:08:39,720
you can retrieve it
with the payloadStringValue.

159
00:08:39,753 --> 00:08:42,556
Two other things you should know
about RecognizedItem:

160
00:08:42,589 --> 00:08:46,560
First, each recognized item
has a unique identifier you can use

161
00:08:46,593 --> 00:08:48,896
to track an item throughout its lifetime.

162
00:08:48,929 --> 00:08:51,498
That lifetime starts
when the item is first seen

163
00:08:51,532 --> 00:08:54,001
and ends when it’s no longer in view.

164
00:08:54,034 --> 00:08:57,204
And second,
each RecognizedItem has a bounds property.

165
00:08:57,237 --> 00:08:59,907
The bounds isn’t a rect,
but it consists of four points,

166
00:08:59,940 --> 00:09:01,441
one for each corner.

167
00:09:01,475 --> 00:09:04,645
Next, let’s talk
about three related delegate methods

168
00:09:04,678 --> 00:09:07,548
that are called when recognized items
in the scene change.

169
00:09:07,581 --> 00:09:09,516
The first is didAdd,

170
00:09:09,550 --> 00:09:12,753
called when items
in the scene are newly recognized.

171
00:09:12,786 --> 00:09:15,255
If you wanted to create
your own custom highlight,

172
00:09:15,289 --> 00:09:18,292
you’d create one here for each new item.

173
00:09:18,325 --> 00:09:23,030
You can keep track of the highlights
using the ID from its associated item.

174
00:09:23,063 --> 00:09:25,866
And when adding your new view
to the view hierarchy,

175
00:09:25,899 --> 00:09:28,836
add them
to DataScanner’s overlayContainerView,

176
00:09:28,869 --> 00:09:33,941
so they appear above the camera preview,
but below any other supplemental chrome.

177
00:09:35,142 --> 00:09:37,477
The next delegate method is didUpdate,

178
00:09:37,511 --> 00:09:40,614
which is called when the items move
or the camera moves.

179
00:09:40,647 --> 00:09:44,551
It can also be called when transcription
for recognized text change.

180
00:09:44,585 --> 00:09:47,554
They change because
the longer the scanner sees the text,

181
00:09:47,588 --> 00:09:50,791
the more accurate it’ll be
with its transcription.

182
00:09:50,824 --> 00:09:54,027
Use the IDs from the updated items
to retrieve your highlights

183
00:09:54,061 --> 00:09:56,530
from the dictionary you just created,

184
00:09:56,563 --> 00:10:00,767
and then animate the views
to their newly updated bounds.

185
00:10:00,801 --> 00:10:03,437
And finally,
the didRemove delegate method,

186
00:10:03,470 --> 00:10:07,040
which is called when items
are no longer visible in the scene.

187
00:10:07,074 --> 00:10:09,977
In this method,
you can forget about any highlight views

188
00:10:10,010 --> 00:10:12,479
you associated
with the removed items,

189
00:10:12,513 --> 00:10:15,282
and you can remove them
from the view hierarchy.

190
00:10:15,315 --> 00:10:18,218
In summary, if you draw
your own highlights over items,

191
00:10:18,252 --> 00:10:20,320
those three delegate methods
will be crucial

192
00:10:20,354 --> 00:10:23,323
for you to control animating highlights
into the scene,

193
00:10:23,357 --> 00:10:26,927
animating their movement,
and animating them out.

194
00:10:26,960 --> 00:10:29,096
And for each of those three
previous delegate methods,

195
00:10:29,129 --> 00:10:33,333
you’ll also be given an array
of all the items currently recognized.

196
00:10:33,367 --> 00:10:35,702
That may come in handy
for text recognition

197
00:10:35,736 --> 00:10:38,705
because the items are placed
in their natural reading order,

198
00:10:38,739 --> 00:10:41,775
meaning the user would
read the item at index 0

199
00:10:41,808 --> 00:10:45,846
before the item at index 1 and so on.

200
00:10:45,879 --> 00:10:48,515
That’s an overview of how
to use the DataScannerViewController.

201
00:10:48,549 --> 00:10:53,287
Before wrapping up, I wanted
to quickly mention a few other features,

202
00:10:53,320 --> 00:10:55,189
like capturing photos.

203
00:10:55,222 --> 00:10:57,024
You can call the capturePhoto method,

204
00:10:57,057 --> 00:11:01,428
which will asynchronously return
a high quality UIImage.

205
00:11:02,462 --> 00:11:04,431
And if you aren’t
creating custom highlights,

206
00:11:04,464 --> 00:11:07,234
you might not need
these three delegate methods.

207
00:11:07,267 --> 00:11:10,704
Instead, you can
use the recognizedItem property.

208
00:11:10,737 --> 00:11:15,742
It’s an AsyncStream that will be
continuously updated as the scene changes.

209
00:11:17,845 --> 00:11:19,379
Thanks for hanging out.

210
00:11:19,413 --> 00:11:21,882
Remember,
the iOS SDK gives you options

211
00:11:21,915 --> 00:11:23,684
for creating computer vision workflows

212
00:11:23,717 --> 00:11:26,854
with the AVFoundation
and Vision frameworks.

213
00:11:26,887 --> 00:11:29,122
But maybe you’re
creating an app that scans text

214
00:11:29,156 --> 00:11:32,025
or machine-readable codes
with a live video feed,

215
00:11:32,059 --> 00:11:33,527
like a Pick-and-pack app,

216
00:11:33,560 --> 00:11:36,697
a back-of-the-warehouse app,
or a point-of-sale app.

217
00:11:36,730 --> 00:11:38,932
If so, then give
the DataScannerViewController

218
00:11:38,966 --> 00:11:40,434
in VisionKit a look.

219
00:11:40,467 --> 00:11:42,302
As I went over today, it has a number

220
00:11:42,336 --> 00:11:45,706
of initialization parameters
and delegate methods that you can use

221
00:11:45,739 --> 00:11:49,943
to provide a custom experience
that matches your app’s style and needs.

222
00:11:50,944 --> 00:11:53,113
And finally,
I wanted to give a shout out

223
00:11:53,146 --> 00:11:55,849
to the “Add Live Text interaction
to your app” session,

224
00:11:55,883 --> 00:12:00,487
where you can learn about VisionKit’s
Live Text abilities for static images.

225
00:12:01,555 --> 00:12:03,357
Until next time, peace.

226
00:12:03,390 --> 00:12:08,662
[upbeat music]

