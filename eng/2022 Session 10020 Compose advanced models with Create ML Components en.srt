1
00:00:00,200 --> 00:00:03,003
♪ instrumental hip hop music ♪

2
00:00:03,003 --> 00:00:09,676
♪

3
00:00:09,676 --> 00:00:11,612
Hi, my name is David Findlay,

4
00:00:11,612 --> 00:00:14,014
and I'm an engineer
on the Create ML team.

5
00:00:14,014 --> 00:00:16,583
This session is all about
Create ML Components,

6
00:00:16,583 --> 00:00:20,487
a powerful new way to build
your own machine learning tasks.

7
00:00:20,487 --> 00:00:23,023
My colleague Alejandro gave
an introduction in the session

8
00:00:23,023 --> 00:00:25,092
"Get to know
Create ML Components."

9
00:00:25,092 --> 00:00:27,394
He explores deconstructing
Create ML tasks

10
00:00:27,394 --> 00:00:29,863
into components
and revealed how easy it is

11
00:00:29,863 --> 00:00:31,865
to build custom models.

12
00:00:31,865 --> 00:00:35,002
Transformers and estimators
are the main building blocks

13
00:00:35,002 --> 00:00:37,938
that you can compose together
to build custom models

14
00:00:37,938 --> 00:00:40,140
like image regression.

15
00:00:40,140 --> 00:00:43,377
In this session, I want to go
way beyond the basics

16
00:00:43,377 --> 00:00:46,613
and demonstrate what's possible
with Create ML Components.

17
00:00:46,613 --> 00:00:49,917
Let's go over the agenda;
there's lots to cover.

18
00:00:49,917 --> 00:00:53,186
I'll start by talking about
video data and go into detail

19
00:00:53,186 --> 00:00:57,090
about new components designed
to handle values over time.

20
00:00:57,090 --> 00:00:58,792
Then I'll put
those concepts to work

21
00:00:58,792 --> 00:01:01,061
and build a human action
repetition counter

22
00:01:01,061 --> 00:01:03,430
using only transformers.

23
00:01:03,430 --> 00:01:05,232
Finally,
I'll move on to training

24
00:01:05,232 --> 00:01:07,200
a custom sound
classifier model.

25
00:01:07,200 --> 00:01:09,469
I'll discuss incremental fitting
which allows you

26
00:01:09,469 --> 00:01:11,271
to update your model
with batches,

27
00:01:11,271 --> 00:01:13,941
stop training early,
and checkpoint your model.

28
00:01:13,941 --> 00:01:17,144
There's so much opportunity
with this level of flexibility.

29
00:01:17,144 --> 00:01:19,012
I can't wait to dive in.

30
00:01:19,012 --> 00:01:21,081
Let's get started.

31
00:01:21,081 --> 00:01:24,551
At WWDC 2020, we introduced
Action Classification

32
00:01:24,551 --> 00:01:26,153
in Create ML,

33
00:01:26,153 --> 00:01:28,989
which allows you to classify
actions from videos.

34
00:01:28,989 --> 00:01:31,625
And we demonstrated how you can
create a fitness classifier

35
00:01:31,625 --> 00:01:34,127
to recognize a person's
workout routines,

36
00:01:34,127 --> 00:01:37,998
such as jumping jacks,
lunges, and squats.

37
00:01:37,998 --> 00:01:40,500
For example, you can use
the action classifier

38
00:01:40,500 --> 00:01:44,037
to recognize the action in
this video as a jumping jack.

39
00:01:44,037 --> 00:01:47,441
But what if you wanted
to count your jumping jacks?

40
00:01:47,441 --> 00:01:49,109
The first thing
you need to consider

41
00:01:49,109 --> 00:01:52,079
is that a jumping jack
spans consecutive frames,

42
00:01:52,079 --> 00:01:55,182
and you'll need a way
to handle values over time.

43
00:01:55,182 --> 00:01:57,050
Thankfully,
Swift's AsyncSequence

44
00:01:57,050 --> 00:01:59,386
makes this really easy.

45
00:01:59,386 --> 00:02:01,888
If you're unfamiliar
with async sequences,

46
00:02:01,888 --> 00:02:06,259
you should check out the session
"Meet AsyncSequence”.

47
00:02:06,259 --> 00:02:09,062
With Create ML Components,
you can read your video

48
00:02:09,062 --> 00:02:13,133
as an async sequence of frames,
using the video reader.

49
00:02:13,133 --> 00:02:16,169
And AsyncSequence provides a
way of iterating over the frames

50
00:02:16,169 --> 00:02:19,840
as they are received
from the video.

51
00:02:19,840 --> 00:02:22,576
For example,
I can easily transform

52
00:02:22,576 --> 00:02:26,146
each video frame asynchronously
using the map method.

53
00:02:26,146 --> 00:02:30,550
This is useful when you want
to process frames one at a time.

54
00:02:30,550 --> 00:02:32,085
But what if you wanted
to process

55
00:02:32,085 --> 00:02:34,254
multiple frames at a time?

56
00:02:34,254 --> 00:02:36,857
That's where
temporal transformers come in.

57
00:02:36,857 --> 00:02:39,626
For example, you may want
to downsample frames

58
00:02:39,626 --> 00:02:42,095
to speed-up an action
in a video.

59
00:02:42,095 --> 00:02:44,064
You can use
a downsampler for that

60
00:02:44,064 --> 00:02:45,766
which takes an async sequence

61
00:02:45,766 --> 00:02:48,802
and returns a downsampled
async sequence.

62
00:02:48,802 --> 00:02:51,505
Or you may want to group
frames into windows,

63
00:02:51,505 --> 00:02:54,708
which is important
for counting action repetitions.

64
00:02:54,708 --> 00:02:58,111
That's where you can use
a sliding window transformer.

65
00:02:58,111 --> 00:03:00,881
You can specify a window length,
which is how many frames

66
00:03:00,881 --> 00:03:03,450
you want to group in a window,
and a stride,

67
00:03:03,450 --> 00:03:06,586
which is how you control
the sliding interval.

68
00:03:06,586 --> 00:03:09,956
The input is, again,
an async sequence,

69
00:03:09,956 --> 00:03:15,662
and the output in this case is
a windowed async sequence.

70
00:03:15,662 --> 00:03:17,964
Generally speaking,
a temporal transformer

71
00:03:17,964 --> 00:03:20,600
provides a way to process
an async sequence

72
00:03:20,600 --> 00:03:22,903
into a new async sequence.

73
00:03:22,903 --> 00:03:25,772
So let's put
these concepts to work.

74
00:03:25,772 --> 00:03:27,941
I don't know about you,
but when I'm working out,

75
00:03:27,941 --> 00:03:30,143
I always lose count of my reps.

76
00:03:30,143 --> 00:03:32,112
So I decided
to shake things up a bit

77
00:03:32,112 --> 00:03:34,214
and build an action
repetition counter

78
00:03:34,214 --> 00:03:36,383
with Create ML Components.

79
00:03:36,383 --> 00:03:39,920
In this example, I'll go over
how to compose transformers

80
00:03:39,920 --> 00:03:42,255
and temporal transformers
together.

81
00:03:42,255 --> 00:03:45,525
Let's start with
pose extraction.

82
00:03:45,525 --> 00:03:49,296
I can extract poses using
the human body pose extractor.

83
00:03:49,296 --> 00:03:50,831
The input is an image

84
00:03:50,831 --> 00:03:54,401
and the output is an array
of human body poses.

85
00:03:54,401 --> 00:03:56,837
Behind the scenes,
we leverage the Vision framework

86
00:03:56,837 --> 00:03:59,573
to extract the poses.

87
00:03:59,573 --> 00:04:02,375
Note that images
can contain multiple people,

88
00:04:02,375 --> 00:04:05,011
which is common
for group workouts.

89
00:04:05,011 --> 00:04:08,415
That's why the output
is an array of poses.

90
00:04:08,415 --> 00:04:11,184
But I'm only interested in
counting action repetitions

91
00:04:11,184 --> 00:04:13,520
for one person at a time.

92
00:04:13,520 --> 00:04:16,356
So I'll compose
the human body pose extractor

93
00:04:16,356 --> 00:04:19,659
with a pose selector.

94
00:04:19,659 --> 00:04:22,129
A pose selector
takes an array of poses

95
00:04:22,129 --> 00:04:26,833
as well as a selection strategy
and returns a single pose.

96
00:04:26,833 --> 00:04:29,236
There's a few selection
strategies to choose from,

97
00:04:29,236 --> 00:04:30,604
but for this example,

98
00:04:30,604 --> 00:04:33,507
I'll use the
rightMostJointLocation strategy.

99
00:04:33,507 --> 00:04:38,311
The next step is to group
the poses into windows.

100
00:04:38,311 --> 00:04:42,015
I'll append a sliding window
transformer for that.

101
00:04:42,015 --> 00:04:44,851
And I'll use a window length
and stride of 90,

102
00:04:44,851 --> 00:04:47,120
which will generate
nonoverlapping windows

103
00:04:47,120 --> 00:04:49,823
of 90 poses.

104
00:04:49,823 --> 00:04:52,826
Recall that a sliding window
transformer is temporal,

105
00:04:52,826 --> 00:04:55,729
which makes
the whole task temporal,

106
00:04:55,729 --> 00:05:00,367
and the expected input is now
an async sequence of frames.

107
00:05:00,367 --> 00:05:05,639
Finally, I'll append
a human body action counter.

108
00:05:05,639 --> 00:05:06,740
This temporal transformer

109
00:05:06,740 --> 00:05:10,043
consumes a windowed
async sequence of poses

110
00:05:10,043 --> 00:05:13,013
and returns a cumulative count
of the action repetitions

111
00:05:13,013 --> 00:05:14,514
so far.

112
00:05:14,514 --> 00:05:15,749
By now, you may have noticed

113
00:05:15,749 --> 00:05:18,285
that the count is
a floating-point number.

114
00:05:18,285 --> 00:05:21,821
And that's because the task
counts partial actions too.

115
00:05:21,821 --> 00:05:23,323
It's that easy.

116
00:05:23,323 --> 00:05:25,625
Now I can count my reps
in my workout videos

117
00:05:25,625 --> 00:05:27,961
and make sure
I'm not cheating.

118
00:05:27,961 --> 00:05:31,264
But it would be even better
to count repetitions live

119
00:05:31,264 --> 00:05:35,168
in an app, so that I can keep
track of my current workouts.

120
00:05:35,168 --> 00:05:38,071
Let me show you
how you can do that.

121
00:05:38,071 --> 00:05:40,607
First, I'll use
the readCamera method

122
00:05:40,607 --> 00:05:42,409
which takes
a camera configuration

123
00:05:42,409 --> 00:05:45,879
and returns an async sequence
of camera frames.

124
00:05:45,879 --> 00:05:49,149
Next, I'll adjust the stride
parameter to 15 frames

125
00:05:49,149 --> 00:05:52,285
so that I get an updated count
more often.

126
00:05:52,285 --> 00:05:55,422
If my camera captures frames at
a rate of 30 frames per second,

127
00:05:55,422 --> 00:05:58,858
then I get counts
every half second.

128
00:05:58,858 --> 00:06:03,396
Now I can workout and not worry
about missing a rep.

129
00:06:03,396 --> 00:06:06,132
So far, I've explored
temporal components

130
00:06:06,132 --> 00:06:08,668
for transforming
async sequences.

131
00:06:08,668 --> 00:06:11,238
Next, I want to focus on
training custom models

132
00:06:11,238 --> 00:06:14,641
that rely on temporal data.

133
00:06:14,641 --> 00:06:18,445
In 2019, we demonstrated how
you can train a sound classifier

134
00:06:18,445 --> 00:06:19,946
in Create ML.

135
00:06:19,946 --> 00:06:22,716
Then in 2021,
we introduced enhancements

136
00:06:22,716 --> 00:06:24,951
to sound classification.

137
00:06:24,951 --> 00:06:26,419
I want to go even further

138
00:06:26,419 --> 00:06:30,657
and train a custom sound
classifier incrementally.

139
00:06:30,657 --> 00:06:33,393
The MLSoundClassifier
in the Create ML framework

140
00:06:33,393 --> 00:06:34,694
is still the easiest way

141
00:06:34,694 --> 00:06:36,896
to train a custom
sound classifier model.

142
00:06:36,896 --> 00:06:39,933
But when you need more
customizability and control,

143
00:06:39,933 --> 00:06:42,168
you can use the components
under the hood.

144
00:06:42,168 --> 00:06:46,406
In its simplest form, the sound
classifier has two components:

145
00:06:46,406 --> 00:06:49,075
an Audio Feature Print
feature extractor

146
00:06:49,075 --> 00:06:51,745
and a classifier of your choice.

147
00:06:51,745 --> 00:06:54,247
AudioFeaturePrint is
a temporal transformer

148
00:06:54,247 --> 00:06:56,850
that extracts audio features
from an async sequence

149
00:06:56,850 --> 00:06:59,252
of audio buffers.

150
00:06:59,252 --> 00:07:01,755
Similar to
a sliding window transformer,

151
00:07:01,755 --> 00:07:04,357
AudioFeaturePrint windows
the async sequence

152
00:07:04,357 --> 00:07:07,327
then extracts features.

153
00:07:07,327 --> 00:07:09,529
There are a few classifiers
to choose from,

154
00:07:09,529 --> 00:07:14,034
but for this example, I'll use
a logistic regression classifier

155
00:07:14,034 --> 00:07:16,836
and then compose it together
with the feature extractor

156
00:07:16,836 --> 00:07:20,573
to build a custom
sound classifier.

157
00:07:20,573 --> 00:07:23,643
The next step is to fit
the custom sound classifier

158
00:07:23,643 --> 00:07:25,378
to labeled training data.

159
00:07:25,378 --> 00:07:27,881
For more information about
collecting training data,

160
00:07:27,881 --> 00:07:30,150
the "Get to know Create ML
Components" session

161
00:07:30,150 --> 00:07:32,085
is a good place to start.

162
00:07:32,085 --> 00:07:34,688
So far, I've covered
the happy path.

163
00:07:34,688 --> 00:07:36,623
But building
machine learning models

164
00:07:36,623 --> 00:07:39,659
can be an iterative process.

165
00:07:39,659 --> 00:07:43,029
For example, you may discover
and collect new training data

166
00:07:43,029 --> 00:07:46,333
over time and want
to refresh your model.

167
00:07:46,333 --> 00:07:49,502
It's possible that you can
improve the model quality.

168
00:07:49,502 --> 00:07:53,340
But retraining your model
from scratch is time-consuming.

169
00:07:53,340 --> 00:07:55,942
That's because you need
to redo feature extraction

170
00:07:55,942 --> 00:07:58,044
for all of your previous data.

171
00:07:58,044 --> 00:08:00,714
Let me give you an example
of how you can save time

172
00:08:00,714 --> 00:08:04,184
when training your models
with newly discovered data.

173
00:08:04,184 --> 00:08:06,686
The key is to preprocess
your training data

174
00:08:06,686 --> 00:08:09,456
separately from
fitting your model.

175
00:08:09,456 --> 00:08:12,792
In this example, I can extract
audio features separately

176
00:08:12,792 --> 00:08:15,428
from the classifier fitting.

177
00:08:15,428 --> 00:08:17,530
And this works in general too.

178
00:08:17,530 --> 00:08:19,966
Whenever you have
a series of transformers

179
00:08:19,966 --> 00:08:21,634
followed by an estimator,

180
00:08:21,634 --> 00:08:24,637
you can preprocess the input
through the transformers

181
00:08:24,637 --> 00:08:26,506
leading up to the estimator.

182
00:08:26,506 --> 00:08:30,143
All you need to do
is call the preprocess method

183
00:08:30,143 --> 00:08:33,513
and then fit the model on
the preprocessed features.

184
00:08:33,513 --> 00:08:35,849
I find this convenient
because I didn't need to change

185
00:08:35,849 --> 00:08:38,785
the sound classifier
composition.

186
00:08:38,785 --> 00:08:41,087
Now that I have the features
extracted separately,

187
00:08:41,087 --> 00:08:44,057
I have the flexibility
to only extract audio features

188
00:08:44,057 --> 00:08:46,760
for the new data.

189
00:08:46,760 --> 00:08:49,129
As you discover new
training data for your model,

190
00:08:49,129 --> 00:08:51,998
you can easily preprocess
this data separately.

191
00:08:51,998 --> 00:08:54,000
And then append
the supplemental features

192
00:08:54,000 --> 00:08:56,770
to the previously
extracted ones.

193
00:08:56,770 --> 00:08:58,605
This is just the first example

194
00:08:58,605 --> 00:09:01,741
of where preprocessing
can save you time.

195
00:09:01,741 --> 00:09:04,644
Let's go back to
the model-building lifecycle.

196
00:09:04,644 --> 00:09:06,813
You may need to tune
your estimator parameters

197
00:09:06,813 --> 00:09:09,482
until you're satisfied
with your model's quality.

198
00:09:09,482 --> 00:09:12,385
By separating the feature
extraction from the fitting,

199
00:09:12,385 --> 00:09:14,821
you can extract
your features only once

200
00:09:14,821 --> 00:09:18,324
and then fit your model with
different estimator parameters.

201
00:09:18,324 --> 00:09:19,592
Let's go over an example

202
00:09:19,592 --> 00:09:21,261
of changing
the classifier parameters

203
00:09:21,261 --> 00:09:24,531
without redoing
feature extraction.

204
00:09:24,531 --> 00:09:26,933
Assuming that I've already
extracted features,

205
00:09:26,933 --> 00:09:30,770
I'll modify the classifier's
L2 penalty parameter.

206
00:09:30,770 --> 00:09:33,006
And then I'll need to append
the new classifier

207
00:09:33,006 --> 00:09:35,375
to the old feature extractor.

208
00:09:35,375 --> 00:09:37,944
It's important not to change
the feature extractor

209
00:09:37,944 --> 00:09:40,747
when tuning your estimator,
because that would invalidate

210
00:09:40,747 --> 00:09:43,049
the previously
extracted features.

211
00:09:43,049 --> 00:09:47,420
Let's move on to incrementally
fitting your model with batches.

212
00:09:47,420 --> 00:09:49,322
Machine learning models
in general

213
00:09:49,322 --> 00:09:51,724
benefit from large amounts
of training data.

214
00:09:51,724 --> 00:09:55,094
However, your app may have
limited memory constraints.

215
00:09:55,094 --> 00:09:56,729
So what do you do?

216
00:09:56,729 --> 00:09:59,265
You can use Create ML Components
to train a model

217
00:09:59,265 --> 00:10:02,569
by loading only a batch of data
into memory at a time.

218
00:10:02,569 --> 00:10:05,605
The first thing I need to do
is replace the classifier

219
00:10:05,605 --> 00:10:07,707
with an updatable classifier.

220
00:10:07,707 --> 00:10:10,076
In order to train
a custom model with batches,

221
00:10:10,076 --> 00:10:12,679
your classifier
needs to be updatable.

222
00:10:12,679 --> 00:10:16,382
For example, the fully connected
neural network classifier,

223
00:10:16,382 --> 00:10:17,684
which I can easily use

224
00:10:17,684 --> 00:10:20,220
instead of the logistic
regression classifier

225
00:10:20,220 --> 00:10:22,021
which is not updatable.

226
00:10:25,291 --> 00:10:28,795
All right, now I'll write
a training loop.

227
00:10:28,795 --> 00:10:32,031
I'll start by creating
a default initialized model.

228
00:10:32,031 --> 00:10:34,033
You won't be able
to make predictions yet;

229
00:10:34,033 --> 00:10:37,837
that's because this is just
the starting point for training.

230
00:10:37,837 --> 00:10:39,873
Then I'll extract
the audio features

231
00:10:39,873 --> 00:10:41,908
before the training starts.

232
00:10:41,908 --> 00:10:43,109
This is an important step

233
00:10:43,109 --> 00:10:46,913
because I don't want to extract
features every iteration.

234
00:10:46,913 --> 00:10:49,749
The next step is
to define the training loop

235
00:10:49,749 --> 00:10:51,384
and specify the number
of iterations

236
00:10:51,384 --> 00:10:53,853
you'd like to train for.

237
00:10:53,853 --> 00:10:57,891
Before I continue, I'll import
the algorithm's Swift package.

238
00:10:57,891 --> 00:11:01,327
I'll need it for creating
batches of training data.

239
00:11:01,327 --> 00:11:02,795
Make sure to check out
the session

240
00:11:02,795 --> 00:11:05,331
"Meet the Swift Algorithms
and Collections packages"

241
00:11:05,331 --> 00:11:08,701
from WWDC 2021
to learn more.

242
00:11:10,403 --> 00:11:13,506
Within the training loop is
where the batching happens.

243
00:11:13,506 --> 00:11:15,708
I'll use the chunks method
to group the features

244
00:11:15,708 --> 00:11:18,344
into batches for training.

245
00:11:18,344 --> 00:11:20,346
The chunk size
is the number of features

246
00:11:20,346 --> 00:11:23,116
that are loaded
into memory at once.

247
00:11:23,116 --> 00:11:26,519
Then, I can update the model
by iterating over the batches

248
00:11:26,519 --> 00:11:28,655
and calling the update method.

249
00:11:31,491 --> 00:11:33,293
When you train your model
incrementally,

250
00:11:33,293 --> 00:11:35,495
you can unlock a few more
training techniques.

251
00:11:35,495 --> 00:11:36,596
For example,

252
00:11:36,596 --> 00:11:39,065
in this training graph,
after about 10 iterations,

253
00:11:39,065 --> 00:11:42,669
the model accuracy
plateaus at 95 percent.

254
00:11:42,669 --> 00:11:45,004
At this point,
the model has converged

255
00:11:45,004 --> 00:11:46,639
and you can stop early.

256
00:11:46,639 --> 00:11:50,510
Let's implement early stopping
in the training loop.

257
00:11:50,510 --> 00:11:52,779
The first thing I need to do
is make predictions

258
00:11:52,779 --> 00:11:54,581
for my validation set.

259
00:11:54,581 --> 00:11:56,349
I'm using the mapFeatures
method here

260
00:11:56,349 --> 00:11:58,818
because I need to pair
the validation predictions

261
00:11:58,818 --> 00:12:01,721
with its annotations.

262
00:12:01,721 --> 00:12:04,691
The next step is to measure
the quality of the model.

263
00:12:04,691 --> 00:12:06,759
I'll use the built-in metrics
for now,

264
00:12:06,759 --> 00:12:08,127
but there's nothing
stopping you

265
00:12:08,127 --> 00:12:10,663
from implementing
your own custom metrics.

266
00:12:10,663 --> 00:12:12,498
And finally, I'll stop training

267
00:12:12,498 --> 00:12:16,269
when my model has reached
an accuracy of 95 percent.

268
00:12:16,269 --> 00:12:19,505
Outside of the training loop,
I'll write the model out to disk

269
00:12:19,505 --> 00:12:22,842
so that I can use it later
to make predictions.

270
00:12:22,842 --> 00:12:24,377
In addition to stopping early,

271
00:12:24,377 --> 00:12:26,779
I'd like to talk about
model checkpointing.

272
00:12:28,548 --> 00:12:30,984
You can save your model's
progress during training

273
00:12:30,984 --> 00:12:33,152
rather than waiting
until the end.

274
00:12:33,152 --> 00:12:34,787
And you can even use
checkpointing

275
00:12:34,787 --> 00:12:37,423
in order to resume training,
which is convenient

276
00:12:37,423 --> 00:12:41,327
especially when your model
takes a long time to train.

277
00:12:41,327 --> 00:12:45,031
All you need to do is write out
your model in the training loop.

278
00:12:45,031 --> 00:12:47,500
We recommend doing this
every few iterations

279
00:12:47,500 --> 00:12:49,469
by defining
a checkpoint interval.

280
00:12:49,469 --> 00:12:51,604
It's that easy.

281
00:12:51,604 --> 00:12:54,674
In this session, I introduced
temporal components,

282
00:12:54,674 --> 00:12:56,643
a new way to build
machine learning tasks

283
00:12:56,643 --> 00:13:00,246
with temporal data
like audio and video.

284
00:13:00,246 --> 00:13:02,915
I composed
temporal components together

285
00:13:02,915 --> 00:13:05,752
to make a human action
repetition counter.

286
00:13:05,752 --> 00:13:08,921
And finally, I talked about
incremental fitting.

287
00:13:08,921 --> 00:13:10,723
This will unlock
new possibilities for you

288
00:13:10,723 --> 00:13:13,426
to build machine learning
into your apps.

289
00:13:13,426 --> 00:13:16,763
Thanks for joining me
and enjoy the rest of WWDC.

290
00:13:16,763 --> 00:13:21,300
♪

