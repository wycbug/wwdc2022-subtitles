1
00:00:00,501 --> 00:00:08,509
♪ ♪

2
00:00:09,643 --> 00:00:13,647
Ken Greenebaum: 大家好 欢迎来到 WWDC 2022

3
00:00:13,680 --> 00:00:15,082
我是 Ken Greenebaum

4
00:00:15,115 --> 00:00:18,352
是 Apple 的
显示和色彩技术团队的成员

5
00:00:18,385 --> 00:00:21,388
很高兴今年能举行三次 EDR 演讲

6
00:00:21,421 --> 00:00:25,559
希望您有机会观看
“探索 iOS 上的 EDR”

7
00:00:25,592 --> 00:00:29,263
我们在里面宣布了
对 iOS 的 EDR API 支持

8
00:00:29,296 --> 00:00:33,901
还有 “借助 Core Image、Metal
和 SwiftUI 显示 EDR 内容”

9
00:00:33,934 --> 00:00:37,838
一些开发者可能
也看过我去年的 EDR 演讲

10
00:00:37,871 --> 00:00:40,240
我们在里面演示了
如何通过使用 EDR

11
00:00:40,274 --> 00:00:43,177
来使用 AVPlayer 播放 HDR 视频

12
00:00:44,178 --> 00:00:46,246
在这次演讲中 我们将深入探讨

13
00:00:46,280 --> 00:00:49,550
如何使用 Core Media 接口来实现

14
00:00:49,583 --> 00:00:51,785
不只是 EDR 播放

15
00:00:51,818 --> 00:00:54,821
而且还有如何解码和播放 HDR 视频

16
00:00:54,855 --> 00:00:57,591
使其进入您的 EDR 层或视图中

17
00:00:59,326 --> 00:01:02,396
然后 我们将继续讨论如何通过

18
00:01:02,429 --> 00:01:05,732
Core Video 的显示链接实时访问

19
00:01:05,766 --> 00:01:08,101
解码的视频帧

20
00:01:08,135 --> 00:01:11,271
将这些帧发送到 CoreImage Filter
或 Metal Shader

21
00:01:11,305 --> 00:01:13,473
以添加颜色管理 视觉效果

22
00:01:13,507 --> 00:01:15,909
或应用其他信号处理

23
00:01:15,943 --> 00:01:20,247
最后 将生成的帧
发送到 Metal 进行渲染

24
00:01:20,280 --> 00:01:24,751
首先 我们回顾一下
与 EDR 兼容的视频媒体框架

25
00:01:24,785 --> 00:01:28,522
帮助您决定哪种最符合您的 App 要求

26
00:01:30,057 --> 00:01:32,226
接下来 我们将简要讨论高级 AVKit

27
00:01:32,259 --> 00:01:35,028
和 AVFoundation framework

28
00:01:35,062 --> 00:01:37,764
如果您的 App 需要直接播放

29
00:01:37,798 --> 00:01:40,701
这样可以完成
播放 HDR 视频的所有工作

30
00:01:42,302 --> 00:01:44,872
最后 我们将讨论

31
00:01:44,905 --> 00:01:48,175
在 EDR 播放
编辑或图像处理引擎中

32
00:01:48,208 --> 00:01:51,979
利用 Core Video 和 Metal
使用解码视频帧的最佳实践

33
00:01:54,781 --> 00:01:58,752
首先 我们快速了解一下
Apple 的视频框架

34
00:01:58,785 --> 00:02:01,355
从最高级别的接口开始

35
00:02:01,388 --> 00:02:03,323
这些是最容易使用的

36
00:02:03,357 --> 00:02:06,827
并继续使用提供更多机会的底部框架

37
00:02:06,860 --> 00:02:10,364
但代价是增加代码的复杂性

38
00:02:10,397 --> 00:02:13,467
最好使用最高级别的框架

39
00:02:13,500 --> 00:02:17,271
以充分利用自动提供的优化

40
00:02:17,304 --> 00:02:20,440
这为我们进入演讲的主体内容
做了准备

41
00:02:20,474 --> 00:02:23,177
我们将探索多种场景

42
00:02:23,210 --> 00:02:24,945
从简单的 EDR 播放

43
00:02:24,978 --> 00:02:27,981
到更复杂的解码视频帧管道

44
00:02:28,015 --> 00:02:31,652
再到 CoreImage
或 Metal 的实时处理

45
00:02:31,685 --> 00:02:34,154
最高级别 是 AVKit

46
00:02:34,188 --> 00:02:38,292
使用 AVKit 您可以创建
用于媒体播放的用户界面

47
00:02:38,325 --> 00:02:41,562
完成传输控件 章节导航

48
00:02:41,595 --> 00:02:43,030
画中画功能支持

49
00:02:43,063 --> 00:02:45,632
以及字幕和隐藏式字幕的显示

50
00:02:45,666 --> 00:02:49,136
AVKit 可以将 HDR 内容
作为 EDR 播放

51
00:02:49,169 --> 00:02:52,840
我们将使用
AVPlayerViewController 进行演示

52
00:02:52,873 --> 00:02:57,377
但如果您的 App 需要
进一步处理视频帧

53
00:02:57,411 --> 00:02:59,179
则必须使用媒体框架

54
00:02:59,213 --> 00:03:01,815
这样才能使您更好地控制管道

55
00:03:01,849 --> 00:03:04,618
接下来是 AVFoundation

56
00:03:04,651 --> 00:03:07,688
AVFoundation 是一个
功能全面的框架

57
00:03:07,721 --> 00:03:12,860
用于在 Apple 平台上
处理基于时间的视听媒体

58
00:03:12,893 --> 00:03:16,096
使用 AVFoundation
您可以轻松播放 创建

59
00:03:16,129 --> 00:03:19,032
和编辑 QuickTime 影片
和 MPEG 4 文件

60
00:03:19,066 --> 00:03:20,634
播放 HLS 流

61
00:03:20,667 --> 00:03:24,271
并在您的 App 中
构建强大的媒体功能

62
00:03:24,304 --> 00:03:26,306
在本次演讲中
我们将探讨 AVPlayer

63
00:03:26,340 --> 00:03:30,577
和相关的 AVPlayerLayer 接口的使用

64
00:03:30,611 --> 00:03:35,182
Core Video 是一个为
数字视频提供管道模型的框架

65
00:03:35,215 --> 00:03:37,417
它通过将流程划分为多个独立的步骤

66
00:03:37,451 --> 00:03:40,220
简化处理视频的方式

67
00:03:40,254 --> 00:03:43,790
Core Video 还使您可以
更轻松地访问和操作

68
00:03:43,824 --> 00:03:46,527
单个帧 而不必担心

69
00:03:46,560 --> 00:03:48,262
数据类型之间的转换

70
00:03:48,295 --> 00:03:51,465
或显示同步

71
00:03:51,498 --> 00:03:53,534
我们将用 Core Image 来演示

72
00:03:53,567 --> 00:03:56,036
DisplayLink 和使用 Core Image 的
CVPixelBuffer

73
00:03:56,069 --> 00:03:59,239
以及使用 Metal 的
CVMetalTextureCache

74
00:03:59,273 --> 00:04:01,341
接下来是 Video Toolbox

75
00:04:01,375 --> 00:04:04,011
这是一个底层框架
可以直接访问

76
00:04:04,044 --> 00:04:06,446
硬件编码器和解码器

77
00:04:06,480 --> 00:04:10,784
Video Toolbox 提供
视频压缩和解压缩服务

78
00:04:10,817 --> 00:04:13,253
以及存储在 Core Video 像素缓冲区的

79
00:04:13,287 --> 00:04:15,556
光栅图像格式之间的转换服务

80
00:04:15,589 --> 00:04:19,159
VTDecompressionSession
是一个功能强大的底层接口

81
00:04:19,193 --> 00:04:21,161
超出了本次演讲的范围

82
00:04:21,195 --> 00:04:24,631
但高级开发者可能想要进一步研究

83
00:04:24,665 --> 00:04:26,800
最后是 Core Media

84
00:04:26,834 --> 00:04:30,537
该框架定义了 AVFoundation
和其他高级媒体框架

85
00:04:30,571 --> 00:04:33,373
使用的媒体管道

86
00:04:33,407 --> 00:04:37,110
您可以始终使用 Core Media 的
底层数据类型和接口

87
00:04:37,144 --> 00:04:39,546
来有效地处理媒体样本

88
00:04:39,580 --> 00:04:41,648
和管理媒体数据队列

89
00:04:41,682 --> 00:04:45,319
在接下来的演讲中
我们将演示如何以及何时

90
00:04:45,352 --> 00:04:47,688
在您的 App 中使用这些框架

91
00:04:47,721 --> 00:04:51,225
首先 如何使用 AVKit 和 AVFoundation

92
00:04:51,258 --> 00:04:55,262
来轻松播放渲染为 EDR 的 HDR 视频

93
00:04:55,295 --> 00:04:59,399
然后是 AVPlayer 的一系列
更复杂的 App

94
00:04:59,433 --> 00:05:01,235
渲染到您自己的层

95
00:05:01,268 --> 00:05:05,105
通过 CADisplayLink
访问单独解码的帧

96
00:05:05,138 --> 00:05:09,243
并将生成的 CVPixelBuffer
发送到 Core Image 进行处理

97
00:05:09,276 --> 00:05:13,247
最后 通过 CVMetalTextureCache
以 Metal 纹理方式

98
00:05:13,280 --> 00:05:15,182
访问解码帧

99
00:05:15,215 --> 00:05:17,818
以便在 Metal 中进行处理和渲染

100
00:05:17,851 --> 00:05:22,322
现在我们已经对 Apple 平台上的
视频媒体层有了大致的了解

101
00:05:22,356 --> 00:05:26,260
我们将重点介绍 AVKit
和 AVFoundation framework

102
00:05:26,293 --> 00:05:28,795
首先 我们来讨论一下
使用 AVFoundation 的

103
00:05:28,829 --> 00:05:30,597
AVPlayer 接口

104
00:05:30,631 --> 00:05:33,433
播放 HDR 视频内容

105
00:05:33,467 --> 00:05:35,969
AVPlayer 是一个控制器对象

106
00:05:36,003 --> 00:05:39,773
用于管理媒体资源的播放和计时

107
00:05:39,806 --> 00:05:44,244
AVPlayer 接口可用于
高性能 HDR 视频播放

108
00:05:44,278 --> 00:05:47,314
在可能的情况下
自动将结果渲染为 EDR

109
00:05:48,282 --> 00:05:51,785
使用 AVPlayer 您可以播放
本地和远程基于文件的媒体

110
00:05:51,818 --> 00:05:53,820
如 QuickTime 影片

111
00:05:53,854 --> 00:05:58,025
以及使用 HLS 提供的流媒体

112
00:05:58,058 --> 00:06:02,729
本质上 AVPlayer 用于
一次播放一个媒体资源

113
00:06:02,763 --> 00:06:07,167
您可以重用播放器实例
来连续播放其他媒体资源

114
00:06:07,201 --> 00:06:12,272
甚至可以创建多个实例
来同时播放一个以上的资源

115
00:06:12,306 --> 00:06:17,277
但 AVPlayer 一次只管理
单个媒体资源的播放

116
00:06:17,311 --> 00:06:21,348
AVFoundation framework
还提供了一个名为 AVQueuePlayer 的

117
00:06:21,381 --> 00:06:24,218
AVPlayer 子类 您可以使用该子类

118
00:06:24,251 --> 00:06:29,556
创建和管理
连续 HDR 媒体资源的排队和播放

119
00:06:29,590 --> 00:06:33,193
如果您的 App 需要简单地播放
渲染到 EDR 的 HDR 视频媒体

120
00:06:33,227 --> 00:06:37,164
那么带有
AVPlayerViewController 的

121
00:06:37,197 --> 00:06:39,233
AVPlayer 可能是最好的方法

122
00:06:39,266 --> 00:06:41,668
使用 AVPlayer 和 AVPlayerLayer

123
00:06:41,702 --> 00:06:45,405
在 iOS 或 macOS 上
播放您自己的视图

124
00:06:46,640 --> 00:06:49,810
这些是使用 AVPlayer 最简单的方法

125
00:06:49,843 --> 00:06:51,845
我们来看看这两者的示例

126
00:06:51,879 --> 00:06:55,916
首先 我们将介绍如何将
AVFoundation 的 AVPlayer 接口

127
00:06:55,949 --> 00:06:59,853
与 AVKit 的
AVPlayerViewController 结合使用

128
00:06:59,887 --> 00:07:03,957
这里 我们从媒体的 URL
实例化 AVPlayer 开始

129
00:07:06,493 --> 00:07:09,630
接下来 我们创建一个
AVPlayerViewController

130
00:07:09,663 --> 00:07:12,332
然后将我们的查看器控制器的
播放器属性设置为

131
00:07:12,366 --> 00:07:15,235
我们刚刚从媒体的 URL
创建的播放器

132
00:07:18,005 --> 00:07:23,076
并以模式呈现视图控制器
以开始播放视频

133
00:07:23,110 --> 00:07:25,579
AVKit 为您管理所有细节

134
00:07:25,612 --> 00:07:29,116
并将在支持 EDR 的显示器上

135
00:07:29,149 --> 00:07:31,852
自动播放 HDR 视频作为 EDR

136
00:07:31,885 --> 00:07:35,656
正如我提到的 一些 App 需要在
自己的视图中

137
00:07:35,689 --> 00:07:37,491
播放 HDR 视频媒体

138
00:07:37,524 --> 00:07:42,563
让我们看看如何使用 AVPlayer
和 AVPlayerLayer 来实现这一点

139
00:07:42,596 --> 00:07:46,266
为了在您自己的视图中
以 EDR 的形式播放 HDR 视频媒体

140
00:07:46,300 --> 00:07:51,371
我们再次开始使用媒体的 URL
创建一个 AVPlayer

141
00:07:51,405 --> 00:07:54,474
然而 这次我们用刚刚创建的播放器

142
00:07:54,508 --> 00:07:57,444
实例化了一个 AVPlayerLayer

143
00:07:57,477 --> 00:08:00,013
接下来 我们需要在播放器层上
设置边界

144
00:08:00,047 --> 00:08:02,649
这是我们从视图中获得的

145
00:08:02,683 --> 00:08:05,385
既然播放器层有了视图的边界

146
00:08:05,419 --> 00:08:10,023
我们可以将播放器层
作为子层添加到视图中

147
00:08:10,057 --> 00:08:12,926
最后 为了播放 HDR 视频媒体

148
00:08:12,960 --> 00:08:15,596
我们调用了 AVPlayer 的播放方法

149
00:08:15,629 --> 00:08:19,333
那就是使用 AVPlayer
和 AVPlayerLayer

150
00:08:19,366 --> 00:08:23,971
在您自己的层中将 HDR 视频媒体
作为 EDR 播放所需的全部内容

151
00:08:24,004 --> 00:08:26,139
我们刚刚使用 AVPlayer 探索了

152
00:08:26,173 --> 00:08:29,443
两种最简单的 HDR 视频播放工作流程

153
00:08:29,476 --> 00:08:34,014
然而 许多 App 需要的
不仅仅是简单的媒体播放

154
00:08:35,282 --> 00:08:38,852
例如 App 可能需要对视频
进行图像处理

155
00:08:38,886 --> 00:08:43,223
如颜色分级或色度键控

156
00:08:43,257 --> 00:08:48,095
我们来探索一个从 AVPlayer
获取解码视频帧的工作流程

157
00:08:48,128 --> 00:08:51,865
应用 CoreImage filter
或 Metal shader 实时处理

158
00:08:51,899 --> 00:08:55,369
并将结果渲染为 EDR

159
00:08:55,402 --> 00:08:59,072
我们将演示如何使用 AVPlayer
和 AVPlayerItem

160
00:08:59,106 --> 00:09:03,010
从您的 HDR 视频媒体解码 EDR 帧

161
00:09:03,043 --> 00:09:06,947
从 Core Video 显示链接访问解码帧

162
00:09:06,980 --> 00:09:11,919
将生成的像素缓冲区发送到
Core Image 或 Metal 进行处理

163
00:09:11,952 --> 00:09:14,354
然后在具有 EDR 支持的显示器上

164
00:09:14,388 --> 00:09:17,958
以 EDR 的形式在
CAMetalLayer 中渲染结果

165
00:09:17,991 --> 00:09:20,360
考虑到这一点 我们将首先演示

166
00:09:20,394 --> 00:09:23,330
在 CAMetalLayer 上
设置几个关键属性

167
00:09:23,363 --> 00:09:28,202
这些属性是确保 HDR 媒体
正确渲染为 EDR 所必需的

168
00:09:28,235 --> 00:09:30,337
首先 我们需要获得 CAMetalLayer

169
00:09:30,370 --> 00:09:34,274
然后将 HDR 视频内容渲染到它上面

170
00:09:34,308 --> 00:09:37,177
在该层上 我们通过将

171
00:09:37,211 --> 00:09:40,614
wantsExtendedDynamicRangeContent
标志设置为真来选择 EDR

172
00:09:42,883 --> 00:09:48,222
请确保使用支持 Extended Dynamic Range
内容的像素格式

173
00:09:48,255 --> 00:09:52,459
对于下面的 AVPlayer 示例
我们将把 CAMetalLayer 设置为

174
00:09:52,492 --> 00:09:54,528
使用半浮点像素格式

175
00:09:54,561 --> 00:09:58,265
但是结合 PQ 或 HLG 传递函数

176
00:09:58,298 --> 00:10:01,535
使用的 10 位格式也可以

177
00:10:01,568 --> 00:10:03,904
为了避免将结果限制为 SDR

178
00:10:03,937 --> 00:10:06,707
我们还需要将层设置为

179
00:10:06,740 --> 00:10:08,909
EDR 兼容的扩展范围色彩空间

180
00:10:10,577 --> 00:10:13,580
在我们的示例中
我们将半浮动 Metal 纹理设置为

181
00:10:13,614 --> 00:10:18,151
扩展的线性 Display P3 色彩空间

182
00:10:18,185 --> 00:10:20,487
我们只是初步了解了 EDR

183
00:10:20,521 --> 00:10:23,090
色彩空间和像素缓冲区格式

184
00:10:23,123 --> 00:10:25,626
您可能想查看我去年的讲座

185
00:10:25,659 --> 00:10:27,528
“HDR rendering with EDR”

186
00:10:27,561 --> 00:10:31,999
以及今年的 “探索 iOS 上的 EDR”
以了解更多信息

187
00:10:33,300 --> 00:10:36,303
现在 我们已经设置了
CAMetalLayer 的基本属性

188
00:10:36,336 --> 00:10:39,873
我们还要继续演示
使用 Core Image 或 Metal shader

189
00:10:39,907 --> 00:10:42,376
添加实时图像处理

190
00:10:42,409 --> 00:10:45,412
我们将结合 AVPlayer 使用显示链接

191
00:10:45,445 --> 00:10:48,415
来实时访问解码的视频帧

192
00:10:49,449 --> 00:10:53,820
对于此工作流程 首先
从 AVPlayerItem 创建 AVPlayer

193
00:10:53,854 --> 00:10:57,291
接下来 实例化
AVPlayerItemVideoOutput

194
00:10:57,324 --> 00:11:02,496
为 EDR 配置适当的
像素缓冲区格式和色彩空间

195
00:11:02,529 --> 00:11:05,399
然后创建并配置 Display Link

196
00:11:05,432 --> 00:11:08,669
最后 运行 Display Link
将像素缓冲区发送到

197
00:11:08,702 --> 00:11:11,805
Core Image 或 Metal 进行处理

198
00:11:11,839 --> 00:11:16,310
我们将演示在 iOS 上
使用的 CADisplayLink

199
00:11:16,343 --> 00:11:21,381
在为 macOS 开发时
请使用等效的 CVDisplayLink

200
00:11:21,415 --> 00:11:24,084
这一次 我们选择从媒体的 URL
创建一个

201
00:11:24,117 --> 00:11:26,486
AVPlayerItem

202
00:11:26,520 --> 00:11:32,326
并用我们刚刚创建的 AVPlayerItem
实例化 AVPlayer

203
00:11:32,359 --> 00:11:35,562
现在我们创建一对字典
来指定解码帧的

204
00:11:35,596 --> 00:11:38,999
色彩空间和像素缓冲区格式

205
00:11:39,032 --> 00:11:41,635
第一个字典 videoColorProperties

206
00:11:41,668 --> 00:11:45,172
是指定色彩空间和传递函数的地方

207
00:11:45,205 --> 00:11:48,475
在本例中
我们要求的是 Display P3 色彩空间

208
00:11:48,509 --> 00:11:51,945
该色彩空间对应于大多数
Apple 显示器的色彩空间

209
00:11:51,979 --> 00:11:55,115
以及线性传递函数
该传递函数允许 AVFoundation

210
00:11:55,148 --> 00:11:58,852
保持 EDR 所需的扩展范围值

211
00:12:00,153 --> 00:12:02,756
第二个字典 outputVideoSettings

212
00:12:02,789 --> 00:12:06,026
指定了像素缓冲区格式的特征

213
00:12:06,059 --> 00:12:08,929
并为我们刚刚创建的
视频色彩属性字典

214
00:12:08,962 --> 00:12:11,899
提供了参考

215
00:12:11,932 --> 00:12:17,504
在本例中 我们请求广色域
和半浮点像素缓冲区格式

216
00:12:17,538 --> 00:12:20,908
AVPlayerItemVideoOutput
非常有用

217
00:12:20,941 --> 00:12:24,378
它不仅可以将视频解码为
我们在输出设置字典中

218
00:12:24,411 --> 00:12:26,480
指定的像素缓冲区格式

219
00:12:26,513 --> 00:12:30,517
还可以通过
像素传输会话自动执行所需的

220
00:12:30,551 --> 00:12:34,021
任何色彩转换

221
00:12:34,054 --> 00:12:36,890
回想一下
一个视频可能包含多个剪辑

222
00:12:36,924 --> 00:12:39,393
可能具有不同的色彩空间

223
00:12:39,426 --> 00:12:42,796
AVFoundation 会自动
为我们管理这些

224
00:12:42,829 --> 00:12:45,065
正如我们即将演示的那样

225
00:12:45,098 --> 00:12:48,368
这种行为还允许将解码后的视频帧

226
00:12:48,402 --> 00:12:51,338
发送到 Metal 等底层框架

227
00:12:51,371 --> 00:12:54,341
而这些框架本身并不会
自动将色彩空间转换为

228
00:12:54,374 --> 00:12:56,643
显示器的色彩空间

229
00:12:57,377 --> 00:13:00,447
现在 我们使用
outputVideoSettings 字典

230
00:13:00,480 --> 00:13:03,784
创建 AVPlayerItemVideoOutput

231
00:13:03,817 --> 00:13:06,353
第三步 我们设置 Display Link

232
00:13:06,386 --> 00:13:10,824
用于实时访问解码的帧

233
00:13:10,858 --> 00:13:15,562
CADisplayLink 在每次
显示更新时执行回调

234
00:13:15,596 --> 00:13:19,366
在我们的例子中
我们调用了一个局部函数

235
00:13:19,399 --> 00:13:24,805
来获取 CVPixelBuffer
我们将把它发送给 CoreImage 进行处理

236
00:13:24,838 --> 00:13:27,508
接下来 我们创建一个
视频播放器项目观察器

237
00:13:27,541 --> 00:13:31,512
以允许我们处理
对指定 Player Item 属性的更改

238
00:13:33,113 --> 00:13:35,516
我们的示例将在

239
00:13:35,549 --> 00:13:38,652
每次 Player Item 的状态更改时
执行此代码

240
00:13:41,088 --> 00:13:44,191
当播放器项目的状态更改为
readyToPlay

241
00:13:44,224 --> 00:13:47,060
我们将我们的
AVPlayerItemVideoOutput

242
00:13:47,094 --> 00:13:52,165
添加到刚刚返回的
新 AVPlayerItem 中

243
00:13:52,199 --> 00:13:57,070
注册 CADisplayLink
将主运行循环设置为普通模式

244
00:13:57,104 --> 00:13:59,673
并通过调用视频播放器上的播放

245
00:13:59,706 --> 00:14:02,976
来启动 HDR 视频的实时解码

246
00:14:04,478 --> 00:14:08,849
最后 我们来看看 CADisplayLink
回调实现的例子

247
00:14:08,882 --> 00:14:12,653
我们之前称之为
displayLinkCopyPixelBuffer

248
00:14:12,686 --> 00:14:15,355
的局部函数

249
00:14:15,389 --> 00:14:17,591
HDR 视频开始播放后

250
00:14:17,624 --> 00:14:22,629
每次刷新显示时都会调用
CADisplayLink 回调函数

251
00:14:22,663 --> 00:14:27,401
例如 对于典型的显示
它可能每秒被调用 60次

252
00:14:27,434 --> 00:14:30,671
如果有一个新的 CVPixelBuffer
我们的代码

253
00:14:30,704 --> 00:14:34,141
就有机会更新显示的帧

254
00:14:34,174 --> 00:14:37,611
在每次显示回调时
我们尝试复制一个 CVPixelBuffer

255
00:14:37,644 --> 00:14:40,447
其中包含要在当前挂钟时间显示的

256
00:14:40,480 --> 00:14:43,717
解码视频帧

257
00:14:43,750 --> 00:14:47,054
然而 复制像素缓冲区调用可能会失败

258
00:14:47,087 --> 00:14:50,123
因为在每次显示器刷新时
并不总是有新的

259
00:14:50,157 --> 00:14:52,359
CVPixelBuffer 可用

260
00:14:52,392 --> 00:14:56,797
尤其是当屏幕刷新率超过
正在播放的视频的刷新率时

261
00:14:56,830 --> 00:14:59,032
如果没有新的 CVPixelBuffer

262
00:14:59,066 --> 00:15:01,735
则调用失败 我们跳过渲染

263
00:15:01,768 --> 00:15:06,373
这会使前一帧保持在屏幕上
以进行另一次显示刷新

264
00:15:06,406 --> 00:15:10,143
但如果复制成功
那么我们在 CVPixelBuffer 中

265
00:15:10,177 --> 00:15:12,112
就有一个新的视频帧

266
00:15:12,145 --> 00:15:16,350
我们可以通过多种方式
处理和渲染这个新的帧

267
00:15:16,383 --> 00:15:19,219
一个方法是将 CVPixelBuffer
发送到 Core Image

268
00:15:19,253 --> 00:15:21,388
进行处理

269
00:15:21,421 --> 00:15:24,424
Core Image 可以将一个或多个
CIFilter 串在一起

270
00:15:24,458 --> 00:15:28,195
以向视频帧提供
GPU 加速的图像处理

271
00:15:29,596 --> 00:15:33,066
请注意 并非所有 CIFilter
都与 EDR 兼容

272
00:15:33,100 --> 00:15:35,369
并且可能在处理 HDR 内容时
遇到问题

273
00:15:35,402 --> 00:15:38,772
包括钳位到 SDR 或更糟

274
00:15:38,805 --> 00:15:42,476
Core Image 提供了许多
EDR 兼容的滤镜

275
00:15:42,509 --> 00:15:46,180
通过
CICategoryHighDynamicRange

276
00:15:46,213 --> 00:15:49,917
调用 filterNames 来枚举
与 EDR 兼容的 CoreImage 滤镜

277
00:15:49,950 --> 00:15:53,787
在我们的示例中 我们将添加一个
简单的色调效果

278
00:15:53,820 --> 00:15:58,025
现在 我们回到例子中
并集成 Core Image

279
00:15:58,058 --> 00:16:01,461
在每个生成新 CVPixelBuffer 的
Display Link 回调上

280
00:16:01,495 --> 00:16:04,398
从该像素缓冲区创建 CIImage

281
00:16:06,099 --> 00:16:09,369
实例化 CIFilter 以实现所需的效果

282
00:16:09,403 --> 00:16:13,574
我之所以使用色调滤镜
是因为它没有参数 非常简单

283
00:16:13,607 --> 00:16:16,810
但系统中内置了许多 CIFilter

284
00:16:16,844 --> 00:16:20,314
您也可以直接编写自己的滤镜

285
00:16:20,347 --> 00:16:24,685
将 CIFilter 的输入图像设置为
我们刚刚创建的 CIImage

286
00:16:26,153 --> 00:16:29,022
处理后的视频结果将出现在

287
00:16:29,056 --> 00:16:32,326
滤镜的输出图像中

288
00:16:32,359 --> 00:16:37,097
根据需要将尽可能多的 CIFilter
连接在一起 以实现您想要的效果

289
00:16:37,130 --> 00:16:39,399
然后使用 CIRenderDestination

290
00:16:39,433 --> 00:16:42,970
将结果图像渲染到
App 的视图代码中

291
00:16:44,805 --> 00:16:50,043
请参考 WWDC 2020 的演讲
“Applying CI Effects to Video”

292
00:16:50,077 --> 00:16:51,845
进一步了解有关此工作流程的信息

293
00:16:51,879 --> 00:16:55,883
另一个方法是使用 Metal
和自定义 Metal shader

294
00:16:55,916 --> 00:16:59,119
来处理和渲染新的 CVPixelBuffer

295
00:16:59,152 --> 00:17:02,523
我们将简要描述
将 CVPixelBuffer 转换为

296
00:17:02,556 --> 00:17:04,057
Metal 纹理的过程

297
00:17:04,091 --> 00:17:07,828
然而 实现这种转换并保持最佳性能
是一个有深度的话题

298
00:17:07,861 --> 00:17:10,764
最好留待下次讨论

299
00:17:10,797 --> 00:17:13,400
我们建议从
CoreVideo 的 Metal 纹理缓存中

300
00:17:13,433 --> 00:17:15,569
导出 Metal 纹理

301
00:17:15,602 --> 00:17:19,640
并将此过程作为
本次演讲的最后一个示例

302
00:17:19,673 --> 00:17:24,711
一般而言 该过程是从
CVPixelBuffer 中获取 IOSurface

303
00:17:24,745 --> 00:17:27,014
创建一个 MetalTextureDescriptor

304
00:17:27,047 --> 00:17:29,483
然后使用 newTextureWithDescriptor

305
00:17:29,516 --> 00:17:32,319
从该 Metal 设备
创建一个 Metal 纹理

306
00:17:33,887 --> 00:17:37,324
但如果没有小心处理锁
纹理可能会被

307
00:17:37,357 --> 00:17:41,428
重复使用和过度绘制

308
00:17:41,461 --> 00:17:45,799
此外 并不是所有的像素缓冲区格式
都被 Metal 纹理支持

309
00:17:45,832 --> 00:17:49,169
这就是为什么我们在本例中
使用半浮点

310
00:17:49,203 --> 00:17:51,972
由于这些复杂性 我们建议直接

311
00:17:52,005 --> 00:17:56,677
从 CoreVideo 获取 Metal 纹理
正如我们现在将要演示的

312
00:17:56,710 --> 00:18:00,147
我们来进一步探索
Core Video 和 Metal

313
00:18:00,180 --> 00:18:03,851
如前所述
CVMetalTextureCache 是使用

314
00:18:03,884 --> 00:18:07,421
CVPixelBuffer 和 Metal 的
一种简单而有效的方法

315
00:18:07,454 --> 00:18:10,757
CVMetalTextureCache 很方便
因为您可以直接

316
00:18:10,791 --> 00:18:14,862
从缓存中获得 Metal 纹理
而不需要进一步的转换

317
00:18:14,895 --> 00:18:18,031
CVMetalTextureCache 自动在

318
00:18:18,065 --> 00:18:21,101
CVPixelBuffer 和 Metal 纹理
之间建立桥梁

319
00:18:21,134 --> 00:18:26,073
从而简化代码并保持快速运行

320
00:18:26,106 --> 00:18:28,876
与 CVPixelBufferPool
结合使用时

321
00:18:28,909 --> 00:18:32,646
CVMetalTextureCache
还通过保持

322
00:18:32,679 --> 00:18:36,116
Metal 纹理到 IOSurface 的实时映射
来提供性能优势

323
00:18:37,784 --> 00:18:41,054
最后 使用 CVMetalTextureCache

324
00:18:41,088 --> 00:18:43,557
消除了手动跟踪 IOSurface 的需要

325
00:18:43,590 --> 00:18:46,293
现在是我们演讲的最后一个例子

326
00:18:46,326 --> 00:18:49,196
如何使用 CVMetalTextureCache

327
00:18:49,229 --> 00:18:51,698
直接从 Core Video 中
提取 Metal 纹理

328
00:18:52,499 --> 00:18:55,469
这里 我们从获取
系统默认 Metal 设备开始

329
00:18:55,502 --> 00:18:58,071
我们用它来创建一个 Metal 纹理缓存

330
00:18:58,105 --> 00:19:01,341
然后实例化一个
与 Metal 纹理缓存关联的

331
00:19:01,375 --> 00:19:04,111
Core Video Metal 纹理缓存

332
00:19:04,144 --> 00:19:08,549
然后可以使用它来访问
解码的视频帧作为 Metal 纹理

333
00:19:08,582 --> 00:19:13,353
可以方便地直接在我们的
Metal 引擎中使用

334
00:19:13,387 --> 00:19:18,292
在本例中 我们创建并使用
Metal 系统默认设备

335
00:19:18,325 --> 00:19:21,261
接下来 我们使用
CVMetalTextureCacheCreate

336
00:19:21,295 --> 00:19:23,530
创建 CVMetalTextureCache

337
00:19:23,564 --> 00:19:26,967
指定我们刚刚创建的 Metal 设备

338
00:19:27,000 --> 00:19:29,970
我们得到了创建
Core Video Metal 纹理

339
00:19:30,003 --> 00:19:33,574
所需的 CVPixelBuffer 的
高度和宽度

340
00:19:33,607 --> 00:19:37,511
然后 我们调用
CVMetalTextureCacheCreateTextureFromImage

341
00:19:37,544 --> 00:19:40,247
来实例化一个
CVMetalTexture 对象

342
00:19:40,280 --> 00:19:43,750
并将其与 CVPixelBuffer 相关联

343
00:19:43,784 --> 00:19:46,653
最后 我们调用
CVMetalTextureGetTexture

344
00:19:46,687 --> 00:19:50,057
以获得所需的 Metal 纹理

345
00:19:50,090 --> 00:19:54,394
Swift App 应使用
CVMetalTexture 的强引用

346
00:19:54,428 --> 00:19:57,731
但是 当使用对象 C 时
您必须确保 Metal

347
00:19:57,764 --> 00:20:01,835
在您释放 CVMetalTextureRef.
之前已经完成了您的纹理

348
00:20:01,869 --> 00:20:05,806
这可以使用
metal 命令缓冲区完成处理程序来完成

349
00:20:07,674 --> 00:20:09,042
就是这些了 各位

350
00:20:09,076 --> 00:20:11,678
回顾一下 我们探讨了一些

351
00:20:11,712 --> 00:20:14,715
将 HDR 视频媒体渲染到
EDR 的工作流程

352
00:20:14,748 --> 00:20:17,684
用于播放 编辑或图像处理

353
00:20:18,685 --> 00:20:23,190
大家学习了如何从 AVPlayer
切换到 AVKit 的 AVPlayerViewController

354
00:20:23,223 --> 00:20:26,360
以播放 HDR 媒体

355
00:20:26,393 --> 00:20:30,264
大家还了解了如何使用 AVPlayer
和 AVPlayerLayer

356
00:20:30,297 --> 00:20:34,234
在您自己的视图上显示 HDR 媒体

357
00:20:34,268 --> 00:20:38,505
最后 我们探讨了
如何在播放过程中添加实时效果

358
00:20:38,539 --> 00:20:42,009
将 AVFoundation 的 AVPlayer
连接到 CoreVideo

359
00:20:42,042 --> 00:20:44,178
然后连接到 Metal 进行渲染

360
00:20:44,211 --> 00:20:46,947
以及使用 CoreImage 滤镜

361
00:20:46,980 --> 00:20:49,082
和 metal shader 应用实时效果

362
00:20:51,852 --> 00:20:55,789
如您想深入了解 我会推荐几个

363
00:20:55,822 --> 00:20:58,258
与创建视频工作流程

364
00:20:58,292 --> 00:21:02,429
以及将 HDR 媒体与 EDR 整合
相关的 WWDC 讲座

365
00:21:02,462 --> 00:21:04,565
我强烈推荐大家观看讲座

366
00:21:04,598 --> 00:21:08,368
“Edit and Playback HDR video
with AVFoundation”

367
00:21:08,402 --> 00:21:11,505
这期讲座探讨了
AVVideoComposition 的使用

368
00:21:11,538 --> 00:21:17,211
通过 applyingCIFiltersWithHandler
将效果应用到 HDR 媒体

369
00:21:17,244 --> 00:21:20,414
在本期讲座中 您还会学到
如何使用自定义合成器

370
00:21:20,447 --> 00:21:23,183
当每个视频帧可供处理时

371
00:21:23,217 --> 00:21:26,820
它可以与 CVPixelBuffer 一起使用

372
00:21:26,854 --> 00:21:29,857
正如我在开始时提到的
今年我们还将举办

373
00:21:29,890 --> 00:21:33,827
另外两场关于 EDR 的讲座
“探索 iOS 上的 EDR”

374
00:21:33,861 --> 00:21:38,665
我们在里面宣布了 EDR API
支持已扩展到包括 iOS

375
00:21:38,699 --> 00:21:44,204
以及 “借助 Core Image、
Metal 和 SwiftUI 显示 EDR 内容“

376
00:21:44,238 --> 00:21:48,976
这节讲座进一步探索 EDR
与其他媒体框架的集成

377
00:21:49,009 --> 00:21:52,946
希望您能把 HDR 视频整合到 macOS

378
00:21:52,980 --> 00:21:56,116
和现在 iOS 上的开启了 EDR 的 App 中

379
00:21:56,149 --> 00:21:57,751
感谢收看

