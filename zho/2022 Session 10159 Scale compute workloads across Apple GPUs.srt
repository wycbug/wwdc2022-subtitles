1
00:00:00,434 --> 00:00:06,440
[欢快的音乐]

2
00:00:09,309 --> 00:00:11,478
大家好 欢迎

3
00:00:11,512 --> 00:00:13,413
我是 Marco Giordano

4
00:00:13,447 --> 00:00:17,050
是 Apple 的
GPU 软件工程团队的成员

5
00:00:17,084 --> 00:00:19,620
在本期讲座中 我将给大家讲一下

6
00:00:19,653 --> 00:00:23,357
如何将工作负载分配到
多个 Apple M1 GPU 上

7
00:00:23,390 --> 00:00:26,727
如果您从事复杂的计算工作
想知道如何

8
00:00:26,760 --> 00:00:31,031
充分利用 Apple 芯片硬件
并实现非常好的可扩展性

9
00:00:31,064 --> 00:00:33,433
那么这个演讲就是为您准备的

10
00:00:33,467 --> 00:00:36,837
我将首先讨论计算可扩展性的概念

11
00:00:36,870 --> 00:00:42,276
以及 App 如何自然地在
M1 GPU 家族中扩展性能

12
00:00:42,309 --> 00:00:45,846
然后 我将一步一步地分享
使用方法

13
00:00:45,879 --> 00:00:48,782
并讨论哪些工具可用于

14
00:00:48,815 --> 00:00:52,252
最大化您的工作负载的计算可扩展性

15
00:00:52,286 --> 00:00:55,122
让我们首先了解一下什么是可扩展性

16
00:00:55,155 --> 00:00:57,991
以及它为什么对您的工作负载很重要

17
00:00:59,293 --> 00:01:02,829
Apple M1 GPU
从开始就为可扩展而设计

18
00:01:02,863 --> 00:01:08,535
它能让您的工作负载
在整个 SoC 家族中实现卓越的性能

19
00:01:08,569 --> 00:01:13,407
相同的 GPU
从 8 核 iPad 到 64 核 Mac Studio

20
00:01:13,440 --> 00:01:16,476
都支持所有 Metal 3 功能

21
00:01:17,544 --> 00:01:20,214
为了利用高水平的可扩展性

22
00:01:20,247 --> 00:01:24,551
为 M1 优化过的 App
是一个很好的起点

23
00:01:24,585 --> 00:01:29,089
许多著名的高端 App 已经
针对 Apple M1 进行了优化

24
00:01:29,122 --> 00:01:33,260
并在所有设备上都达到了
出色的可扩展性

25
00:01:34,962 --> 00:01:39,933
例如 这里我们有
Affinity Photo 和 DaVinci Resolve

26
00:01:39,967 --> 00:01:43,971
后期制作行业的照片和视频编辑器

27
00:01:44,004 --> 00:01:47,608
这些 App 都达到了优秀可扩展性

28
00:01:47,641 --> 00:01:53,447
让我们来定义可扩展性的真正含义
以及如何实现 “理想的” 可扩展性

29
00:01:53,480 --> 00:01:58,385
GPU 工作负载可扩展性
是指通过增加 GPU 内核数量

30
00:01:58,418 --> 00:02:01,555
来提高性能的能力

31
00:02:01,588 --> 00:02:04,658
右边的图表显示了 App
随着GPU核心数的增加

32
00:02:04,691 --> 00:02:07,194
也在加速

33
00:02:07,227 --> 00:02:10,964
线性比例改进被认为是理想的

34
00:02:12,232 --> 00:02:16,403
然而 当您在做您的 App 时
您可能会注意到一种类型的扩展

35
00:02:16,436 --> 00:02:20,307
它会进入一个平台期
随扩展的回报逐渐减少

36
00:02:20,340 --> 00:02:24,945
或者由于 GPU 时间轴的间隙
而根本无法扩展

37
00:02:25,579 --> 00:02:29,983
或者您可能会看到
另一种类型的扩展 性能有所提高

38
00:02:30,017 --> 00:02:32,819
但在各阶段中并不统一

39
00:02:32,853 --> 00:02:37,157
有些地方工作负载
会达到某些 GPU 限制

40
00:02:37,191 --> 00:02:42,930
比如这里的 24 到 32 核
或 48 到 64 核

41
00:02:44,431 --> 00:02:48,569
您的目标是尽可能接近线性扩展

42
00:02:48,602 --> 00:02:51,338
我将向您展示识别瓶颈和实现

43
00:02:51,371 --> 00:02:55,108
您想要的结果的工具和技术

44
00:02:56,276 --> 00:03:01,682
在下个部分 我将讨论
最大化 GPU 扩展性的方法

45
00:03:01,715 --> 00:03:06,954
对于每个工作负载
首先应该确定瓶颈在哪里

46
00:03:06,987 --> 00:03:11,225
工作负载会受限于计算或带宽

47
00:03:11,258 --> 00:03:14,027
在优化过程中

48
00:03:14,061 --> 00:03:16,930
您可能会在两者之间来回切换

49
00:03:16,964 --> 00:03:21,635
如果您有受限于算力
您可以尝试转移一些负载

50
00:03:21,668 --> 00:03:26,039
利用内存来减少计算 反之亦然

51
00:03:26,073 --> 00:03:29,576
当您扩充时 瓶颈可能会发生变化

52
00:03:29,610 --> 00:03:32,379
一个好的解决方案是
使用像 MPS 或 MPSGraph

53
00:03:32,412 --> 00:03:34,982
这样的 Apple 框架

54
00:03:35,015 --> 00:03:37,184
如果可以利用它们的原语

55
00:03:37,217 --> 00:03:41,522
我们就可以确保每个计算内核
在所有硬件上都能运行得最好

56
00:03:41,555 --> 00:03:44,725
然而 您不能用 MPS 替换一切

57
00:03:44,758 --> 00:03:48,529
所以配置并理解您的工作负载
至关重要

58
00:03:50,297 --> 00:03:51,164
我将首先介绍三个
可以帮助最小化 GPU 间隙的事项

59
00:03:54,868 --> 00:03:58,839
改进工作分配 消除 GPU 时间轴间隙

60
00:03:58,872 --> 00:04:01,508
以及对原子操作的考虑

61
00:04:02,476 --> 00:04:06,613
然后 我将解释
如何优化 GPU 限制

62
00:04:06,647 --> 00:04:10,951
首先调研工作负载的计算网格尺寸

63
00:04:10,984 --> 00:04:13,387
和内存布局的影响

64
00:04:13,420 --> 00:04:18,892
然后研究 Blender Cycles 中的
一个特定示例

65
00:04:18,926 --> 00:04:22,362
首先专注于最小化 GPU 间隙

66
00:04:22,396 --> 00:04:26,967
这种扩展可能是
GPU 没有被充分利用的结果

67
00:04:27,000 --> 00:04:30,070
GPU 时间轴上
存在硬件空闲间隙

68
00:04:32,139 --> 00:04:35,976
让我们看看是否可以通过
研究任务分布来改善可扩展性

69
00:04:37,544 --> 00:04:41,715
小的工作负载
通常不会使整个 GPU 饱和

70
00:04:41,748 --> 00:04:44,418
而且内核同步也有一定的成本

71
00:04:44,451 --> 00:04:47,955
因此两者都可能妨碍适当的扩展性

72
00:04:47,988 --> 00:04:53,093
理解工作负载如何映射到硬件
是非常重要的

73
00:04:53,126 --> 00:04:54,928
所以我们来讨论一下这一点

74
00:04:55,696 --> 00:05:00,267
工作负载以线程组
3D 网格的形式分派

75
00:05:00,300 --> 00:05:04,137
线程组被均匀地分布到
GPU 核心中

76
00:05:04,171 --> 00:05:08,976
并且可以访问GPU内核本地
大小有限 但是非常快速的

77
00:05:09,009 --> 00:05:11,945
线程组内存

78
00:05:12,913 --> 00:05:16,783
单个线程组被进一步
分解为 SIMD 组

79
00:05:16,817 --> 00:05:21,088
在不同的计算词汇中
也称为 wave 或 warp

80
00:05:21,989 --> 00:05:26,059
在计算管道状态对象上检查
“threadExecutionWidth”

81
00:05:26,093 --> 00:05:28,161
将返回 SIMD 并行宽度

82
00:05:28,195 --> 00:05:31,765
在所有 Apple GPU 上
它都等于 32

83
00:05:33,033 --> 00:05:37,337
每个线程组最多可以
有 1024 个线程

84
00:05:37,371 --> 00:05:41,575
线程可以共享最多 32K 的
线程组内存

85
00:05:42,876 --> 00:05:45,679
为了保持 GPU 忙碌
所有 GPU 核心

86
00:05:45,712 --> 00:05:47,347
都应该有足够的工作要做

87
00:05:48,749 --> 00:05:51,618
下面是一个要分派的网格的示例

88
00:05:51,652 --> 00:05:54,555
线程组会被分配到 GPU 集群中

89
00:05:54,588 --> 00:05:58,091
并分散到 GPU 核心中

90
00:05:59,159 --> 00:06:01,228
如果线程组太少

91
00:06:01,261 --> 00:06:04,565
工作负载就不会使机器完全饱和

92
00:06:04,598 --> 00:06:06,033
下面是解决这个问题的方法

93
00:06:08,101 --> 00:06:11,438
首先计算工作负载会产生多少线程

94
00:06:11,471 --> 00:06:15,475
大致查看分派是否会
使整个机器饱和

95
00:06:16,410 --> 00:06:22,082
对于相对复杂的内核
每着色器核心有 1K 到 2K 并发线程

96
00:06:22,115 --> 00:06:24,651
被认为是非常好的占用情况

97
00:06:24,685 --> 00:06:30,591
所以每个 GPU 核
1 到 2K 线程是一个经验法则

98
00:06:30,624 --> 00:06:35,596
现在 如果有足够的工作
使硬件完全饱和 就可以进行计算了

99
00:06:35,629 --> 00:06:39,032
这里的表显示了
使不同 SoC 饱和的

100
00:06:39,066 --> 00:06:41,301
最低推荐线程数

101
00:06:43,670 --> 00:06:45,739
另一件需要考虑的事
是避免使用不必要的

102
00:06:45,772 --> 00:06:48,909
大线程组尺寸

103
00:06:48,942 --> 00:06:54,481
更小的线程组可以更均匀地
将负载映射到硬件

104
00:06:54,515 --> 00:06:58,752
使用更大的线程组可能会
阻止更均匀的分布

105
00:06:58,785 --> 00:07:01,588
导致 GPU 核心不平衡

106
00:07:02,890 --> 00:07:05,926
最好使用能够很好地
映射到工作负载的

107
00:07:05,959 --> 00:07:07,995
SIMD 宽度的最小倍数

108
00:07:08,629 --> 00:07:12,165
通过使用更小的线程组
GPU 有更多的机会

109
00:07:12,199 --> 00:07:14,668
更好地平衡其工作负载

110
00:07:16,170 --> 00:07:19,039
请经常使用 Xcode
或 Instruments GPU 工具

111
00:07:19,072 --> 00:07:21,942
检查您的内核运行时性能

112
00:07:23,443 --> 00:07:28,882
例如 在这个 GPU 捕获中
有一个内核在执行一些计算

113
00:07:28,916 --> 00:07:32,653
占用率很低 这是意料之外的

114
00:07:32,686 --> 00:07:36,089
编译器统计数据显示
最大理论占用率是100%

115
00:07:36,123 --> 00:07:40,060
这是 Xcode 14 中的新数据

116
00:07:40,093 --> 00:07:43,997
这表明可能没有足够的线程 实际上

117
00:07:44,031 --> 00:07:48,569
我们可以看到
算法开始分派越来越少的线程

118
00:07:48,602 --> 00:07:50,604
不再使机器饱和

119
00:07:51,805 --> 00:07:54,975
占用率低可能还有其他几个原因

120
00:07:55,008 --> 00:08:01,014
要了解所有细节 请查看演讲
Metal Compute on MacBook Pro Tech talk

121
00:08:01,849 --> 00:08:05,152
好了 现在工作负载已经正确分配

122
00:08:05,185 --> 00:08:08,655
是时候确保 GPU 一直忙碌了

123
00:08:09,957 --> 00:08:13,393
低利用率的 GPU
永远不会带来理想的扩展性

124
00:08:13,427 --> 00:08:18,131
最糟糕的情况是让 GPU 闲置

125
00:08:18,165 --> 00:08:21,768
GPU 时间轴间隙
会导致 GPU 空闲

126
00:08:23,971 --> 00:08:26,807
考虑一下这个例子

127
00:08:26,840 --> 00:08:30,110
这是一个因为 CPU
和 GPU 之间的工作序列化

128
00:08:30,143 --> 00:08:34,248
导致只使用了 50% GPU 的工作负载

129
00:08:34,281 --> 00:08:39,152
在这种情况下
总的任务时间是 CPU 和 GPU

130
00:08:39,186 --> 00:08:41,455
无重叠的工作总和

131
00:08:42,489 --> 00:08:46,727
GPU 核数加倍会使
GPU 轨迹完成速度更快

132
00:08:46,760 --> 00:08:49,796
但 CPU 轨迹不会受影响

133
00:08:49,830 --> 00:08:56,103
整体性能只提高了 33%
与理想的扩展性相差甚远

134
00:08:57,237 --> 00:09:02,676
如果 GPU 核心再次翻倍
GPU 上的工作负载甚至会更快

135
00:09:02,709 --> 00:09:08,315
但总体延迟只比原来减少了 60%

136
00:09:08,348 --> 00:09:13,053
因此在这种情况下
GPU 核心的扩展带来的回报被抵消

137
00:09:13,086 --> 00:09:16,089
这远非理想 让我们解决它吧

138
00:09:17,858 --> 00:09:23,397
M1 Pro 的 Instrument trace 显示了
很大的 GPU 时间间隔

139
00:09:23,430 --> 00:09:26,667
这明显将阻止适当的扩展

140
00:09:28,035 --> 00:09:31,738
在 M1 Ultra 上 相同的工作负载
确实更快一些

141
00:09:31,772 --> 00:09:34,441
但 GPU 空闲时间变长了

142
00:09:34,474 --> 00:09:38,278
工作负载不能很好地扩展

143
00:09:38,312 --> 00:09:41,348
大间隙是因为 CPU 在命令缓冲区上

144
00:09:41,381 --> 00:09:44,651
使用 waitUntilCompleted 同步引起的

145
00:09:45,552 --> 00:09:49,189
在改变了等待逻辑
并移除序列化之后

146
00:09:49,223 --> 00:09:53,327
GPU 得到了充分的利用
这是非常棒的

147
00:09:54,661 --> 00:09:56,029
比较工作负载

148
00:09:56,063 --> 00:09:57,030
扩展前后的情况

149
00:09:57,064 --> 00:09:58,765
我们可以得出结论

150
00:09:58,799 --> 00:10:01,635
扩展性已经更加接近理想状态了

151
00:10:03,403 --> 00:10:06,773
在前面的例子中
完全移除 CPU/GPU 同步

152
00:10:06,807 --> 00:10:09,810
是可能的

153
00:10:09,843 --> 00:10:15,482
但由于您的 App 的性质
这并非总是如此

154
00:10:15,516 --> 00:10:20,587
还有其他方法可以减少空闲时间

155
00:10:20,621 --> 00:10:23,724
使用 MTLSharedEvents 来通知 CPU

156
00:10:23,757 --> 00:10:27,961
输送更多的工作
考虑使用 GPU 驱动的编码

157
00:10:27,995 --> 00:10:30,497
并使用并发调度

158
00:10:30,531 --> 00:10:35,335
所以让我们来讨论一下
那些最小化 GPU 时间间隔的方法

159
00:10:35,369 --> 00:10:37,905
它们中的一些可能适合您的工作流程

160
00:10:39,139 --> 00:10:44,378
等待 CPU 完成 GPU
会导致扩展性不理想

161
00:10:44,411 --> 00:10:46,947
如果您的 App
正在使用 WaitUntilCompleted

162
00:10:46,980 --> 00:10:50,584
您可能会想尝试
使用 MTLSharedEvents 代替

163
00:10:51,919 --> 00:10:54,555
MTLSharedEvent 具有较低的开销

164
00:10:54,588 --> 00:10:57,724
可以帮助您减少时间间隔

165
00:10:57,758 --> 00:10:58,959
接下来要考虑的事情是

166
00:10:58,992 --> 00:11:01,228
工作负载管线化

167
00:11:02,329 --> 00:11:04,498
如果算法拥有下一批处理

168
00:11:04,531 --> 00:11:06,633
所需的数据

169
00:11:06,667 --> 00:11:09,937
那么在等待 MTLSharedEvent 之前
就可以提前

170
00:11:09,970 --> 00:11:12,973
对一批或多批进行编码

171
00:11:13,006 --> 00:11:15,843
通过这样做 GPU 就不会无事可做

172
00:11:15,876 --> 00:11:18,312
并且总是有工作要处理

173
00:11:19,713 --> 00:11:23,116
如果工作不能在同一个队列上
提前编码

174
00:11:23,150 --> 00:11:26,787
那么可以考虑使用第二个队列
来重叠工作

175
00:11:26,820 --> 00:11:30,424
使用多个队列会允许您
提交独立的工作

176
00:11:30,457 --> 00:11:33,026
并且不会在等待事件时

177
00:11:33,060 --> 00:11:35,462
使其他提交线程暂停

178
00:11:35,495 --> 00:11:40,067
这样 GPU 就有机会
继续接收和处理工作

179
00:11:41,602 --> 00:11:46,807
在某些情况下 算法可以
直接从 GPU 编码工作

180
00:11:47,841 --> 00:11:49,576
使用间接命令缓冲区

181
00:11:49,610 --> 00:11:53,380
可以将下一批编码
直接转移到 GPU 上

182
00:11:53,413 --> 00:11:56,416
避免了同步的需求

183
00:11:56,450 --> 00:12:00,153
要了解更多
关于间接命令缓冲区的细节

184
00:12:00,187 --> 00:12:02,456
请查看 “Modern Rendering with Metal”

185
00:12:02,489 --> 00:12:06,527
该工作负载现在消除或尽可能减少了

186
00:12:06,560 --> 00:12:09,763
CPU 和 GPU 之间高代价的同步

187
00:12:09,796 --> 00:12:15,035
但即使 GPU 时间线很繁忙
扩展挑战也仍然存在

188
00:12:15,068 --> 00:12:17,137
让我们来看看

189
00:12:17,171 --> 00:12:19,973
这个图来自图像处理工作负载

190
00:12:20,007 --> 00:12:23,744
在这里 每次处理 1 帧图像

191
00:12:23,777 --> 00:12:28,982
大量的连续计算串行调度
也会限制扩展性

192
00:12:29,016 --> 00:12:31,718
GPU 很忙 但是内核同步

193
00:12:31,752 --> 00:12:36,223
是有成本的
此外 每次分派都会有一个小的增量

194
00:12:36,256 --> 00:12:38,392
用于在还未饱和的内核上

195
00:12:38,425 --> 00:12:41,361
继续分配线程组

196
00:12:41,395 --> 00:12:45,032
同样地 当线程组完成并停用时

197
00:12:45,065 --> 00:12:49,303
可能会没有足够的工作
使核心完全饱和

198
00:12:49,336 --> 00:12:54,174
在这种情况下 建议
尽可能使独立的工作互相重叠

199
00:12:54,208 --> 00:12:56,810
让我们来看一个直观的例子

200
00:12:56,844 --> 00:13:00,747
我们有一个工作负载
在一个接一个地处理两个图像

201
00:13:00,781 --> 00:13:04,017
正常情况下 内核之间需要进行同步

202
00:13:04,051 --> 00:13:07,688
然而 这并不是安排工作的唯一方法

203
00:13:07,721 --> 00:13:13,327
您可以使用并发分派
使两个图像的独立工作互相交错

204
00:13:13,360 --> 00:13:16,730
在这里 由于并发调度

205
00:13:16,763 --> 00:13:19,299
驱动程序能够交错不同的工作

206
00:13:19,333 --> 00:13:22,402
我们可以看到
以前连续的两个内核

207
00:13:22,436 --> 00:13:27,140
现在被独立的工作分开了

208
00:13:27,174 --> 00:13:30,644
但是 当您使用
MTLDispatchTypeConcurrent 时

209
00:13:30,677 --> 00:13:33,780
必须手动设置障碍

210
00:13:33,814 --> 00:13:38,118
并发分派使驱动程序
能够更紧密地打包工作

211
00:13:38,151 --> 00:13:42,222
隐藏依赖内核之间的
大部分同步成本

212
00:13:42,256 --> 00:13:47,427
并衔接了不同内核的起始增量和结束

213
00:13:47,461 --> 00:13:50,764
这种优化大大提高了
从 M1 Max

214
00:13:50,797 --> 00:13:55,402
到 M1 Ultra 的
工作负载性能和扩展性

215
00:13:55,435 --> 00:13:59,506
与之前的扩展性相比 两个图像交错时
工作负载运行速度提高了 30%

216
00:13:59,540 --> 00:14:04,811
3 个图像并行时
运行速度提高了 70%

217
00:14:07,014 --> 00:14:11,652
仔细考虑内核正在执行的
原子操作很重要

218
00:14:11,685 --> 00:14:15,422
让我们确保它以最有效的方式使用

219
00:14:15,455 --> 00:14:19,126
原子操作允许以安全的方式

220
00:14:19,159 --> 00:14:22,229
从多个线程读取和写入数据

221
00:14:22,262 --> 00:14:26,333
全局原子在整个 GPU 中是一致的

222
00:14:26,366 --> 00:14:29,937
当许多线程试图读写相同的全局值时

223
00:14:29,970 --> 00:14:32,472
这会导致争用

224
00:14:32,506 --> 00:14:38,445
增加 GPU 核心的数量不能改变它
实际上还会导致更多的竞争

225
00:14:38,478 --> 00:14:41,682
让我们通过一个例子
来研究如何改进算法中的

226
00:14:41,715 --> 00:14:44,051
原子行为

227
00:14:45,719 --> 00:14:49,356
这是一个归约算法
会将缓冲区中的

228
00:14:49,389 --> 00:14:51,792
所有值都加起来

229
00:14:51,825 --> 00:14:54,928
最简单的方法是在主内存中
为每个线程

230
00:14:54,962 --> 00:14:57,464
执行一个原子加操作

231
00:14:57,497 --> 00:15:02,603
但是 这并不理想 因为这会
给主内存中的单个值带来很大的压力

232
00:15:02,636 --> 00:15:08,008
直接导致每个内存写入操作的序列化

233
00:15:09,376 --> 00:15:11,945
硬件提供了两个方式来帮助处理

234
00:15:11,979 --> 00:15:14,381
原子内存竞争

235
00:15:14,414 --> 00:15:16,950
SIMD 组指令和线程组原子

236
00:15:18,585 --> 00:15:24,291
prefix_exclusive sum 和 simd_min 等
SIMD 指令

237
00:15:24,324 --> 00:15:28,161
允许在 SIMD 组之间通过寄存器
进行操作和交换内存

238
00:15:28,195 --> 00:15:31,932
而不需要往返于内存

239
00:15:31,965 --> 00:15:35,769
线程组原子由线程组内存完成

240
00:15:35,802 --> 00:15:38,839
每个 GPU 核心都有自己的
线程组内存

241
00:15:38,872 --> 00:15:42,476
使得可以按照 GPU 核的数量进行扩展

242
00:15:42,509 --> 00:15:46,847
让我们看看这两个特性
能如何帮助您改善工作负载

243
00:15:48,549 --> 00:15:50,617
这里我们有同样的归约问题

244
00:15:50,651 --> 00:15:54,321
但是这次它开始使用一个
SIMD 组指令

245
00:15:54,354 --> 00:15:56,390
一个包含内存求和

246
00:15:56,423 --> 00:16:01,094
这样的操作将把 SIMD 组中
所有数字的总和

247
00:16:01,128 --> 00:16:03,664
留在最后一个线程中

248
00:16:03,697 --> 00:16:08,001
然后 每个 simd 组中的
最后一个线程可以在线程组内存中

249
00:16:08,035 --> 00:16:12,739
执行单个原子加操作
将所有 simd 组归约到

250
00:16:12,773 --> 00:16:14,908
线程组内存中的单个值

251
00:16:14,942 --> 00:16:19,246
通过这种方式
使用 SIMD 组指令和线程组内存

252
00:16:19,279 --> 00:16:24,484
在完全不使用主内存的情况下
完成了整个线程组的归约

253
00:16:24,518 --> 00:16:28,488
每一组将能够独立并行归约

254
00:16:29,823 --> 00:16:32,926
现在每个线程组已经归约到一个值

255
00:16:32,960 --> 00:16:35,229
每个线程组可以在主内存中

256
00:16:35,262 --> 00:16:37,898
执行一个原子

257
00:16:37,931 --> 00:16:39,666
这不仅使每个线程组

258
00:16:39,700 --> 00:16:41,201
只需要一个原子

259
00:16:41,235 --> 00:16:43,136
而且由于线程组

260
00:16:43,170 --> 00:16:44,771
在不同的时间完成

261
00:16:44,805 --> 00:16:47,107
它会随着时间的推移分散原子

262
00:16:47,140 --> 00:16:50,677
从而进一步减少内存争用

263
00:16:50,711 --> 00:16:54,114
总结一下
要最大限度地提高原子的效率

264
00:16:54,147 --> 00:16:59,119
请尝试利用内存局部性
尝试使用 SIMD 组操作

265
00:16:59,152 --> 00:17:02,756
以及利用线程组内存原子

266
00:17:02,789 --> 00:17:07,628
所有这些都将极大程度
帮助减少妨碍扩展性的原子操作压力

267
00:17:08,829 --> 00:17:15,035
现在 GPU 间隙已经修复
是时候看看扩展性是否更接近理想了

268
00:17:15,068 --> 00:17:19,706
Xcode 和 Metal System Trace 中的
GPU 限制器

269
00:17:19,740 --> 00:17:25,179
有助于优化 GPU 核心
执行管道中的瓶颈和低效

270
00:17:25,212 --> 00:17:28,382
例如 低效的内存访问模式

271
00:17:28,415 --> 00:17:33,020
总是导致末级缓存
或 Memory Management Unit

272
00:17:33,053 --> 00:17:36,723
也就是 MMU 限制很高
而利用率则很低

273
00:17:36,757 --> 00:17:43,497
首先要解决的是调整线程组
和内存布局方法

274
00:17:43,530 --> 00:17:46,200
减少内存跨度和发散的关键

275
00:17:46,233 --> 00:17:50,537
是要清楚地理解
工作负载内存访问模式

276
00:17:50,571 --> 00:17:54,007
包括空间和时间上的模式

277
00:17:54,041 --> 00:17:58,312
一旦理解了这一点
就有两种可能的调整方向

278
00:17:58,345 --> 00:18:01,915
重新组织数据布局
以提高数据访问局部性

279
00:18:01,949 --> 00:18:05,953
或者调整访问模式
以更好地匹配数据布局

280
00:18:05,986 --> 00:18:09,056
并提高内存和缓存局部性

281
00:18:09,089 --> 00:18:10,490
让我们来看一个例子

282
00:18:12,659 --> 00:18:16,663
这有一个内存缓冲区
数据是横向排列的

283
00:18:16,697 --> 00:18:18,732
一行接着一行

284
00:18:18,765 --> 00:18:21,802
然而 当调度计算内核时

285
00:18:21,835 --> 00:18:24,404
通常会有一个类似 2D 的模式

286
00:18:24,438 --> 00:18:27,374
其中分布着方形的线程组

287
00:18:27,407 --> 00:18:29,843
这在很大程度上是空间局部化的

288
00:18:29,877 --> 00:18:34,414
这种访问模式和数据布局
对于数据局部性来说不是很合适

289
00:18:36,216 --> 00:18:39,653
例如 当第一个 SIMD 组访问数据时

290
00:18:39,686 --> 00:18:42,322
请求被打包在缓存线中

291
00:18:42,356 --> 00:18:44,992
大部分缓存线不会被使用

292
00:18:45,025 --> 00:18:50,063
但是仍然会占用缓存空间

293
00:18:50,097 --> 00:18:53,100
要重新排列数据
以更好地适应访问模式

294
00:18:53,133 --> 00:18:56,837
例如 它不跨越整行

295
00:18:56,870 --> 00:18:59,940
而是被局部化为条带

296
00:19:01,008 --> 00:19:04,144
使用这种新的内存布局
线程组将能够利用

297
00:19:04,178 --> 00:19:07,080
在缓存线中请求的大部分数据

298
00:19:07,114 --> 00:19:11,585
从而减少分化并提高缓存效率

299
00:19:12,586 --> 00:19:16,089
另一种选择是改变
3D 网格的分配方式

300
00:19:16,123 --> 00:19:19,126
以更好地适应当前的数据布局

301
00:19:19,159 --> 00:19:23,664
尝试调整线程组的大小
来创建能更好地映射到内存布局的组

302
00:19:23,697 --> 00:19:27,768
例如 一个更偏向矩形的

303
00:19:27,801 --> 00:19:29,670
形状

304
00:19:29,703 --> 00:19:33,440
在这种情况下
访问模式与内存布局保持一致

305
00:19:33,473 --> 00:19:36,977
从而提供了更高的缓存效率

306
00:19:37,010 --> 00:19:41,415
您可能需要尝试找到
最适合您的工作负载的方法

307
00:19:41,448 --> 00:19:44,418
有时 您可能需要做出权衡

308
00:19:44,451 --> 00:19:49,756
牺牲线程发散性来换取内存位置
或者反过来

309
00:19:49,790 --> 00:19:54,661
更改数据布局 网格分派
或它们的组合

310
00:19:54,695 --> 00:19:57,631
每个工作负载和访问模式都是不相同

311
00:20:00,100 --> 00:20:03,270
现在您已经了解了
改善内存位置的方法

312
00:20:03,303 --> 00:20:06,273
让我们看看 Blender Cycles 中
更具体的例子

313
00:20:08,408 --> 00:20:13,680
Cycles 是 Blender 用于
产品渲染的基于物理的路径追踪器

314
00:20:13,714 --> 00:20:17,885
它旨在为生产需要
提供开箱即用的具有艺术控制

315
00:20:17,918 --> 00:20:22,422
和灵活的着色节点的基于物理的结果

316
00:20:24,258 --> 00:20:30,230
这个 Instrument trace 清楚地显示了
低读带宽 高最高 GPU 限制器

317
00:20:30,264 --> 00:20:34,368
高缓存限制器
和低末级缓存利用率

318
00:20:36,170 --> 00:20:41,975
把握带宽和 MMU 限制器
对于扩展性是很重要的

319
00:20:42,009 --> 00:20:45,679
如果您的最高限制器
是末级缓存或 MMU

320
00:20:45,712 --> 00:20:50,551
您就需要减少您的内存跨度
并最大化数据局部性

321
00:20:50,584 --> 00:20:52,152
让我们看一个例子

322
00:20:53,887 --> 00:20:57,958
Cycle 使用数据排序来减少分化

323
00:20:57,991 --> 00:21:01,762
它通过按材质类型
对光线碰撞进行分类来实现这点

324
00:21:01,795 --> 00:21:04,131
这利于减少线程分化

325
00:21:04,164 --> 00:21:07,401
但是它增加了空间内存分化

326
00:21:07,434 --> 00:21:11,004
导致了高 MMU 限制器

327
00:21:11,038 --> 00:21:14,641
为了解决这一点 我们尝试了
在排序之前对内存范围进行分区

328
00:21:14,675 --> 00:21:17,711
以增加数据的局部性

329
00:21:17,744 --> 00:21:18,579
让我们设想一下

330
00:21:18,612 --> 00:21:24,051
当光线被射入场景以模拟光的传播时

331
00:21:24,084 --> 00:21:28,055
它们会击中物体
而数据则被收集到缓冲区中

332
00:21:28,088 --> 00:21:30,958
在交点上 我们能知道很多东西

333
00:21:30,991 --> 00:21:34,728
被击中的材料类型
比如玻璃 金属等等

334
00:21:34,761 --> 00:21:39,099
交点的位置 光线等等

335
00:21:39,132 --> 00:21:43,303
为了简单起见 我们只关注材料类型

336
00:21:43,337 --> 00:21:45,906
这是内存缓冲区中的材料

337
00:21:47,341 --> 00:21:49,910
由于每次射线命中都会收集大量数据

338
00:21:49,943 --> 00:21:53,380
内存缓冲区可能会变得相当大

339
00:21:53,413 --> 00:21:55,616
为了避免移动大量内存

340
00:21:55,649 --> 00:22:00,521
要填充索引列表并对其进行排序

341
00:22:00,554 --> 00:22:03,957
在排序之后 相同材料类型的索引

342
00:22:03,991 --> 00:22:06,560
现在被包装在了一起

343
00:22:06,593 --> 00:22:11,932
SIMD 组可以开始
加载索引并处理材料了

344
00:22:11,965 --> 00:22:15,269
SIMD 组将使用索引

345
00:22:15,302 --> 00:22:16,837
加载原始缓冲区中的相应数据

346
00:22:18,472 --> 00:22:22,576
但是 simd 组
将在整个内存范围内读取数据

347
00:22:22,609 --> 00:22:25,279
这会给 MMU 带来了压力

348
00:22:25,312 --> 00:22:27,981
让我们来研究一下新方法

349
00:22:28,015 --> 00:22:31,985
内存范围被划分在一个理想的分区中

350
00:22:32,019 --> 00:22:36,390
这个分区不允许混合
来自不同分区的索引

351
00:22:36,423 --> 00:22:41,962
在排序时 很明显
访问的数据范围被限制在分区中

352
00:22:41,995 --> 00:22:46,733
而不是像以前那样
跨越整个内存范围

353
00:22:46,767 --> 00:22:52,739
这是线程分化
和内存分化之间的权衡和平衡

354
00:22:52,773 --> 00:22:55,609
理想的分区数量和大小

355
00:22:55,642 --> 00:22:58,078
高度依赖于工作负载

356
00:22:58,111 --> 00:23:02,216
您可能需要进行实验
看看哪种方法效果最好

357
00:23:02,249 --> 00:23:07,421
让我们以另一个 metal system trace 为例
看看工作负载是否有所改善

358
00:23:07,454 --> 00:23:11,124
在这里 我们看到了优化版本的
限制条件和使用情况

359
00:23:11,158 --> 00:23:17,364
最高性能限制器下降了
末级缓存限制器也下降了

360
00:23:17,397 --> 00:23:22,436
因此 带宽和着色器运行时间
有了显著改善

361
00:23:22,469 --> 00:23:24,137
让我们看看改进了多少

362
00:23:24,171 --> 00:23:28,976
最高限制器和 LLC 限制器
减少了约 20%

363
00:23:29,009 --> 00:23:32,145
这意味着数据流更加高效

364
00:23:32,179 --> 00:23:35,215
GPU 读取带宽显著增加

365
00:23:35,249 --> 00:23:38,652
允许更多数据推送到 GPU 核心

366
00:23:39,987 --> 00:23:43,557
总的来说 在这个实验中
增加内存局部性

367
00:23:43,590 --> 00:23:48,896
可以提高 10% 到 30% 的性能
具体是多少取决于实际情况

368
00:23:48,929 --> 00:23:54,034
这只是许多
改进内存访问模式方法中的一个例子

369
00:23:54,067 --> 00:23:58,472
请不断试验
优化最高性能限制器

370
00:23:58,505 --> 00:24:02,476
GPU 工具有更多
有用的计数器帮助优化

371
00:24:03,677 --> 00:24:08,916
Xcode 在编译器统计窗口中
提供了一个新的理论占用率

372
00:24:08,949 --> 00:24:15,556
Xcode 和 Instruments 现在都有
几个 MMU 相关限制器和计数器

373
00:24:15,589 --> 00:24:20,060
特别是新 MMU 限制器
MMU 占用率计数

374
00:24:20,093 --> 00:24:23,130
和 MMU TLB 缺失率计数

375
00:24:24,431 --> 00:24:27,301
我今天讲了很多内容

376
00:24:27,334 --> 00:24:31,972
我讨论了 GPU 的可扩展性
以及放大时瓶颈如何转移

377
00:24:32,005 --> 00:24:36,009
还有这些工具能如何帮助您
发现并解决与可缩放性相关的问题

378
00:24:36,043 --> 00:24:40,147
我还讨论了您可能需要
如何进行试验并做出权衡

379
00:24:40,180 --> 00:24:43,417
以为您的应用获得最佳结果

380
00:24:43,450 --> 00:24:47,855
我期待看到所有开发者的
优秀的 App 能够

381
00:24:47,888 --> 00:24:48,956
在 Apple 芯片上完美扩展

382
00:24:48,989 --> 00:24:50,624
感谢收看

