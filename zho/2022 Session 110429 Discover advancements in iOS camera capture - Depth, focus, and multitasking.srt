1
00:00:00,334 --> 00:00:06,340
[欢快的音乐]

2
00:00:09,810 --> 00:00:11,478
Nikolas Gelo：您好
欢迎来到

3
00:00:11,512 --> 00:00:14,147
“发现 iOS 相机捕获功能的进展”

4
00:00:14,181 --> 00:00:16,383
我是相机软件团队的 Nikolas Gelo

5
00:00:16,416 --> 00:00:20,754
我将展示 iOS 和 iPadOS
新相机中一些令人兴奋的功能

6
00:00:20,787 --> 00:00:25,025
首先 我将讲解如何利用使用 AVFoundation 的
LiDAR 扫描仪将深度信息以流媒体传输

7
00:00:25,058 --> 00:00:28,495
然后展示如何在您的 App 上
通过面部自动对焦和自动曝光

8
00:00:28,529 --> 00:00:31,565
获得更好的面部渲染效果

9
00:00:31,598 --> 00:00:36,303
再带您了解
高级 AVCaptureSession 流配置

10
00:00:36,336 --> 00:00:38,772
最后向您展示您的 App 将如何

11
00:00:38,805 --> 00:00:41,675
能够在处理多任务时使用相机

12
00:00:41,708 --> 00:00:45,646
我将讲解如何利用使用 AVFoundation 的
LiDAR 扫描仪达成流式传输深度

13
00:00:45,679 --> 00:00:50,150
iPhone 12 Pro、iPhone 13 Pro
和 iPad Pro 均配备

14
00:00:50,184 --> 00:00:53,887
能够输出密集深度图的
LiDAR 扫描仪

15
00:00:53,921 --> 00:00:57,157
LiDAR 扫描仪向周围环境
发射光

16
00:00:57,191 --> 00:01:00,594
然后收集场景中表面反射的光

17
00:01:00,627 --> 00:01:03,397
通过测量光线从 LiDAR 发射至
环境

18
00:01:03,430 --> 00:01:07,401
再反射回扫描仪的时间来估计深度

19
00:01:07,434 --> 00:01:11,138
这一过程每秒可运行数百万次

20
00:01:11,171 --> 00:01:14,308
我将向您展示在操作中
使用 AVFoundation 的 LiDAR 扫描仪

21
00:01:14,341 --> 00:01:17,945
我在这台
iPhone 13 Pro Max 上运行了一个

22
00:01:17,978 --> 00:01:20,914
使用了新 LiDAR 深度相机
AVCaptureDevice 的 App

23
00:01:20,948 --> 00:01:24,551
该 App 在直播摄像头视频流上
呈现出流式深度数据

24
00:01:24,585 --> 00:01:29,189
蓝色表示近处的对象
红色表示较远的对象

25
00:01:29,223 --> 00:01:33,360
拖动滑动条
我可以调整深度的不透明度

26
00:01:33,393 --> 00:01:36,430
这个 App 还可以拍摄
带有高分辨率深度图的照片

27
00:01:36,463 --> 00:01:39,566
当我拍照时 会应用相同的深度叠加

28
00:01:39,600 --> 00:01:42,903
但对于静止物体 分辨率甚至会更高

29
00:01:42,936 --> 00:01:45,305
这个 App 还有一个很好用的功能

30
00:01:45,339 --> 00:01:49,076
当我按下手电筒按钮时 App 会使用
带有彩色图像的高分辨率深度图

31
00:01:49,109 --> 00:01:52,880
用 RealityKit
在场景上渲染出一个聚光灯

32
00:01:52,913 --> 00:01:57,150
我可以四处点击
将聚光灯指向场景中的不同对象

33
00:01:57,184 --> 00:01:59,720
看这里用聚光灯效果突出了吉他

34
00:01:59,753 --> 00:02:02,222
或者如果我在墙角点击了正确的位置

35
00:02:02,256 --> 00:02:04,691
聚光灯会打出心形

36
00:02:04,725 --> 00:02:08,028
让我们把聚光灯重新
移回到那把吉他上 看起来很酷

37
00:02:09,363 --> 00:02:14,668
LiDAR 扫描仪的 API
在 iPadOS 13.4 的 ARKit 中首次引入

38
00:02:14,701 --> 00:02:19,039
如果您还没有看过
WWDC 2020 的演讲“探索 ARKit 4”

39
00:02:19,072 --> 00:02:21,508
我建议您去观看

40
00:02:21,542 --> 00:02:26,547
iOS 15.4 有一项新增功能
即您的 App 使用 AVFoundation 即可访问 LiDAR 扫描仪

41
00:02:26,580 --> 00:02:28,982
我们引入了一种新的
AVCapture 设备类型

42
00:02:29,016 --> 00:02:33,153
内置 LiDAR 深度相机
可传送视频和深度

43
00:02:33,187 --> 00:02:36,924
产生高质量、高精度的深度信息

44
00:02:36,957 --> 00:02:40,194
这个新 AVCaptureDevice
使用后置广角摄像头

45
00:02:40,227 --> 00:02:43,764
再使用 LiDAR 扫描
仪传送视频以捕获深度

46
00:02:43,797 --> 00:02:47,935
广角摄像头
可在视野中捕获视频和深度

47
00:02:47,968 --> 00:02:50,304
就像 TrueDepth AVCaptureDevice

48
00:02:50,337 --> 00:02:53,941
它的所有格式都支持深度数据传送

49
00:02:53,974 --> 00:02:57,110
这个新 AVCaptureDevice 通过融合

50
00:02:57,144 --> 00:02:59,546
带有后置广角摄像头的彩色图像
LiDAR 扫描仪的稀疏输出

51
00:02:59,580 --> 00:03:02,449
能产生高质量的深度数据

52
00:03:02,482 --> 00:03:06,086
LiDAR 和颜色输入可通过
能够输出密集深度图的

53
00:03:06,119 --> 00:03:08,455
机器学习模型来处理

54
00:03:08,488 --> 00:03:11,692
由于 LiDAR 深度相机
采用的是后置广角摄像头

55
00:03:11,725 --> 00:03:14,761
长焦和超广角相机也均可使用

56
00:03:14,795 --> 00:03:17,030
使用 AVCaptureMultiCamSession

57
00:03:17,064 --> 00:03:20,934
这项功能对希望同时使用
多个摄像头的 App 很有帮助

58
00:03:20,968 --> 00:03:24,304
LiDAR 深度相机
可提供多种格式

59
00:03:24,338 --> 00:03:27,107
从 640 x 480 的视频分辨率

60
00:03:27,140 --> 00:03:32,179
到 4032 x 3024 的
完整 12 兆像素图像

61
00:03:32,212 --> 00:03:36,917
流传输时 它可以输出高达
320 x 240 的深度图

62
00:03:36,950 --> 00:03:42,990
拍摄照片可以捕获
768 x 576 的深度图

63
00:03:43,023 --> 00:03:47,961
注意 16 x 9 和 4 x 3 格式
的深度分辨率略有不同

64
00:03:47,995 --> 00:03:50,998
这是为了匹配视频的纵横比

65
00:03:51,031 --> 00:03:53,867
在 iPhone 12 Pro
iPhone 13 Pro 和 iPad Pro 第 5 代上

66
00:03:53,901 --> 00:03:58,672
可以使用 LiDAR 深度相机
AVCaptureDevice

67
00:03:58,705 --> 00:04:03,477
iPhone 13 Pro 可以
使用后置摄像头的组合传送深度数据

68
00:04:03,510 --> 00:04:07,147
AVFoundation Capture API
将这些称为“虚拟设备”

69
00:04:07,181 --> 00:04:09,449
它们由物理设备组成

70
00:04:09,483 --> 00:04:11,451
在 iPhone 13 Pro 的背面

71
00:04:11,485 --> 00:04:15,255
有四个虚拟 AVCaptureDevices
可供使用：

72
00:04:15,289 --> 00:04:18,192
新 LiDAR 深度相机
使用广角摄像头的

73
00:04:18,225 --> 00:04:20,427
LiDAR 扫描仪

74
00:04:20,460 --> 00:04:24,231
双摄像头使用了广角和长焦摄像头

75
00:04:24,264 --> 00:04:25,766
双广角摄像头

76
00:04:25,799 --> 00:04:28,669
使用了广角和超广角摄像头

77
00:04:28,702 --> 00:04:30,003
以及三摄像头

78
00:04:30,037 --> 00:04:33,707
使用了广角、超广角和长焦摄像头

79
00:04:33,740 --> 00:04:37,611
这些设备生成的深度类型存在差异

80
00:04:37,644 --> 00:04:41,081
LiDAR 深度相机能生成
“绝对深度”

81
00:04:41,114 --> 00:04:45,586
飞行时间技术
能够计算真实世界的规模

82
00:04:45,619 --> 00:04:49,690
比方说 它很适用于
测量等计算机视觉任务

83
00:04:49,723 --> 00:04:52,125
TrueDepth 摄像头
双摄像头、双宽摄像头

84
00:04:52,159 --> 00:04:56,296
和三摄像头能生成
相对的、基于视差的深度

85
00:04:56,330 --> 00:05:00,734
这会消耗更少的电量
非常适合需要呈现照片效果的 App

86
00:05:00,767 --> 00:05:04,571
AVFoundation 使用
AVDepthData 类来表示深度

87
00:05:04,605 --> 00:05:07,174
此类的像素缓冲区包含深度

88
00:05:07,207 --> 00:05:08,976
还有可以描述它的其他属性

89
00:05:09,009 --> 00:05:13,213
其中包括深度数据类型
准确率、是否过滤

90
00:05:13,247 --> 00:05:16,149
它像新 LiDAR 深度相机一样

91
00:05:16,183 --> 00:05:18,118
由具有深度功能的
AVCaptureDevice 提供

92
00:05:18,151 --> 00:05:20,888
您可以从 AVCaptureDepthDataOutput
达成流式传输深度

93
00:05:20,921 --> 00:05:25,425
或从 AVCapturePhotoOutput
接收附加到照片的深度

94
00:05:25,459 --> 00:05:27,728
默认情况下会过滤深度数据

95
00:05:27,761 --> 00:05:29,329
过滤能降低噪音

96
00:05:29,363 --> 00:05:32,566
并填充深度图中的缺失值或孔

97
00:05:32,599 --> 00:05:34,768
非常适合用于视频和摄影 App

98
00:05:34,801 --> 00:05:37,070
所以 使用深度图在彩色图像上

99
00:05:37,104 --> 00:05:39,506
应用效果时 不会出现伪影

100
00:05:39,540 --> 00:05:43,110
但是 计算机视觉 App
应该更容易处理未经过滤的深度数据

101
00:05:43,143 --> 00:05:45,979
保留深度图中的原始值

102
00:05:46,013 --> 00:05:48,482
禁用过滤时 LiDAR 深度相机

103
00:05:48,515 --> 00:05:51,118
会排除低置信点

104
00:05:51,151 --> 00:05:53,187
要禁用深度数据过滤

105
00:05:53,220 --> 00:05:57,991
可将 AVCaptureDepthDataOutput 上的
isFilteringEnabled 属性设置为 false

106
00:05:58,025 --> 00:06:01,495
当您从委托回调中收到
AVDepthData 对象时

107
00:06:01,528 --> 00:06:03,463
它就不会被过滤

108
00:06:03,497 --> 00:06:06,433
由于 ARKit 已经提供了
对 LiDAR 扫描仪的访问权限

109
00:06:06,466 --> 00:06:09,503
您可能会问
“AVFoundation 相比如何？”

110
00:06:10,804 --> 00:06:14,141
AVFoundation
专为视频和摄影 App 而设计

111
00:06:14,174 --> 00:06:16,577
您可以使用 AVFoundation
把 LiDAR 扫描仪捕获的

112
00:06:16,610 --> 00:06:20,047
深度数据嵌入高分辨率照片

113
00:06:20,080 --> 00:06:23,183
ARKit 顾名思义 最适合

114
00:06:23,217 --> 00:06:24,852
增强现实 App

115
00:06:24,885 --> 00:06:28,322
有了 LiDAR 扫描仪 就可以

116
00:06:28,355 --> 00:06:31,058
使用 ARKit
场景几何和物体放置等功能

117
00:06:31,091 --> 00:06:33,994
AVFoundation 可提供
高分辨率视频

118
00:06:34,027 --> 00:06:36,763
非常适合录制电影和拍照

119
00:06:36,797 --> 00:06:42,236
AVFoundation 的 LiDAR 深度相机
可以输出高达 768 x 576 的深度

120
00:06:42,269 --> 00:06:47,774
高于 ARKit 深度分辨率
256 x 192 的两倍

121
00:06:47,808 --> 00:06:50,511
ARKit 使用分辨率较低的
深度图

122
00:06:50,544 --> 00:06:54,982
因此它可以针对其功能
应用增强现实算法

123
00:06:55,015 --> 00:06:59,686
想要了解更多有关如何使用
AVFoundation 捕获深度数据的“深入”信息

124
00:06:59,720 --> 00:07:03,323
观看我们 WWDC 2017 的课程

125
00:07:03,357 --> 00:07:05,859
“捕获 iPhone 摄影的深度”

126
00:07:05,893 --> 00:07:07,661
我们很高兴看到
您能在您的 App 中

127
00:07:07,694 --> 00:07:10,430
对 LiDAR 深度摄像头的趣用

128
00:07:10,464 --> 00:07:15,068
接下来 我将讨论
自动对焦和自动曝光系统

129
00:07:15,102 --> 00:07:18,372
如何帮助提高
App 场景中人脸的可见度

130
00:07:18,405 --> 00:07:21,608
自动对焦和自动曝光系统会分析场景

131
00:07:21,642 --> 00:07:23,110
捕获最佳图像

132
00:07:23,143 --> 00:07:27,247
自动对焦系统
会调整镜头 保证焦点在主体上

133
00:07:27,281 --> 00:07:29,883
自动曝光系统则会平衡场景最亮的

134
00:07:29,917 --> 00:07:33,654
和最暗是区域 保证主体可见

135
00:07:33,687 --> 00:07:36,156
但是 有时拍照做出的自动调整

136
00:07:36,190 --> 00:07:38,926
不需要对焦在拍摄对象的脸部

137
00:07:38,959 --> 00:07:41,595
还有的时候 在明亮的背光下

138
00:07:41,628 --> 00:07:44,831
很难看见拍摄对象的脸

139
00:07:44,865 --> 00:07:49,036
数码单反相机和其他专业相机
有一个共同特点：跟踪场景中的人脸

140
00:07:49,069 --> 00:07:52,940
动态调整焦点和曝光 确保人脸可见

141
00:07:52,973 --> 00:07:58,612
iOS 15.4 有一项新增功能
焦点和曝光系统将为面部优先

142
00:07:58,645 --> 00:08:01,849
我们非常喜欢它的效果
以至于我们在 iOS 15.4

143
00:08:01,882 --> 00:08:04,852
或更高版本上的
所有 App 中启用了这项功能

144
00:08:04,885 --> 00:08:07,354
我会给您看一些例子

145
00:08:07,387 --> 00:08:10,557
没有面部自动对焦 相机的焦点

146
00:08:10,591 --> 00:08:13,227
一直在背景上而不会重新在脸上对焦

147
00:08:13,260 --> 00:08:14,428
再看一遍

148
00:08:14,461 --> 00:08:16,964
看看他转身时他的脸是如何失焦的

149
00:08:16,997 --> 00:08:19,499
并且背景中的树木保持锋利

150
00:08:19,533 --> 00:08:23,637
启用面部自动对焦后
您可以清楚地看到他的脸

151
00:08:23,670 --> 00:08:27,708
当他把脸转开
相机将焦点会转移到背景上

152
00:08:28,742 --> 00:08:32,246
把视频放在一起比较 差异会很明显

153
00:08:32,279 --> 00:08:34,648
右侧启用了面部自动对焦

154
00:08:34,681 --> 00:08:37,618
您可以在他的胡须中
看到更清楚的细节

155
00:08:37,651 --> 00:08:42,456
在明亮的逆光场景中
保持脸部曝光良好是一项挑战

156
00:08:42,489 --> 00:08:45,359
但是在自动曝光系统
优先处理面部的情况下

157
00:08:45,392 --> 00:08:47,561
我们能很容易看到他

158
00:08:48,896 --> 00:08:52,299
放在一起比较
可以再次看到这里的区别

159
00:08:52,332 --> 00:08:55,969
请注意 右侧图片中
他的脸部曝光良好

160
00:08:56,003 --> 00:08:57,938
背景中的树木会显得更亮

161
00:08:57,971 --> 00:08:59,339
天空也是

162
00:08:59,373 --> 00:09:03,310
人脸优先时
系统会调整整个场景的曝光

163
00:09:04,545 --> 00:09:08,415
在 iOS 15.4 中
启用面部自动对焦和自动曝光时

164
00:09:08,448 --> 00:09:11,718
AVCaptureDevice
上有可控的新属性

165
00:09:11,752 --> 00:09:14,955
您可以控制设备
是否会“自动调整”这些设置

166
00:09:14,988 --> 00:09:17,224
并决定何时启用

167
00:09:17,257 --> 00:09:19,359
在切换“isEnabled”
属性之前

168
00:09:19,393 --> 00:09:23,030
您必须首先禁用自动调整

169
00:09:23,063 --> 00:09:26,600
摄影 App
自动启用此行为效果会很好

170
00:09:26,633 --> 00:09:28,302
它应用于 Apple 的
相机 App

171
00:09:28,335 --> 00:09:30,304
也非常适合用于视频会议 App

172
00:09:30,337 --> 00:09:32,506
可在通话期间保持面部可见

173
00:09:32,539 --> 00:09:34,541
FaceTime 就利用了这一点

174
00:09:34,575 --> 00:09:36,944
但有时 人脸驱动的
自动对焦和自动曝光系统

175
00:09:36,977 --> 00:09:40,480
并不是 App 的最佳选择

176
00:09:40,514 --> 00:09:43,183
例如 如果您希望
您的 App 为用户提供

177
00:09:43,217 --> 00:09:46,887
手动控制捕获的图像
您可以考虑将其关闭

178
00:09:48,355 --> 00:09:50,891
如果您认为在您的 App 中

179
00:09:50,924 --> 00:09:53,493
面部自动对焦或自动曝光有误
您可以选择关闭此行为

180
00:09:53,527 --> 00:09:56,964
首先锁定 AVCaptureDevice
进行配置

181
00:09:56,997 --> 00:09:59,199
然后 关闭面部自动对焦或自动曝光

182
00:09:59,233 --> 00:10:02,002
中的自动调整

183
00:10:02,035 --> 00:10:05,772
接着 禁用面部自动对焦或自动曝光

184
00:10:05,806 --> 00:10:09,510
最后 解锁设备进行配置

185
00:10:10,511 --> 00:10:13,547
我将接着谈谈如何使用高级流配置

186
00:10:13,580 --> 00:10:18,085
接收为您 App 的需求
量身定制的音频和视频数据

187
00:10:18,118 --> 00:10:20,521
AVFoundation Capture API 允许

188
00:10:20,554 --> 00:10:23,023
开发人员使用相机构建沉浸式 App

189
00:10:23,056 --> 00:10:27,694
AVCaptureSession 管理的是
摄像头和麦克风等输入设备的数据流

190
00:10:27,728 --> 00:10:31,031
连接在可传送视频、音频、照片等

191
00:10:31,064 --> 00:10:33,800
的 AVCaptureOutputs 上

192
00:10:33,834 --> 00:10:36,537
让我们以一个常见的
相机 App 用例为例：

193
00:10:36,570 --> 00:10:40,407
将自定义效果（如过滤器或叠加）
应用于已录制的视频

194
00:10:40,440 --> 00:10:42,409
像这样的 App 将具有以下功能

195
00:10:42,442 --> 00:10:46,847
一个带有两个输入的 AVCaptureSession
需要连接两个输出的

196
00:10:46,880 --> 00:10:51,752
一个摄像头和一个麦克风
分别用于视频数据和音频数据

197
00:10:51,785 --> 00:10:54,121
然后在视频数据上应用效果

198
00:10:54,154 --> 00:10:56,423
并将处理后的视频发送到两处

199
00:10:56,456 --> 00:10:57,824
一处为视频预览

200
00:10:57,858 --> 00:11:00,127
另一处为用于记录的 AVAssetWriter

201
00:11:00,160 --> 00:11:01,562
音频数据也会发送

202
00:11:01,595 --> 00:11:03,697
到 AVAssetWriter

203
00:11:03,730 --> 00:11:07,601
iOS 16 和 iPadOS 16 有一项新增功能
App 可以同时使用

204
00:11:07,634 --> 00:11:10,871
多个 AVCaptureVideoDataOutputs

205
00:11:10,904 --> 00:11:14,775
您可以自定义
每个视频数据输出的分辨率

206
00:11:14,808 --> 00:11:19,213
稳定性、方向和像素格式

207
00:11:19,246 --> 00:11:21,315
让我们回到示例相机 App

208
00:11:21,348 --> 00:11:25,385
这个 App 正在
平衡冲突捕获要求

209
00:11:25,419 --> 00:11:28,956
它想要显示捕获内容的实时视频预览

210
00:11:28,989 --> 00:11:31,959
并录制高质量视频 供以后播放

211
00:11:31,992 --> 00:11:36,063
预览的分辨率需要刚好适合设备屏幕

212
00:11:36,096 --> 00:11:39,466
并且处理速度需要足够快
才能实现低延迟预览

213
00:11:39,499 --> 00:11:42,603
但是在录制时 最好以高分辨率捕获

214
00:11:42,636 --> 00:11:44,872
同时应用质量效果

215
00:11:44,905 --> 00:11:46,473
在能添加第二个

216
00:11:46,507 --> 00:11:48,075
AVCaptureVideoDataOutput 的情况下

217
00:11:48,108 --> 00:11:50,711
可以扩展捕获图

218
00:11:50,744 --> 00:11:52,679
现在 可以优化

219
00:11:52,713 --> 00:11:53,981
视频数据输出了

220
00:11:54,014 --> 00:11:55,849
一个输出可以为预览提供

221
00:11:55,883 --> 00:11:57,184
较小的缓冲区

222
00:11:57,217 --> 00:11:58,619
另一个可以为录制提供

223
00:11:58,652 --> 00:12:01,522
全尺寸 4K 缓冲区

224
00:12:01,555 --> 00:12:04,024
此外 该 App 可以在

225
00:12:04,057 --> 00:12:05,325
较小的预览缓冲区上

226
00:12:05,359 --> 00:12:07,160
呈现更简单的、性能更好的效果

227
00:12:07,194 --> 00:12:08,929
并在录制时留下

228
00:12:08,962 --> 00:12:11,565
全尺寸缓冲区的优质效果

229
00:12:11,598 --> 00:12:13,267
现在该 App 不用再

230
00:12:13,300 --> 00:12:14,501
削弱其预览

231
00:12:14,535 --> 00:12:16,503
或录制的视频了

232
00:12:17,671 --> 00:12:20,674
使用单独的视频数据输出

233
00:12:20,707 --> 00:12:24,311
进行预览和录制的另一个原因
是为了应用不同的稳定模式

234
00:12:24,344 --> 00:12:26,947
视频稳定会对视频捕获管道

235
00:12:26,980 --> 00:12:28,582
造成额外的延迟

236
00:12:28,615 --> 00:12:31,018
预览不需要延迟

237
00:12:31,051 --> 00:12:34,188
因为延迟太明显会造成内容很难捕获

238
00:12:34,221 --> 00:12:36,557
录制上可以应用稳定功能

239
00:12:36,590 --> 00:12:38,959
以便稍后观看视频时获得更好的体验

240
00:12:38,992 --> 00:12:42,996
因此您可以
在低延迟预览视频数据输出上

241
00:12:43,030 --> 00:12:45,098
不应用任何稳定功能

242
00:12:45,132 --> 00:12:48,702
并将稳定功能
应用到另一个上 供以后播放

243
00:12:48,735 --> 00:12:52,739
配置视频数据输出
分辨率的方法有很多

244
00:12:52,773 --> 00:12:56,677
配置全尺寸输出 首先要

245
00:12:56,710 --> 00:12:58,412
禁用输出缓冲区尺寸的自动配置

246
00:12:58,445 --> 00:13:02,282
然后禁用预览大小中
输出缓冲区的传递

247
00:13:02,316 --> 00:13:04,918
然而 在大多情况下 视频数据输出

248
00:13:04,952 --> 00:13:08,455
已配置为全尺寸输出

249
00:13:08,488 --> 00:13:12,893
配置预览大小的输出
需要再次禁用自动配置

250
00:13:12,926 --> 00:13:16,830
然后启用预览大小中
输出缓冲区的传递

251
00:13:16,864 --> 00:13:21,535
这在使用照片
AVCaptureSessionPreset 时默认启用

252
00:13:21,568 --> 00:13:25,272
要请求自定义分辨率
请指定宽度和高度

253
00:13:25,305 --> 00:13:27,841
在输出的视频设置字典中

254
00:13:27,875 --> 00:13:31,245
宽高的纵横比必须与源设备的

255
00:13:31,278 --> 00:13:32,846
activeFormat 纵横比匹配

256
00:13:32,880 --> 00:13:35,616
配置您的视频数据输出的方法有很多

257
00:13:35,649 --> 00:13:39,186
若要应用稳定 请像电影扩展一样

258
00:13:39,219 --> 00:13:40,754
将首选的稳定设置为模式

259
00:13:40,787 --> 00:13:43,490
会生成非常适合观看的视频

260
00:13:43,524 --> 00:13:47,561
您可以更改方向以接收纵向的缓冲区

261
00:13:47,594 --> 00:13:52,399
并且您可以指定像素格式 以接收
10 位无损 YUV 缓冲区

262
00:13:53,567 --> 00:13:55,536
更多 AVCaptureVideoDataOutput

263
00:13:55,569 --> 00:14:00,274
选择像素格式的信息
请参阅“技术说明 3121”

264
00:14:01,375 --> 00:14:04,077
从 iOS 16 和 iPadOS 16 开始

265
00:14:04,111 --> 00:14:06,880
除了可以使用多个视频数据输出

266
00:14:06,914 --> 00:14:09,516
App 可以在
从 AVCaptureVideoDataOutput

267
00:14:09,550 --> 00:14:12,586
和 AVCaptureAudioDataOutput 接收数据时

268
00:14:12,619 --> 00:14:14,922
使用 AVCaptureMovieFileOutput 进行录制

269
00:14:14,955 --> 00:14:17,024
要确定可以添加到会话中的内容

270
00:14:17,057 --> 00:14:19,159
您可以检查
是否可以将输出添加到其中

271
00:14:19,193 --> 00:14:21,528
并查询会话的 hardwareCost 属性

272
00:14:21,562 --> 00:14:25,065
以确定系统是否可以支持您的配置

273
00:14:25,098 --> 00:14:28,202
您可以在接收
带有电影文件输出的视频数据时

274
00:14:28,235 --> 00:14:33,073
录制和分析场景时检查视频

275
00:14:33,106 --> 00:14:35,776
接收带有电影文件输出的音频数据时

276
00:14:35,809 --> 00:14:37,778
您可以在录制时对音频进行采样

277
00:14:37,811 --> 00:14:40,547
并聆听正在录制的内容

278
00:14:40,581 --> 00:14:42,316
有了这样的捕获图

279
00:14:42,349 --> 00:14:45,919
您可以将录制机制卸载到
AVCaptureMovieFileOutput

280
00:14:45,953 --> 00:14:50,224
同时仍接收未压缩的视频和音频样本

281
00:14:50,958 --> 00:14:53,493
实现这些高级流配置

282
00:14:53,527 --> 00:14:55,596
不需要使用新的 API

283
00:14:55,629 --> 00:14:59,800
我们可以让您使用现有 API
启用高级流配置

284
00:15:01,068 --> 00:15:03,537
最后 我将讨论
您的 App 将如何

285
00:15:03,570 --> 00:15:06,340
在用户处理多任务时使用相机

286
00:15:06,373 --> 00:15:09,409
在 iPad 上 用户可以
通过多种方式执行多项任务

287
00:15:09,443 --> 00:15:14,114
例如 使用分屏浏览
同时阅读备忘录和录制语音备忘录

288
00:15:14,147 --> 00:15:16,817
或使用侧拉在
全屏 Safari 浏览器的上方

289
00:15:16,850 --> 00:15:19,520
使用备忘录的浮动窗口

290
00:15:19,553 --> 00:15:22,456
使用 Picture in Picture
您可以继续播放视频

291
00:15:22,489 --> 00:15:26,693
同时添加观看
更多 WWDC 视频的提醒

292
00:15:26,727 --> 00:15:29,463
借助 iPadOS 16 的
全新 Stage Manager

293
00:15:29,496 --> 00:15:33,767
用户可在大小不一的
浮动窗口中打开多个 App

294
00:15:33,800 --> 00:15:36,970
从 iOS 16 开始
AVCaptureSessions 将能够

295
00:15:37,004 --> 00:15:38,605
在处理多任务时使用相机

296
00:15:38,639 --> 00:15:41,508
我们之前在多任务时阻止了相机访问

297
00:15:41,542 --> 00:15:43,477
因为担心在处理多任务时

298
00:15:43,510 --> 00:15:46,246
相机系统呈现的效果会不好

299
00:15:46,280 --> 00:15:50,284
像游戏这类资源密集型 App
会与使用相机的 App 一起运行

300
00:15:50,317 --> 00:15:54,321
可能会导致丢帧和其他延迟
从而导致摄像头输入不佳

301
00:15:54,354 --> 00:15:57,991
用户在数月或数年后
观看质量较差的视频的时候

302
00:15:58,025 --> 00:16:00,894
可能不记得这是他们
在处理多任务处理时录制的视频

303
00:16:00,928 --> 00:16:05,065
提供良好的相机使用体验
是我们的首要任务

304
00:16:05,098 --> 00:16:07,601
当系统检测到相机在多任务过程中

305
00:16:07,634 --> 00:16:10,204
录制视频时 将显示一个对话框

306
00:16:10,237 --> 00:16:13,674
告知用户视频的质量可能会不高

307
00:16:13,707 --> 00:16:16,577
这个对话框在录制完成后

308
00:16:16,610 --> 00:16:20,314
由 AVCaptureMovieFileOutput
或 AVAssetWriter 显示

309
00:16:20,347 --> 00:16:23,050
这个对话框只会显示一次

310
00:16:23,083 --> 00:16:26,353
可用“确定”按钮关闭

311
00:16:26,386 --> 00:16:29,723
AVCaptureSession
添加了两个新属性

312
00:16:29,756 --> 00:16:33,527
在支持并启用
多任务相机访问时给予指示

313
00:16:33,560 --> 00:16:36,697
启用此功能的捕获会话将不再中断

314
00:16:36,730 --> 00:16:41,468
因为“视频设备
不适用于多个前台 App”

315
00:16:41,502 --> 00:16:43,937
某些 App 可能希望需要全屏

316
00:16:43,971 --> 00:16:45,305
使用相机

317
00:16:45,339 --> 00:16:47,908
如果您希望您的 App 不与其他
前台 App 一起竞争系统资源

318
00:16:47,941 --> 00:16:50,711
这可能会很有用

319
00:16:50,744 --> 00:16:55,048
例如 ARKit 不支持
在处理多任务时使用相机

320
00:16:56,350 --> 00:16:59,720
您应该确保您的 App 在与
其他 App 一起运行时表现良好

321
00:16:59,753 --> 00:17:02,422
监控 App 的通知
使您的 App 能够适应

322
00:17:02,456 --> 00:17:04,658
系统压力不断增加

323
00:17:04,691 --> 00:17:08,295
并采取措施减少压力的影响
例如降低帧速率

324
00:17:08,328 --> 00:17:10,264
您可以通过请求较低分辨率、分箱

325
00:17:10,297 --> 00:17:15,002
或非 HDR 格式
减少 App 在系统上的占用空间

326
00:17:15,035 --> 00:17:18,472
想了解更多保持性能最佳实践的信息

327
00:17:18,505 --> 00:17:22,042
请阅读文章
“在处理多任务时访问相机”

328
00:17:23,243 --> 00:17:27,681
此外 视频通话和视频会议 App
可以在系统提供的 Picture in Picture 窗口

329
00:17:27,714 --> 00:17:30,817
显示远程参与者

330
00:17:30,851 --> 00:17:33,921
现在 您 App 的用户可以
在 iPad 上进行多任务处理时

331
00:17:33,954 --> 00:17:36,323
继续流畅地视频通话

332
00:17:36,356 --> 00:17:41,128
AVKit 在 iOS 15 中引入了 API
是为 App 指定一个可以显示

333
00:17:41,161 --> 00:17:43,797
远程呼叫参与者的视图控制器

334
00:17:43,830 --> 00:17:46,366
视频通话视图控制器允许您自定义

335
00:17:46,400 --> 00:17:48,735
窗口的内容

336
00:17:48,769 --> 00:17:50,571
要了解更多采用方法

337
00:17:50,604 --> 00:17:55,008
请参阅文章
“采用 Picture in Picture 进行视频通话”

338
00:17:55,042 --> 00:17:58,045
至此即为
iOS 相机捕获技术的所有进步

339
00:17:58,078 --> 00:18:02,182
我展示了如何利用使用 AVFoundation 的
LiDAR 扫描仪达成流式传输深度

340
00:18:02,216 --> 00:18:05,185
您的 App 将如何
获得更好的面部渲染效果

341
00:18:05,219 --> 00:18:08,956
为您 App 量身定制的
高级 AVCaptureSession 流配置

342
00:18:08,989 --> 00:18:12,526
和您的 App 如何
在处理多任务时使用相机

343
00:18:12,559 --> 00:18:14,695
我希望您的 WWDC 精彩绝伦

344
00:18:14,728 --> 00:18:19,433
[欢快的音乐]

