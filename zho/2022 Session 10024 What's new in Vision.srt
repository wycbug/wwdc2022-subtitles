1
00:00:00,334 --> 00:00:07,341
♪ ♪

2
00:00:09,676 --> 00:00:11,645
Brett Keating:
大家好 我叫 Brett Keating

3
00:00:11,678 --> 00:00:15,549
很高兴为您介绍
Vision 框架中的新内容

4
00:00:15,582 --> 00:00:17,551
您可能刚接触 Vision

5
00:00:17,584 --> 00:00:21,388
也许这是您首次参加
有关 Vision 框架的讲座

6
00:00:21,421 --> 00:00:23,991
如果是这样的话 诚挚欢迎

7
00:00:24,024 --> 00:00:28,862
我们先简要回顾下
Vision 框架的亮点

8
00:00:28,896 --> 00:00:31,665
以下是关于
Vision 框架的一些信息

9
00:00:31,698 --> 00:00:34,434
Vision 于 2017 年首次推出

10
00:00:34,468 --> 00:00:37,871
从那时起 人们就使用
Vision 提供的技术

11
00:00:37,905 --> 00:00:40,841
开发出了无数好用的 App

12
00:00:40,874 --> 00:00:43,510
Vision 是
计算机视界算法的集合

13
00:00:43,544 --> 00:00:45,646
可随着时间的推移持续增长

14
00:00:45,679 --> 00:00:47,781
包括人脸检测 图像分类

15
00:00:47,814 --> 00:00:52,019
和轮廓定位 诸如此类

16
00:00:52,052 --> 00:00:56,490
所以这些算法中都可以
通过简便 统一的 API 来实现

17
00:00:56,523 --> 00:00:58,926
如果您知道如何在 Vision 框架中
运行一种算法

18
00:00:58,959 --> 00:01:01,061
就知道如何运行全部算法

19
00:01:01,094 --> 00:01:04,698
Vision 在它支持的所有平台上

20
00:01:04,731 --> 00:01:06,934
充分利用 Apple 芯片的优势

21
00:01:06,967 --> 00:01:10,771
在 Vision 的许多核心算法中
加强了机器学习能力

22
00:01:10,804 --> 00:01:13,473
Vision 适用于 tvOS

23
00:01:13,507 --> 00:01:15,709
iOS 和 macOS

24
00:01:15,742 --> 00:01:19,179
并将在 Mac 上充分利用
Apple 芯片的优势

25
00:01:20,113 --> 00:01:23,884
这里将展示关于 Vision 框架的
一些最新补充

26
00:01:23,917 --> 00:01:25,252
包括 Person Segmentation

27
00:01:27,287 --> 00:01:31,358
还有手势估计也会有相关的演示

28
00:01:34,761 --> 00:01:37,631
这是我们的 Action
和 Vision 示范 App

29
00:01:37,664 --> 00:01:41,134
它使用的是人体姿势估计
和轨迹分析

30
00:01:42,069 --> 00:01:46,306
我们先以一些新的修订
来开始今天的议程

31
00:01:46,340 --> 00:01:49,176
即对现有请求的更新

32
00:01:49,209 --> 00:01:54,081
可能会提供更多的功能
提高性能或提高准确性

33
00:01:57,150 --> 00:02:00,454
首先 我们有一个新的修订版
用于文本识别

34
00:02:00,487 --> 00:02:06,527
这是 VNRecognizeTextRequestRevision3
提供的第三次修订版

35
00:02:06,560 --> 00:02:10,464
这是文本识别器
有强大的实时文本功能

36
00:02:10,497 --> 00:02:13,333
文本识别器支持多种语言

37
00:02:13,367 --> 00:02:15,969
您可以
使用 supportedRecognitionLanguages

38
00:02:16,003 --> 00:02:19,173
查看目前支持的语种

39
00:02:19,206 --> 00:02:23,443
我们现在添加了一些新的语种
可以看下以下的例子

40
00:02:23,477 --> 00:02:27,247
现在 Vision 中支持韩语

41
00:02:27,281 --> 00:02:31,618
这是用 Vison 提取
韩语收据的例子

42
00:02:31,652 --> 00:02:35,255
这是一个对应的日语示例

43
00:02:35,289 --> 00:02:38,058
在当前支持语种中

44
00:02:38,091 --> 00:02:40,961
也显示了 Vision 的
文本识别结果

45
00:02:40,994 --> 00:02:46,767
文本识别方面 我们有
一种全新的自动语言识别功能

46
00:02:46,800 --> 00:02:50,170
您仍然可以使用
recognitionLanguages 属性

47
00:02:50,204 --> 00:02:53,740
指定要使用的识别语言

48
00:02:53,774 --> 00:02:56,577
但是要假设您之前并不知道

49
00:02:56,610 --> 00:02:59,913
您的 App 用户可能会
尝试识别哪种语言

50
00:02:59,947 --> 00:03:03,417
现在 仅对于准确的识别模式

51
00:03:03,450 --> 00:03:07,955
您可能会通过将 automaticallyDetectsLanguage的
值设为 true

52
00:03:07,988 --> 00:03:11,091
让文本识别器自动检测语言

53
00:03:12,759 --> 00:03:15,562
在您不知道识别哪种语言的情况下

54
00:03:15,596 --> 00:03:18,398
最好使用这种方法

55
00:03:18,432 --> 00:03:21,702
因为语言检测偶尔会出错

56
00:03:21,735 --> 00:03:25,038
如果您已经提前知道了识别哪种语言

57
00:03:25,072 --> 00:03:27,975
最好还是向 Vision
指定这些语言

58
00:03:28,008 --> 00:03:31,612
并关闭
automaticallyDetectsLanguage

59
00:03:34,681 --> 00:03:39,119
接下来 我们来看下用于
条码检测的第三次修订

60
00:03:39,152 --> 00:03:43,123
名为
VNDetectBarcodesRequestRevision3

61
00:03:43,156 --> 00:03:46,360
本次修订利用了
现代机器的后台学习能力

62
00:03:46,393 --> 00:03:49,162
这与之前的修订不同

63
00:03:49,196 --> 00:03:51,431
条码有多种符号

64
00:03:51,465 --> 00:03:55,769
包括商店产品上常见的条形码
二维码

65
00:03:55,802 --> 00:03:59,239
用于医疗保健的专业代码

66
00:03:59,273 --> 00:04:01,775
为了了解 Vision 支持哪些符号

67
00:04:01,808 --> 00:04:03,977
您可以
调用 supportedSymbologies

68
00:04:06,113 --> 00:04:08,582
接下来 我们谈谈性能

69
00:04:08,615 --> 00:04:10,617
一部分是因为我们使用的是
机器学习

70
00:04:10,651 --> 00:04:15,222
可一次检测多个代码时
而不是一次一个

71
00:04:15,255 --> 00:04:20,160
所以图像包含多个代码的
请求会更快一些

72
00:04:20,194 --> 00:04:25,465
此外 由于提高了准确性

73
00:04:25,499 --> 00:04:27,167
且极少重复检测

74
00:04:27,201 --> 00:04:31,538
在包含许多代码的给定图像中
可以检测更多代码

75
00:04:31,572 --> 00:04:34,441
针对一些代码 边界框做了改进

76
00:04:34,474 --> 00:04:39,780
特别是线性代码 例如 ean13
之前是返回了一行

77
00:04:39,813 --> 00:04:42,850
现在 边界框包围了
整个可见代码

78
00:04:45,018 --> 00:04:49,022
最后 机器学习模型可以忽略弧面

79
00:04:49,056 --> 00:04:53,393
反射以及其它曾经
影响检测准确性的东西

80
00:04:56,797 --> 00:04:59,733
这两个新修订版可用于文本识别

81
00:04:59,766 --> 00:05:03,504
条形码检测
为插入式 UI 元素的

82
00:05:03,537 --> 00:05:08,075
VisionKit 数据扫描器 API
搭建技术基础

83
00:05:08,108 --> 00:05:11,879
可以设置相机流
扫描并返回条形码和文本

84
00:05:11,912 --> 00:05:14,948
这对我们的 SDK 来说
是一个神级补充

85
00:05:14,982 --> 00:05:19,119
我强烈建议您查看
有关的讲座以了解更多信息

86
00:05:19,152 --> 00:05:22,956
今天我要告诉您的
最后的一个全新修订版

87
00:05:22,990 --> 00:05:25,626
是一个光流请求的新版本

88
00:05:25,659 --> 00:05:29,363
名为
VNGenerateOpticalFlowRequestRevision2

89
00:05:29,396 --> 00:05:32,900
和条码检测器一样
这个新版本也利用了

90
00:05:32,933 --> 00:05:35,102
现代机器的后台学习能力

91
00:05:37,471 --> 00:05:41,508
虽然光流法是一个
研究时间最长的计算机视觉问题

92
00:05:41,542 --> 00:05:44,444
与构成我们日常生活一部分的
文字和条形码检测相比

93
00:05:44,478 --> 00:05:47,981
您可能对它的作用不太了解

94
00:05:48,849 --> 00:05:52,452
光流分析两个连续的图像

95
00:05:52,486 --> 00:05:54,621
通常来自视频的帧

96
00:05:54,655 --> 00:05:56,590
根据您的使用场景 可能会看到

97
00:05:56,623 --> 00:06:00,027
两个相邻的帧之间的动作
或在其间跳过几帧

98
00:06:00,060 --> 00:06:02,996
但无论如何 这两个图像
应该按时间顺序排列

99
00:06:04,831 --> 00:06:09,303
该分析提供了一个
运动的方向和幅度的估计

100
00:06:09,336 --> 00:06:13,774
或第一张图像有多少部分需要
“move (移动)”

101
00:06:13,807 --> 00:06:17,544
才能在第二张图像中正确定位

102
00:06:17,578 --> 00:06:20,480
结果是 VNPixelBufferObservation

103
00:06:20,514 --> 00:06:23,684
这代表这个动作
在图像中的所有位置

104
00:06:23,717 --> 00:06:25,485
这是一个双通道图像

105
00:06:25,519 --> 00:06:28,021
一个通道包含 X 幅度

106
00:06:28,055 --> 00:06:30,791
另一个包含 Y 幅度

107
00:06:30,824 --> 00:06:34,094
这些合起来在每个像素上
形成二维向量

108
00:06:34,127 --> 00:06:37,064
排列在这个 2D 图像中
以便他们的位置

109
00:06:37,097 --> 00:06:41,134
映射到图像中的相应位置
作为输入

110
00:06:41,168 --> 00:06:43,470
我们从这个视角看一下

111
00:06:43,504 --> 00:06:47,541
假设有一个传入的视频
并且有几帧画面传进来

112
00:06:47,574 --> 00:06:50,210
我们特别关注一下这两个图像

113
00:06:50,244 --> 00:06:52,479
在这里 有一只狗在沙滩上奔跑

114
00:06:52,513 --> 00:06:54,248
从左图到右图

115
00:06:54,281 --> 00:06:57,017
似乎这只狗已经向左移动了一点

116
00:06:57,050 --> 00:06:59,419
您会如何估算并表现这个移动

117
00:07:01,255 --> 00:07:03,323
对 您会运行光流法

118
00:07:03,357 --> 00:07:05,926
然后得到类似于下图的图像

119
00:07:05,959 --> 00:07:08,562
较暗的区域是发现移动的地方

120
00:07:08,595 --> 00:07:12,566
它确实看起来像狗的外形

121
00:07:12,599 --> 00:07:16,170
这是因为这个场景中
只有狗真正有所移动

122
00:07:16,203 --> 00:07:19,907
在这张图中我们使用
“false color”展示了运动矢量

123
00:07:19,940 --> 00:07:23,777
从向量到调色板映射 x y

124
00:07:23,810 --> 00:07:27,681
在这种 false color 的表现中
红色色调恰好表明

125
00:07:27,714 --> 00:07:30,150
向左移动

126
00:07:30,184 --> 00:07:33,253
现在您已经看到了一帧的例子

127
00:07:33,287 --> 00:07:35,989
接下来我们看看在
整个视频剪辑中是怎么样的

128
00:07:36,023 --> 00:07:39,660
这是小狗在沙滩接水瓶的短片

129
00:07:39,693 --> 00:07:42,196
我们对其进行了光流计算

130
00:07:42,229 --> 00:07:45,065
左边是修订版 1 的结果

131
00:07:45,098 --> 00:07:49,002
右边是我们基于机器语言的
修订版 2 的结果

132
00:07:49,036 --> 00:07:52,639
希望可以清楚看到
修订版 2 的一些改进

133
00:07:52,673 --> 00:07:55,142
一方面 也是最明显的

134
00:07:55,175 --> 00:07:58,712
水瓶的动作捕捉更准确

135
00:07:58,745 --> 00:08:03,083
您可能还会看到
狗的预估动作有所改进

136
00:08:03,116 --> 00:08:05,953
到尾部的变化最为清楚

137
00:08:05,986 --> 00:08:10,023
但是在新的版本中
也能看到它的耳朵在动

138
00:08:10,057 --> 00:08:13,293
第一个修订版还包含
一些背景声音的变化

139
00:08:13,327 --> 00:08:17,664
而第二次修订更加连贯
表示背景没有移动

140
00:08:17,698 --> 00:08:21,668
希望这个例子能让您对这项技术的
作用有更全面的了解

141
00:08:21,702 --> 00:08:25,272
现在我们深入了解一下
如何在您的 App 中使用这一技术

142
00:08:25,305 --> 00:08:29,910
很明显 最主要的使用案例
是发现视频中的局部运动

143
00:08:29,943 --> 00:08:32,980
这可直接应用到安防视频使用场景

144
00:08:33,013 --> 00:08:35,682
在这类视频中 识别和定位

145
00:08:35,716 --> 00:08:37,784
偏离背景的局部动作是最重要的

146
00:08:37,818 --> 00:08:40,053
应该说 对于固定相机来说

147
00:08:40,087 --> 00:08:44,391
光流确实效果最好
例如大多数安全摄像头

148
00:08:44,424 --> 00:08:46,393
您可能想要使用
Vision 的目标跟踪器

149
00:08:46,426 --> 00:08:48,795
跟踪视频中移动的对象

150
00:08:48,829 --> 00:08:51,265
但需要知道在哪里初始化跟踪器

151
00:08:51,298 --> 00:08:54,501
光流也可以提供帮助

152
00:08:54,535 --> 00:08:58,338
如果您懂得计算机视觉
或图像处理

153
00:08:58,372 --> 00:09:00,407
您可能会利用我们的光流结果

154
00:09:00,440 --> 00:09:03,076
以启用进一步的视频处理

155
00:09:03,110 --> 00:09:06,747
视频插值或视频动作分析

156
00:09:06,780 --> 00:09:10,851
可以从信息光流中大大受益

157
00:09:10,884 --> 00:09:13,854
现在 我们深入研究下
修订版 1 和修订版 2 之间

158
00:09:13,887 --> 00:09:16,023
重要的附加差异

159
00:09:16,857 --> 00:09:19,459
修订版 1 总能返回光流场

160
00:09:19,493 --> 00:09:22,029
作为输入 具有相同的分辨率

161
00:09:22,062 --> 00:09:25,032
默认情况下 修订版 2
也将执行此默认设定值

162
00:09:25,065 --> 00:09:28,135
不过 这里也有一个小问题
一部分是因为

163
00:09:28,168 --> 00:09:32,039
修订版 2 是基于机器语言
基础模型的输出

164
00:09:32,072 --> 00:09:36,710
与大多数输入图像分辨率相比
分辨率相对较低

165
00:09:36,743 --> 00:09:40,414
因此 要匹配修订版 1 的
默认行为

166
00:09:40,447 --> 00:09:42,382
必须进行一些上采样

167
00:09:42,416 --> 00:09:45,853
我们使用双线性上采样来实现

168
00:09:45,886 --> 00:09:48,856
这是解释上采样作用的视觉示例

169
00:09:48,889 --> 00:09:52,659
左边 我们有一个网络输出的
放大部分

170
00:09:52,693 --> 00:09:55,863
这是低分辨率 因此出现像素化

171
00:09:55,896 --> 00:10:00,000
整体流场可能宽高比为 7:5

172
00:10:00,033 --> 00:10:03,504
右边 我们有一个取自同一场的
相同的区域

173
00:10:03,537 --> 00:10:06,507
上采样到原始图像分辨率

174
00:10:06,540 --> 00:10:11,645
也许那个图像也有
不同的纵横比 如 16:9

175
00:10:11,678 --> 00:10:14,948
您会注意到流场的边缘
通过双线性上采样

176
00:10:14,982 --> 00:10:18,018
变得平滑

177
00:10:18,051 --> 00:10:20,854
由于潜在不同的纵横比

178
00:10:20,888 --> 00:10:23,524
记住 作为上采样过程的一部分

179
00:10:23,557 --> 00:10:25,893
流动图像会被拉伸

180
00:10:25,926 --> 00:10:28,061
为的是流场

181
00:10:28,095 --> 00:10:30,230
与图像中发生的变化相对应

182
00:10:30,264 --> 00:10:32,266
当直接进行网络输出时

183
00:10:32,299 --> 00:10:36,136
您应该考虑这与映射流结果
到原始图像时

184
00:10:36,170 --> 00:10:39,006
分辨率和纵横比的方式是类似的

185
00:10:41,575 --> 00:10:43,777
您可以通过请求
打开 keepNetworkOutput

186
00:10:43,810 --> 00:10:47,581
选择跳过上采样

187
00:10:47,614 --> 00:10:49,983
这将为您提供原始模型输出

188
00:10:50,017 --> 00:10:54,154
为了选择可用的输出分辨率

189
00:10:54,188 --> 00:10:57,357
有四种计算精度
可以应用于请求的设置

190
00:10:57,391 --> 00:11:00,694
您可以看到分辨率
在此表中的每个精度设置

191
00:11:00,727 --> 00:11:03,297
但一定要经常检查观察到的

192
00:11:03,330 --> 00:11:05,265
像素缓冲区的宽度和高度

193
00:11:06,066 --> 00:11:07,634
什么时候应该使用网络输出

194
00:11:07,668 --> 00:11:10,404
什么时候应该允许
Vision 上采样

195
00:11:10,437 --> 00:11:14,408
如果您已经在使用光流
并想要这种行为向后兼容

196
00:11:14,441 --> 00:11:17,678
默认行为无疑是最好的

197
00:11:17,711 --> 00:11:20,280
如果您想要上采样输出
也是一个不错的选择

198
00:11:20,314 --> 00:11:24,885
双线性对您来说是可以接受的
值得额外的内存和延迟

199
00:11:24,918 --> 00:11:28,355
如果您不需要全分辨率
网络输出是最好的

200
00:11:28,388 --> 00:11:33,227
它可以即时形成对应关系
或者初始化一个跟踪器

201
00:11:33,260 --> 00:11:35,262
如果您确实需要完整的分辨率流

202
00:11:35,295 --> 00:11:37,364
但更愿意使用
您自己的上采样方法

203
00:11:37,397 --> 00:11:40,567
网络输出也可能是
正确的选择

204
00:11:40,601 --> 00:11:44,705
这涵盖了本次讲座中
新的算法修订

205
00:11:44,738 --> 00:11:47,140
接下来我们讨论一下
在 Vision 框架中

206
00:11:47,174 --> 00:11:50,511
所讲到的所有内容
以及对您有何影响

207
00:11:50,544 --> 00:11:53,780
五年前
当 Vision 最初发布时

208
00:11:53,814 --> 00:11:56,316
我们首次介绍了人脸检测和
人脸关键点标记

209
00:11:56,350 --> 00:11:59,419
这是每个算法的修订版 1

210
00:11:59,453 --> 00:12:03,123
从那之后
我们发布了两个更新的版本

211
00:12:03,156 --> 00:12:06,593
使用效率更高 技术更准确

212
00:12:06,627 --> 00:12:10,330
所以 我们现在正在
不断从 Vision 框架

213
00:12:10,364 --> 00:12:15,035
移除这些算法的第一次修订
并保留第二次和第三次修订

214
00:12:15,068 --> 00:12:18,138
不过 如果您使用的是修订版 1
那也没关系

215
00:12:18,172 --> 00:12:21,708
我们将继续支持
指定修订版 1 的代码

216
00:12:21,742 --> 00:12:26,947
或已编译的代码
仅包含修订版 1 的 SDK

217
00:12:26,980 --> 00:12:28,749
您可能会问 这怎么可能呢

218
00:12:28,782 --> 00:12:32,252
修订版 1 在后台执行算法

219
00:12:32,286 --> 00:12:36,256
在此图中 我称之为
“修订版 1 检测器”

220
00:12:36,290 --> 00:12:40,527
同样 修订版 2 使用
修订版 2 检测器

221
00:12:40,561 --> 00:12:42,796
我们为此版本所做的

222
00:12:42,829 --> 00:12:45,065
是为了使用修订版 2 检测器的输出

223
00:12:45,098 --> 00:12:48,101
满足修订 1 的请求

224
00:12:48,135 --> 00:12:52,506
此外 修订 1 请求
将被标记为已弃用

225
00:12:52,539 --> 00:12:56,376
这允许我们完全删除
旧的版本 1 的检测器

226
00:12:56,410 --> 00:12:59,613
允许 Vision 框架保持精简

227
00:12:59,646 --> 00:13:01,448
这有多个好处

228
00:13:01,481 --> 00:13:04,318
不只是为了节省磁盘空间

229
00:13:04,351 --> 00:13:09,256
降低操作系统版本和 SDK 的
下载和安装成本

230
00:13:09,289 --> 00:13:13,760
所有的 Vision 专家都可能有
对自己说 “但是 等一下”

231
00:13:13,794 --> 00:13:18,298
“修订版 2 会返回倒置
而修订版 1 不会”

232
00:13:18,332 --> 00:13:21,435
“难道这种行为差异不会
影响某些 App 吗”

233
00:13:21,468 --> 00:13:24,605
答案是肯定的 除非我们采取

234
00:13:24,638 --> 00:13:27,541
保留修订版 1 行为的
预防性措施

235
00:13:27,574 --> 00:13:32,312
我们不会从修订版 2 检测器
返回倒置

236
00:13:32,346 --> 00:13:35,616
类似的
修订版 2 标记检测器

237
00:13:35,649 --> 00:13:40,053
将返回与修订版 1 标记
相匹配的结果

238
00:13:40,087 --> 00:13:42,389
执行时间相当

239
00:13:42,422 --> 00:13:45,726
您应该体验准确性的提升

240
00:13:45,759 --> 00:13:48,262
任何情况下
此改变不需要任何 App

241
00:13:48,295 --> 00:13:52,332
对其代码进行任何修改
将继续有效

242
00:13:54,134 --> 00:13:57,237
不过 我们仍鼓励您采取行动

243
00:13:57,271 --> 00:13:59,573
当有更好的选择时

244
00:13:59,606 --> 00:14:02,509
不应该满足于使用修订版 1

245
00:14:02,543 --> 00:14:04,645
我们一直推荐使用最新版本

246
00:14:04,678 --> 00:14:07,548
应这些请求 则应该是修订版 3

247
00:14:08,749 --> 00:14:11,251
当然 这个建议的主要原因

248
00:14:11,285 --> 00:14:14,588
是使用最新的技术
它提供了最高级别的

249
00:14:14,621 --> 00:14:18,725
准确性和性能 谁又会拒绝呢

250
00:14:18,759 --> 00:14:22,129
此外 我们经过了数次建立和沟通

251
00:14:22,162 --> 00:14:24,565
我们在此再次重申

252
00:14:24,598 --> 00:14:28,368
最佳的实践是明确指定
您的修订版本

253
00:14:28,402 --> 00:14:31,371
而不是依靠默认行为

254
00:14:31,405 --> 00:14:34,408
这就是我们在
Spring 清扫中所做的

255
00:14:34,441 --> 00:14:36,944
现在来谈一谈我们是如何
使用 Vision 框架

256
00:14:36,977 --> 00:14:38,812
简化调试 App 流程的

257
00:14:38,846 --> 00:14:41,949
我们为 Vision
添加了快速查看预览支持

258
00:14:41,982 --> 00:14:44,685
这对 Vision 来说意味着什么

259
00:14:44,718 --> 00:14:48,255
好了 现在您可以将鼠标悬停在
调试器中的 VNObservations

260
00:14:48,288 --> 00:14:52,860
只需单击一下 输入图像上的结果
就得以可视化

261
00:14:52,893 --> 00:14:55,863
Xcode Playgrounds 也同样适用

262
00:14:55,896 --> 00:14:59,099
要了解这对您的调试有何帮助
唯一方法

263
00:14:59,132 --> 00:15:00,334
就是演示一遍

264
00:15:00,367 --> 00:15:02,603
我们来看一个 Xcode 演示

265
00:15:04,471 --> 00:15:08,208
这里有一个简单的例程
用来检测面部标记

266
00:15:08,242 --> 00:15:11,345
并返回人脸观察结果

267
00:15:11,378 --> 00:15:15,849
首先 我们设置了人脸标记请求

268
00:15:15,883 --> 00:15:20,921
然后 如果准备好了一个
课堂上用的图像 就展示出来

269
00:15:20,954 --> 00:15:24,458
然后 公布一个数列来保存结果

270
00:15:26,126 --> 00:15:27,995
在自动释放池内

271
00:15:28,028 --> 00:15:31,098
我们用那个图像
实例化一个请求处理程序

272
00:15:31,131 --> 00:15:34,034
然后执行我们的请求

273
00:15:34,067 --> 00:15:38,338
假设一切顺利 我们可以检索
请求的结果

274
00:15:39,206 --> 00:15:44,011
在我们检索结果之后
我运行它并到达断点

275
00:15:44,044 --> 00:15:45,812
现在我在调试程序

276
00:15:45,846 --> 00:15:47,581
当我将鼠标悬停在结果上时

277
00:15:47,614 --> 00:15:50,284
覆盖图显示我检测到三张脸

278
00:15:50,317 --> 00:15:53,754
不错 我的输入图像中
确实有三张图

279
00:15:53,787 --> 00:15:56,557
但我怎么知道观察到的是哪张脸呢

280
00:15:56,590 --> 00:15:59,927
这就是
Quick Look Preview 的工作

281
00:15:59,960 --> 00:16:04,898
当我提出这个要求时
为了可视化结果

282
00:16:04,932 --> 00:16:07,501
可以点击每个“eye (眼睛)”图标

283
00:16:07,534 --> 00:16:12,206
标记的地方和人脸边界框

284
00:16:12,239 --> 00:16:14,408
出现了绘制的叠加层

285
00:16:15,676 --> 00:16:18,612
现在您就知道
第一个观察到的是哪个图像

286
00:16:19,780 --> 00:16:23,250
我可以点击下一个
画出第二次观察的叠加

287
00:16:23,283 --> 00:16:25,619
以及第三次观察

288
00:16:27,521 --> 00:16:30,924
继续下一个断点
我们运行一些

289
00:16:30,958 --> 00:16:34,561
打印面部观察结果代码
到调试控制台

290
00:16:34,595 --> 00:16:37,497
可以想象 在打印面部信息的

291
00:16:37,531 --> 00:16:40,534
调试控制台中

292
00:16:40,567 --> 00:16:43,570
您的脑海里很难马上想象
每张脸是属于谁的

293
00:16:43,604 --> 00:16:46,907
或者从这些打印的标记来判断
结果是否正确

294
00:16:48,976 --> 00:16:51,245
这里还要指出一点

295
00:16:51,278 --> 00:16:54,515
我通过引入 autoreleasepool

296
00:16:54,548 --> 00:16:57,918
人为地强制请求超出范围的
处理程序

297
00:16:57,951 --> 00:17:00,420
既然请求处理程序超出范围

298
00:17:00,454 --> 00:17:03,924
我们再次使用 Quick Look Preview
支持这个结果

299
00:17:03,957 --> 00:17:06,693
好的 您知道覆盖仍然被绘制

300
00:17:06,727 --> 00:17:08,395
但图像是不可用的

301
00:17:09,496 --> 00:17:12,966
需要牢记的是
使用的图像请求处理程序

302
00:17:13,000 --> 00:17:16,737
生成观察结果
必须仍在某个范围内

303
00:17:16,770 --> 00:17:21,108
从而让 Quick Look Preview
可显示原始图像

304
00:17:21,141 --> 00:17:25,212
那是因为图像请求处理程序
是您的输入图像所在的位置

305
00:17:25,245 --> 00:17:28,348
操作将继续 但图像不可用

306
00:17:28,382 --> 00:17:31,952
这种快速查看预览支持
在 Xcode Playgrounds 会话中

307
00:17:31,985 --> 00:17:34,054
做快速实验时 查看运作时

308
00:17:34,087 --> 00:17:37,691
非常有用

309
00:17:37,724 --> 00:17:40,160
现在我们来看看

310
00:17:40,194 --> 00:17:44,865
在这里 我们设置了一个简单的场地
分析图像的条形码

311
00:17:44,898 --> 00:17:47,935
而不是查看这段代码
我们做一些修改

312
00:17:47,968 --> 00:17:50,537
并检查它对结果的影响

313
00:17:50,571 --> 00:17:52,773
我们首先在两个条形码的图像上

314
00:17:52,806 --> 00:17:56,743
使用修订版 2

315
00:17:56,777 --> 00:18:00,113
如果我们要求所有的结果
所有结果就会一次性显示出来

316
00:18:00,147 --> 00:18:03,150
第一个结果也在最后显示出来

317
00:18:04,718 --> 00:18:07,287
修订版 2 有几个问题

318
00:18:07,321 --> 00:18:10,257
首先 它错过了第一个条形码

319
00:18:10,290 --> 00:18:13,427
此外 它检测到第二个条形码两次

320
00:18:13,460 --> 00:18:15,329
它通过条形码
给您提供了一条线

321
00:18:15,362 --> 00:18:17,564
而不是一个完整的边界框

322
00:18:19,566 --> 00:18:23,637
如果我们现在改为修订版 3 而非 2
会发生什么呢

323
00:18:26,106 --> 00:18:28,609
首先 我们检测两个条形码

324
00:18:28,642 --> 00:18:32,346
得到的不是一条线
而是完整的边界框

325
00:18:33,313 --> 00:18:35,949
Quick Look Preview 支持
最了不起的就是

326
00:18:35,983 --> 00:18:39,553
无需您的帮助 也能以可视化结果

327
00:18:39,586 --> 00:18:41,522
编写各种实用函数

328
00:18:41,555 --> 00:18:44,625
它们可以直接叠加
在调试器中的图像上

329
00:18:44,658 --> 00:18:46,560
或 Xcode Playground 中

330
00:18:49,796 --> 00:18:54,101
这就是 Vision 中的
Quick Look Preview 支持

331
00:18:54,134 --> 00:18:58,071
现在您可以更轻松地了解
观察到的是哪一个

332
00:18:58,105 --> 00:19:00,407
为了将它与您的输入图像
一起使用

333
00:19:00,440 --> 00:19:02,976
只要确保保持范围内的
图像请求处理程序

334
00:19:03,010 --> 00:19:06,113
希望 Xcode Playground 的支持
帮助您更容易实现

335
00:19:06,146 --> 00:19:08,482
Vision 框架代码 Live Tuning

336
00:19:08,515 --> 00:19:11,418
我们今天讲了 Vision 中
一些重要的更新

337
00:19:11,451 --> 00:19:14,588
出于快速查看的目的
我们为文本识别

338
00:19:14,621 --> 00:19:19,493
条形码检测和光流添加了重大修订

339
00:19:21,295 --> 00:19:25,999
随着修订的持续进行
我们也将删除旧的版本

340
00:19:26,033 --> 00:19:27,768
所以要保持更新

341
00:19:27,801 --> 00:19:30,771
使用最新 最好的技术

342
00:19:30,804 --> 00:19:34,241
我们也让 Vision 的应用
在 Quick Look Preview 支持下的调试

343
00:19:34,274 --> 00:19:36,643
更加容易

344
00:19:36,677 --> 00:19:41,048
希望您喜欢本次讲座
并享受精彩纷呈的 WWDC

