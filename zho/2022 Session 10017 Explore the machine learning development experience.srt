1
00:00:00,334 --> 00:00:07,341
♪ ♪

2
00:00:09,943 --> 00:00:12,246
Geppy Parziale:
大家好 我是 Geppy Parziale

3
00:00:12,279 --> 00:00:15,148
我是 Apple 的一名机器学习工程师

4
00:00:15,182 --> 00:00:18,952
今天 我想带大家来一趟
构建 App 之旅

5
00:00:18,986 --> 00:00:21,355
运用机器学习来解决

6
00:00:21,388 --> 00:00:25,893
通常需要专家才能完成的工作

7
00:00:27,394 --> 00:00:31,098
这段旅程让我有机会向您展示

8
00:00:31,131 --> 00:00:33,600
如何将开源机器学习模型
添加到您的 App 中

9
00:00:33,634 --> 00:00:37,471
并创造奇妙的新体验

10
00:00:37,504 --> 00:00:39,640
在这一过程中 我也会重点介绍

11
00:00:39,673 --> 00:00:44,411
Apple 开发生态中的一些工具 框架和 API

12
00:00:44,444 --> 00:00:48,749
方便您用机器学习来构建 App

13
00:00:49,983 --> 00:00:53,654
在构建 App 时
您作为开发者会做一系列决定

14
00:00:53,687 --> 00:00:57,824
希望能为您的用户提供最佳体验

15
00:00:57,858 --> 00:01:02,396
在添加机器学习功能到 App 时
也是如此

16
00:01:04,031 --> 00:01:07,067
在开发过程中 您或许会问

17
00:01:07,100 --> 00:01:10,637
我应该使用机器学习来
构建这个功能吗

18
00:01:10,671 --> 00:01:14,408
如何获得机器学习模型

19
00:01:14,441 --> 00:01:18,745
我该如何让那个模型
与 Apple 平台相兼容

20
00:01:18,779 --> 00:01:22,716
这个模型是否适合我的用户场景

21
00:01:22,749 --> 00:01:26,320
它可以在
Apple 神经网络引擎上运行吗

22
00:01:26,353 --> 00:01:29,590
让我们一起踏上这段旅程吧

23
00:01:29,623 --> 00:01:33,193
我想构建一个
给我家庭黑白照片

24
00:01:33,227 --> 00:01:38,198
添加逼真色彩的 App 这些照片
是我在地下室的一个旧盒子里发现的

25
00:01:39,666 --> 00:01:44,338
当然 专业摄影师可能要通过
辛苦的劳动

26
00:01:44,371 --> 00:01:47,908
花一些时间用照片编辑工具做到这一点

27
00:01:47,941 --> 00:01:50,744
但是 如果我想自动化这个过程

28
00:01:50,777 --> 00:01:54,515
那么如何在几秒钟内完成着色呢

29
00:01:54,548 --> 00:01:57,584
对于机器学习来说
这似乎是一个完美的任务

30
00:01:58,919 --> 00:02:02,155
Apple 提供了数量巨大的框架和工具

31
00:02:02,189 --> 00:02:06,026
可以帮助您在 App 内
构建和集成 ML 功能

32
00:02:06,960 --> 00:02:09,296
从数据处理到模型训练和推理

33
00:02:09,329 --> 00:02:11,999
都可以提供

34
00:02:12,032 --> 00:02:15,669
这一次 我会对其中一些进行使用

35
00:02:15,702 --> 00:02:19,006
但是要记住 根据您正在进行的

36
00:02:19,039 --> 00:02:22,910
具体的机器学习 您有很多种选择

37
00:02:22,943 --> 00:02:25,712
我在我的 App 中
开发机器学习功能时

38
00:02:25,746 --> 00:02:28,582
经历了一系列阶段

39
00:02:29,650 --> 00:02:32,753
首先 我在技术论文
或专门的网站

40
00:02:32,786 --> 00:02:36,723
找到正确的机器学习模型

41
00:02:38,058 --> 00:02:40,360
我搜索了照片着色

42
00:02:40,394 --> 00:02:45,332
并找到了一个名为 Colorizer 的模型
可能适用于我的需求

43
00:02:46,233 --> 00:02:50,337
通过这个模型
我可以获得这样的着色效果

44
00:02:53,473 --> 00:02:55,175
这是另一个

45
00:02:56,643 --> 00:03:00,547
这是另一个 非常好

46
00:03:00,581 --> 00:03:03,083
我来给您演示一下它是如何运行的

47
00:03:03,116 --> 00:03:07,821
着色器模型期望输入黑白图像

48
00:03:07,855 --> 00:03:14,228
我找到的 Python 源代码可以将任何
RGB 图像转换为 LAB 颜色空间图像

49
00:03:15,362 --> 00:03:18,031
这个颜色空间有三个通道

50
00:03:18,065 --> 00:03:22,102
一个通道代表图像亮度或简称 L 通道

51
00:03:22,135 --> 00:03:25,806
而另外两个通道代表颜色组成

52
00:03:25,839 --> 00:03:27,875
把亮度作为着色器模型的输入

53
00:03:27,908 --> 00:03:31,378
丢弃两个颜色通道

54
00:03:32,646 --> 00:03:35,516
模型会预测出两个新的颜色通道

55
00:03:35,549 --> 00:03:40,053
与输入的 L 通道相结合
形成最终的彩色图像

56
00:03:41,154 --> 00:03:45,459
让我们开始制作和我的 App
相兼容的模型

57
00:03:45,492 --> 00:03:49,496
为此 我可以将原始的 PyTorch 模型

58
00:03:49,530 --> 00:03:53,300
通过 coremltools 转换成
Core ML 格式

59
00:03:53,333 --> 00:03:58,071
这是我使用的将 PyTorch 模型
转换为 Core ML 的简单 Python 脚本

60
00:03:59,439 --> 00:04:02,910
首先 我导入 PyTorch 模型结构和权重

61
00:04:04,378 --> 00:04:07,114
然后跟踪导入的模型

62
00:04:07,147 --> 00:04:11,285
最终将 PyTorch 模型
转换成 Core ML 并保存

63
00:04:12,953 --> 00:04:15,389
一旦模型采用 Core ML 格式

64
00:04:15,422 --> 00:04:18,992
我就需要验证一下
转换工作是否成功

65
00:04:19,026 --> 00:04:23,130
我可以直接用 Python
在 coremltools 中验证

66
00:04:23,163 --> 00:04:25,499
这很简单

67
00:04:25,532 --> 00:04:28,702
我在 RGB 颜色空间中导入图像

68
00:04:28,735 --> 00:04:31,238
并将其转换为 Lab 颜色空间

69
00:04:32,940 --> 00:04:37,144
单独取出亮度通道
把颜色通道丢弃

70
00:04:38,745 --> 00:04:41,515
使用 Core ML 模型进行预测

71
00:04:42,616 --> 00:04:44,918
最后用预测的颜色通道
和输入的亮度通道

72
00:04:44,952 --> 00:04:48,355
组合成 LAB 图像再转换为 RGB

73
00:04:49,723 --> 00:04:53,794
这让我可以验证
转换后的模型功能

74
00:04:53,827 --> 00:04:57,464
与原始 PyTorch 模型的功能是否匹配

75
00:04:57,497 --> 00:05:01,301
我将此阶段称为模型验证

76
00:05:01,335 --> 00:05:05,305
不过 还有一个重要的检验需要完成

77
00:05:05,339 --> 00:05:10,644
我需要了解这个模型
在我的目标设备上运行得是否足够快

78
00:05:10,677 --> 00:05:13,747
所以我需要在设备上评估模型

79
00:05:13,780 --> 00:05:17,851
确保它可以提供
最好的用户体验

80
00:05:17,885 --> 00:05:22,322
在 Xcode 14 中支持
新的 Core ML 性能报告

81
00:05:22,356 --> 00:05:26,393
针对 Core ML 模型
执行一个基于时间的分析

82
00:05:26,426 --> 00:05:29,630
只需要将模型拖放到 Xcode 中

83
00:05:29,663 --> 00:05:32,699
在几秒之内就能创建一个性能报告

84
00:05:33,834 --> 00:05:37,871
使用这个工具
可以看到预估的预测时间

85
00:05:37,905 --> 00:05:43,577
在 M1 和运行 iPadOS 16 的 iPad Pro 上
差不多是 90 毫秒

86
00:05:44,578 --> 00:05:48,215
对我的照片着色 App 很完美

87
00:05:48,248 --> 00:05:51,718
如果您想了解更多关于
性能报告的信息

88
00:05:51,752 --> 00:05:56,957
建议您观看今年的课程
“优化您的 Core ML 使用”

89
00:05:56,990 --> 00:06:01,128
性能报告可以帮助您
对模型进行评估

90
00:06:01,161 --> 00:06:04,798
并确保它提供
最佳的设备端用户体验

91
00:06:05,966 --> 00:06:09,937
确定模型功能和性能后

92
00:06:09,970 --> 00:06:11,872
下面让我将它集成到 App 内

93
00:06:13,507 --> 00:06:18,178
集成过程与之前在 Python 中
所做的是相同的

94
00:06:18,212 --> 00:06:20,848
但这一次 使用 Xcode
和所有您熟悉的其他工具

95
00:06:20,881 --> 00:06:24,151
在 Swift 中无缝完成

96
00:06:26,286 --> 00:06:29,289
记住 这个模型现在是 Core ML 格式

97
00:06:29,323 --> 00:06:33,227
需要一个代表亮度的单通道图像

98
00:06:34,695 --> 00:06:37,798
所以类似于我
之前在 Python 中所做的

99
00:06:37,831 --> 00:06:43,203
我需要将输入的 RGB 图像
转换到 Lab 颜色空间图像

100
00:06:45,839 --> 00:06:48,408
我可以以多种方式实现这个转换

101
00:06:48,442 --> 00:06:52,212
直接在 Swift 中
使用 vImage 或 Metal

102
00:06:53,514 --> 00:06:57,784
深入探索文档
我发现 Core Image 框架

103
00:06:57,818 --> 00:07:00,554
可以提供一些有帮助的东西

104
00:07:02,656 --> 00:07:06,360
所以 让我告诉您如何实现
RGB 到 LAB 的转换

105
00:07:06,393 --> 00:07:08,929
并使用 Core ML 模型进行预测

106
00:07:10,864 --> 00:07:13,667
这是 Swift 代码用来从 RGB 图像中

107
00:07:13,700 --> 00:07:17,538
提取亮度并将其传递给 Core ML 模型

108
00:07:17,571 --> 00:07:22,509
首先 我将 RGB 图像转换成 LAB
并提取亮度

109
00:07:23,944 --> 00:07:27,014
然后 我将亮度转换成 CGImage

110
00:07:27,047 --> 00:07:30,050
并为 Core ML 模型准备输入

111
00:07:31,585 --> 00:07:33,921
最后 进行预测

112
00:07:33,954 --> 00:07:37,658
要从输入的 RGB 图像得到 L 通道

113
00:07:37,691 --> 00:07:41,094
首先使用新的
CIFilter convertRGBtoLab

114
00:07:41,128 --> 00:07:45,365
将 RGB 图像转换成 LAB 图像

115
00:07:45,399 --> 00:07:49,570
亮度值被设定在 0 到 100 之间

116
00:07:51,038 --> 00:07:54,708
然后 我用颜色矩阵和 Lab 图像相乘

117
00:07:54,741 --> 00:07:59,613
来去掉颜色通道
将亮度返回给调用者

118
00:07:59,646 --> 00:08:02,983
现在我们分析一下
在模型输出时发生了什么

119
00:08:04,751 --> 00:08:07,921
Core ML 模型返回了

120
00:08:07,955 --> 00:08:10,624
两个 MLShapedArray
其中包含预测的颜色通道

121
00:08:12,326 --> 00:08:18,031
在预测之后 我将两个 MLShapedArray
转换成 CIImage

122
00:08:19,533 --> 00:08:23,937
最后 将它们
与输入的亮度结合起来

123
00:08:23,971 --> 00:08:28,075
这会生成一个新的 LAB 图像
将它转换为 RGB 并返回

124
00:08:30,344 --> 00:08:34,715
要将两个 MLShapedArray
转换成 CIImage

125
00:08:34,748 --> 00:08:38,752
我首先从每个成型阵列中提取值

126
00:08:38,785 --> 00:08:42,356
然后创建两个 CIImage
代表两个颜色通道

127
00:08:42,389 --> 00:08:44,958
并返回它们

128
00:08:44,992 --> 00:08:47,961
要将亮度通道与预测的颜色通道结合起来

129
00:08:47,995 --> 00:08:51,498
我使用了一个自定义的 CIKernel

130
00:08:51,532 --> 00:08:55,068
它以三个通道作为输入
并返回一个 CIImage

131
00:08:56,503 --> 00:09:00,107
然后 我使用新的 CIFilter
convertLabToRGB

132
00:09:00,140 --> 00:09:04,978
将 LAB 图像转换为 RGB
并将其返回给调用者

133
00:09:05,012 --> 00:09:08,015
这是自定义 CIKernel 的源码

134
00:09:08,048 --> 00:09:12,052
用于把亮度通道和预测的两个颜色通道

135
00:09:12,085 --> 00:09:13,754
结合到一个 CIImage 内

136
00:09:14,888 --> 00:09:19,359
关于 RGB 图像和 LAB 图像互相转换
的新的 CIFilter

137
00:09:19,393 --> 00:09:23,664
更多信息请参阅课程

138
00:09:23,697 --> 00:09:27,734
“使用 Core Image Metal
和SwiftUI 显示 EDR 内容”

139
00:09:29,002 --> 00:09:32,472
现在我已经在我的 App 中
完成了这个 ML 功能的整合

140
00:09:32,506 --> 00:09:34,541
让我们看看它的实际效果

141
00:09:34,575 --> 00:09:36,443
但是 等一下

142
00:09:36,476 --> 00:09:41,014
在 App 中如何实时
为我的旧家庭照片着色

143
00:09:41,048 --> 00:09:45,519
我可以花一些时间将它们一一数字化
并将它们导入我的 App

144
00:09:46,653 --> 00:09:48,956
但我有一个更好的主意

145
00:09:48,989 --> 00:09:51,925
可以使用 iPad 相机扫描这些照片

146
00:09:51,959 --> 00:09:54,061
并实时给它们着色

147
00:09:54,094 --> 00:09:58,465
我想这会很有趣
而且我有实现这个的所有条件

148
00:09:58,498 --> 00:10:01,535
但首先 我必须解决一个问题

149
00:10:02,970 --> 00:10:06,607
我的模型处理图像需要 90 毫秒

150
00:10:06,640 --> 00:10:09,877
如果我想处理视频 就需要更快一些

151
00:10:11,111 --> 00:10:13,013
为了获得流畅的用户体验

152
00:10:13,046 --> 00:10:16,850
我想让设备的摄像头
运行速度至少在每秒 30 帧

153
00:10:17,885 --> 00:10:22,256
这意味着相机
大约每 30 毫秒就要产生一帧

154
00:10:24,191 --> 00:10:28,529
但是由于模型为单个视频帧着色
需要大约 90 毫秒

155
00:10:28,562 --> 00:10:32,566
所以每次着色都会错过 2 到 3 帧

156
00:10:35,269 --> 00:10:39,406
模型的总预测时间依赖于模型架构

157
00:10:39,439 --> 00:10:44,111
以及使用的计算单元运算

158
00:10:44,144 --> 00:10:48,515
再来看性能报告
我注意到我的模型

159
00:10:48,549 --> 00:10:54,188
共有61个运算
结合使用了神经网络引擎和 CPU

160
00:10:55,622 --> 00:10:59,726
如果我想要更快的预测时间
我需要改变模型

161
00:11:00,694 --> 00:11:03,530
我决定尝试修改模型架构

162
00:11:03,564 --> 00:11:06,967
探索一些可能更快的替代方案

163
00:11:07,000 --> 00:11:11,972
然而 修改架构
意味着需要重新训练网络

164
00:11:13,941 --> 00:11:15,609
Apple 提供了不同的解决方案

165
00:11:15,642 --> 00:11:19,112
让我可以在 Mac 上
直接训练机器学习模型

166
00:11:20,514 --> 00:11:25,018
就我而言 由于原始模型
是用 PyTorch 开发的

167
00:11:25,052 --> 00:11:27,888
我决定使用 Metal 上新的 PyTorch

168
00:11:27,921 --> 00:11:31,625
这样我就可以利用到
由 Apple 芯片提供的

169
00:11:31,658 --> 00:11:33,660
众多硬件加速能力

170
00:11:35,696 --> 00:11:40,167
如果您有兴趣了解
使用 Metal 加速 PyTorch

171
00:11:40,200 --> 00:11:43,937
请查看课程
“使用 Metal 加速机器学习”

172
00:11:46,173 --> 00:11:49,810
修改之后
我们需要回退一步

173
00:11:50,878 --> 00:11:53,914
重新训练之后
我需要将模型重新转成 Core ML 格式

174
00:11:53,947 --> 00:11:57,317
并再次进行验证

175
00:11:59,052 --> 00:12:01,455
这一次 模型集成

176
00:12:01,488 --> 00:12:04,725
只要简单地把旧模型更换成新模型

177
00:12:04,758 --> 00:12:08,195
重新训练了一些候选的替代模型后

178
00:12:08,228 --> 00:12:11,932
经过验证
其中一个可以满足我的需求

179
00:12:11,965 --> 00:12:16,537
这是对应的性能报告

180
00:12:16,570 --> 00:12:18,672
它完全在神经网络引擎上运行

181
00:12:18,705 --> 00:12:22,843
现在的预测时间大约是 16 毫秒

182
00:12:22,876 --> 00:12:24,545
适用于视频

183
00:12:27,347 --> 00:12:32,186
但性能报告
只展示了 App 性能的一方面

184
00:12:33,353 --> 00:12:36,723
在运行 App 后
我立即注意到

185
00:12:36,757 --> 00:12:40,294
着色并不如预期流畅

186
00:12:40,327 --> 00:12:43,697
App 在运行时究竟发生了什么

187
00:12:45,132 --> 00:12:50,103
为了理解这一点 我可以使用
Instruments 中新的 Core ML 模板

188
00:12:52,706 --> 00:12:55,576
分析 Core ML 踪迹的初始部分

189
00:12:55,609 --> 00:13:00,514
加载模型后 我注意到
App 会累积预测

190
00:13:00,547 --> 00:13:02,549
这是出乎意料的

191
00:13:02,583 --> 00:13:06,587
我希望每次只针对一帧做一次预测

192
00:13:08,121 --> 00:13:12,826
放大踪迹 并检查最初的几次预测

193
00:13:12,860 --> 00:13:16,296
我发现在第一个预测完成前

194
00:13:16,330 --> 00:13:18,565
App 请求了第二个 Core ML 预测

195
00:13:19,800 --> 00:13:23,904
在这里 当第二个请求提交给 Core ML 时

196
00:13:23,937 --> 00:13:26,440
神经网络引擎仍在处理第一个请求

197
00:13:27,574 --> 00:13:30,310
同样 第三个预测开始时

198
00:13:30,344 --> 00:13:33,413
同时仍在处理第二个

199
00:13:33,447 --> 00:13:35,549
甚至经过四次预测之后

200
00:13:35,582 --> 00:13:39,052
请求和执行之间的延迟

201
00:13:39,086 --> 00:13:42,055
已经有大约 20 毫秒

202
00:13:42,089 --> 00:13:45,759
不应如此
我需要确保只有完成之前的预测后

203
00:13:45,792 --> 00:13:50,330
才会开始新的预测
以避免这些串联

204
00:13:51,798 --> 00:13:55,936
在解决这个问题的同时
我也发现我不小心将

205
00:13:55,969 --> 00:14:01,175
相机帧速率设置为 60fps
而不是所需的 30fps

206
00:14:03,377 --> 00:14:06,513
确保 App 在上一个预测完成后

207
00:14:06,547 --> 00:14:08,849
才开始预测新的视频帧

208
00:14:08,882 --> 00:14:12,219
并将相机帧率设置为每秒 30 帧

209
00:14:12,252 --> 00:14:15,923
我可以看到
Core ML正确地分派了单个预测

210
00:14:15,956 --> 00:14:20,160
给到 Apple 神经网络引擎
现在 App 运行顺利

211
00:14:22,362 --> 00:14:24,731
所以我们的旅程也到达了终点

212
00:14:26,066 --> 00:14:28,869
我们用我的旧家庭照片
来测试一下 App

213
00:14:34,508 --> 00:14:38,512
这是我在我的地下室发现的黑白照片

214
00:14:38,545 --> 00:14:42,583
这些照片上是我很久以前
在意大利到访过的一些地方

215
00:14:49,389 --> 00:14:52,426
这是在罗马斗兽场拍的一张
很棒的照片

216
00:14:53,760 --> 00:14:56,730
墙壁和天空的颜色是如此逼真

217
00:14:59,266 --> 00:15:01,034
我们来看看这张

218
00:15:03,370 --> 00:15:06,240
这是意大利南部的蒙特堡

219
00:15:06,273 --> 00:15:07,741
非常好

220
00:15:09,643 --> 00:15:12,513
这是我的家乡 格罗塔利

221
00:15:12,546 --> 00:15:15,949
为这些图像添加颜色
触发了很多回忆

222
00:15:17,184 --> 00:15:20,554
请注意我只在照片上进行着色

223
00:15:20,587 --> 00:15:23,390
相机流中的其他场景保持黑白

224
00:15:26,326 --> 00:15:29,796
在这里 我利用的是 Vision 框架中的

225
00:15:29,830 --> 00:15:32,432
矩形检测算法

226
00:15:32,466 --> 00:15:37,204
使用 VNDetectRectangleRequest
我可以分离出场景中的照片

227
00:15:37,237 --> 00:15:39,806
把它作为着色模型的输入

228
00:15:41,041 --> 00:15:43,277
现在让我回顾一下

229
00:15:44,645 --> 00:15:48,982
在这次旅程中 我探索了大量
Apple 提供的框架、API 和工具

230
00:15:49,016 --> 00:15:52,819
让您的 App

231
00:15:52,853 --> 00:15:56,423
可以准备、集成和评估机器学习功能

232
00:15:56,456 --> 00:15:59,660
开始这段旅程时
我定义了一个问题

233
00:15:59,693 --> 00:16:02,863
这个问题
需要一个开源的机器学习模型解决

234
00:16:03,964 --> 00:16:06,800
我找到了一个符合功能条件的模型

235
00:16:06,834 --> 00:16:10,304
并使其与 Apple 平台相兼容

236
00:16:10,337 --> 00:16:13,440
我使用新的性能报告

237
00:16:13,473 --> 00:16:16,276
直接在设备上评估了模型性能

238
00:16:16,310 --> 00:16:19,379
通过您熟悉的工具和框架

239
00:16:19,413 --> 00:16:21,548
我在 App 内集成了模型

240
00:16:22,983 --> 00:16:27,154
我使用 Instruments 中
新的 Core ML 模板优化了模型

241
00:16:27,187 --> 00:16:30,524
借助 Apple 的工具和框架
我可以直接在 Apple 设备和平台上

242
00:16:30,557 --> 00:16:34,828
处理开发的各个阶段

243
00:16:34,862 --> 00:16:38,866
从数据准备 训练 到集成和优化

244
00:16:40,901 --> 00:16:44,938
今天我们只讲了一小部分
您作为开发者

245
00:16:44,972 --> 00:16:49,042
可以通过 Apple
提供的框架和工具实现强大功能

246
00:16:49,076 --> 00:16:52,446
请参考与此相关的其他课程

247
00:16:52,479 --> 00:16:56,950
为您的 App 引入机器学习
带来更多灵感

248
00:16:56,984 --> 00:16:59,786
探索和尝试框架和工具

249
00:16:59,820 --> 00:17:03,724
利用软件和硬件之间
强大的协同能力

250
00:17:03,757 --> 00:17:06,026
来加速您的机器学习功能

251
00:17:06,059 --> 00:17:09,196
并丰富您 App 的用户体验

252
00:17:09,229 --> 00:17:12,332
祝您有一个愉快的 WWDC 再见

